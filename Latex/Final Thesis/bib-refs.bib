@inproceedings{Lin2024SWC,
    author = {Lin, L. and Zhu, D. and Shang, J.},
    title = {{Overview of the Comprehensive Evaluation of Large Language Models}},
    booktitle = {2024 IEEE Smart World Congress (SWC)},
    year = {2024},
    pages = {1504--1512},
    doi = {10.1109/SWC62898.2024.00231},
    url = {https://doi.org/10.1109/SWC62898.2024.00231}
}

@misc{HuZhou2024LLMMetrics,
  title        = {Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions},
  author       = {Hu, T. and Zhou, X.-H.},
  year         = {2024},
  eprint       = {2404.09135},
  archivePrefix= {arXiv},
  note         = {Version 1},
  doi          = {10.48550/arXiv.2404.09135}
}

@misc{Bektas2025CriticalReview,
    title = {{Large Language Models in Software Engineering: A Critical Review of Evaluation Strategies}},
    author = {Bektas, Ali},
    year = {2025},
    institution = {Freie Universit{\"{a}}t Berlin},
    url = {https://elib.dlr.de/217570/1/Bektas_Ali_MA.pdf}
}

@misc{Liang2022HELM,
    title = {{Holistic Evaluation of Language Models}},
    author = {Liang, P. and Bommasani, R. and Lee, T. and Tsipras, D. and Soylu, D. and Yasunaga, M. and Zhang, Y. and Narayanan, D. and Wu, Y. and Kumar, A. and Newman, B. and Yuan, B. and Yan, B. and Zhang, C. and Cosgrove, C. and Manning, C. D. and R{\'{e}}, C. and Acosta-Navas, D. and Hudson, D. A. and ... and Koreeda, Y.},
    year = {2022},
    doi = {10.48550/arXiv.2211.09110},
    url = {https://doi.org/10.48550/arXiv.2211.09110},
    note = {arXiv}
}

@misc{Guo2023EvalLLMs,
    title = {{Evaluating Large Language Models: A Comprehensive Survey (Version 3)}},
    author = {Guo, Z. and Jin, R. and Liu, C. and Huang, Y. and Shi, D. and Supryadi and Yu, L. and Liu, Y. and Li, J. and Xiong, B. and Xiong, D.},
    year = {2023},
    doi = {10.48550/arXiv.2310.19736},
    url = {https://doi.org/10.48550/arXiv.2310.19736},
    note = {arXiv}
}

@misc{Hou2023LLMsSESLR,
    title = {{Large Language Models for Software Engineering: A Systematic Literature Review (Version 6)}},
    author = {Hou, X. and Zhao, Y. and Liu, Y. and Yang, Z. and Wang, K. and Li, L. and Luo, X. and Lo, D. and Grundy, J. and Wang, H.},
    year = {2023},
    doi = {10.48550/arXiv.2308.10620},
    url = {https://doi.org/10.48550/arXiv.2308.10620},
    note = {arXiv}
}

@misc{Hu2025BenchmarksSE,
    title = {{Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks (Version 4)}},
    author = {Hu, X. and Niu, F. and Chen, J. and Zhou, X. and Zhang, J. and He, J. and Xia, X. and Lo, D.},
    year = {2025},
    doi = {10.48550/arXiv.2505.08903},
    url = {https://doi.org/10.48550/arXiv.2505.08903},
    note = {arXiv}
}

@misc{Wang2023AMBER,
  title        = {AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation},
  author       = {Wang, J. and Wang, Y. and Xu, G. and Zhang, J. and Gu, Y. and Jia, H. and Wang, J. and Xu, H. and Yan, M. and Zhang, J. and Sang, J.},
  year         = {2023},
  eprint       = {2311.07397},
  archivePrefix= {arXiv},
  note         = {Version 2},
  doi          = {10.48550/arXiv.2311.07397}
}

@misc{Chang2023SurveyLLMs,
    title = {{A Survey on Evaluation of Large Language Models (Version 9)}},
    author = {Chang, Y. and Wang, X. and Wang, J. and Wu, Y. and Yang, L. and Zhu, K. and Chen, H. and Yi, X. and Wang, C. and Wang, Y. and Ye, W. and Zhang, Y. and Chang, Y. and Yu, P. S. and Yang, Q. and Xie, X.},
    year = {2023},
    doi = {10.48550/arXiv.2307.03109},
    url = {https://doi.org/10.48550/arXiv.2307.03109},
    note = {arXiv}
}

@misc{Chen2024SurveyCodeGen,
  title        = {A Survey on Evaluating Large Language Models in Code Generation Tasks},
  author       = {Chen, L. and Guo, Q. and Jia, H. and Zeng, Z. and Wang, X. and Xu, Y. and Wu, J. and Wang, Y. and Gao, Q. and Wang, J. and Ye, W. and Zhang, S.},
  year         = {2024},
  eprint       = {2408.16498},
  archivePrefix= {arXiv},
  note         = {Version 2},
  doi          = {10.48550/arXiv.2408.16498}
}

@misc{Zhou2025LLMAsJudge,
  title        = {An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks},
  author       = {Zhou, X. and Kim, K. and Zhang, T. and Weyssow, M. and Gomes, L. F. and Yang, G. and Liu, K. and Xia, X. and Lo, D.},
  year         = {2025},
  eprint       = {2505.20854},
  archivePrefix= {arXiv},
  note         = {Version 2},
  doi          = {10.48550/arXiv.2505.20854}
}

@misc{Zhuo2023ICEScore,
  title        = {ICE-Score: Instructing Large Language Models to Evaluate Code},
  author       = {Zhuo, T. Y.},
  year         = {2023},
  eprint       = {2304.14317},
  archivePrefix= {arXiv},
  note         = {Version 2},
  doi          = {10.48550/arXiv.2304.14317}
}

@misc{Li2024FullSDLC,
  title        = {Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study},
  author       = {Li, B. and Wu, W. and Tang, Z. and Shi, L. and Yang, J. and Li, J. and Yao, S. and Qian, C. and Hui, B. and Zhang, Q. and Yu, Z. and Du, H. and Yang, P. and Lin, D. and Peng, C. and Chen, K.},
  year         = {2024},
  eprint       = {2403.08604},
  archivePrefix= {arXiv},
  note         = {Version 3},
  doi          = {10.48550/arXiv.2403.08604}
}

@misc{Jimenez2023SWEBench,
  title        = {SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},
  author       = {Jimenez, C. E. and Yang, J. and Wettig, A. and Yao, S. and Pei, K. and Press, O. and Narasimhan, K.},
  year         = {2023},
  eprint       = {2310.06770},
  archivePrefix= {arXiv},
  note         = {Version 3},
  doi          = {10.48550/arXiv.2310.06770}
}

@misc{Srivastava2022BeyondImitation,
  title        = {Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models},
  author       = {Srivastava, A. and Rastogi, A. and Rao, A. and Shoeb, A. A. M. and Abid, A. and Fisch, A. and Brown, A. R. and Santoro, A. and Gupta, A. and Garriga-Alonso, A. and Kluska, A. and Lewkowycz, A. and Agarwal, A. and Power, A. and Ray, A. and Warstadt, A. and Kocurek, A. W. and Safaya, A. and Tazarv, A. and Wu, Z.},
  year         = {2022},
  eprint       = {2206.04615},
  archivePrefix= {arXiv},
  doi          = {10.48550/arXiv.2206.04615}
}

@article{Gallegos2024BiasFairness,
  title   = {Bias and Fairness in Large Language Models: A Survey},
  author  = {Gallegos, I. O. and Rossi, R. A. and Barrow, J. and Tanjim, M. M. and Kim, S. and Dernoncourt, F. and Yu, T. and Zhang, R. and Ahmed, N. K.},
  journal = {Computational Linguistics},
  volume  = {50},
  number  = {3},
  pages   = {1097--1179},
  year    = {2024},
  doi     = {10.1162/coli_a_00524}
}

@article{Ersoy2024BenchmarkingLlama3,
  author  = {Ersoy, P. and Erşahin, M.},
  title   = {Benchmarking Llama 3 70B for Code Generation: A Comprehensive Evaluation},
  journal = {Orclever Proceedings of Research and Development},
  volume  = {4},
  number  = {1},
  pages   = {52--58},
  year    = {2024},
  doi     = {10.56038/oprd.v4i1.444}
}

@incollection{Anand2024AnalysisLLMCode,
  author    = {Anand, A. and Chopra, S. and Arora, M.},
  title     = {Analysis of LLM Code Synthesis in Software Productivity},
  booktitle = {Applied Intelligence and Computing},
  pages     = {247--259},
  publisher = {Soft Computing Research Society},
  year      = {2024},
  doi       = {10.56155/978-81-955020-9-7-24}
}

@article{Paul2024BenchmarksMetricsCodeGen,
  author  = {Paul, D. G. and Zhu, H. and Bayley, I.},
  title   = {Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2406.12655},
  doi     = {10.48550/arXiv.2406.12655}
}

@article{Liu2023IsYourCodeCorrect,
  author  = {Liu, J. and Xia, C. S. and Wang, Y. and Zhang, L.},
  title   = {Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
  journal = {arXiv},
  year    = {2023},
  note    = {arXiv:2305.01210},
  doi     = {10.48550/arXiv.2305.01210}
}

@article{Yeo2024FrameworkEvaluatingCode,
  author  = {Yeo, S. and Ma, Y. and Kim, S. C. and Jun, H. and Kim, T.},
  title   = {Framework for evaluating code generation ability of large language models},
  journal = {ETRI Journal},
  volume  = {46},
  number  = {1},
  pages   = {106--117},
  year    = {2024},
  doi     = {10.4218/etrij.2023-0357}
}

@article{Li2024DevEval,
  author  = {Li, J. and Li, G. and Zhao, Y. and Li, Y. and Jin, Z. and Zhu, H. and Liu, H. and Liu, K. and Wang, L. and Fang, Z. and Wang, L. and Ding, J. and Zhang, X. and Dong, Y. and Zhu, Y. and Gu, B. and Yang, M.},
  title   = {DevEval: Evaluating Code Generation in Practical Software Projects},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2401.06401},
  doi     = {10.48550/arXiv.2401.06401}
}

@article{Wang2022ReCode,
  author  = {Wang, S. and Li, Z. and Qian, H. and Yang, C. and Wang, Z. and Shang, M. and Kumar, V. and Tan, S. and Ray, B. and Bhatia, P. and Nallapati, R. and Ramanathan, M. K. and Roth, D. and Xiang, B.},
  title   = {ReCode: Robustness Evaluation of Code Generation Models},
  journal = {arXiv},
  year    = {2022},
  note    = {arXiv:2212.10264},
  doi     = {10.48550/arXiv.2212.10264}
}

@article{Zhou2023CodeBERTScore,
  author  = {Zhou, S. and Alon, U. and Agarwal, S. and Neubig, G.},
  title   = {CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code},
  journal = {arXiv},
  year    = {2023},
  note    = {arXiv:2302.05527},
  doi     = {10.48550/arXiv.2302.05527}
}

@article{Evtikhiev2023OutOfBLEU,
  author  = {Evtikhiev, M. and Bogomolov, E. and Sokolov, Y. and Bryksin, T.},
  title   = {Out of the BLEU: How should we assess quality of the Code Generation models?},
  journal = {Journal of Systems and Software},
  volume  = {203},
  pages   = {111741},
  year    = {2023},
  doi     = {10.1016/j.jss.2023.111741}
}

@article{Yang2024CodeScoreR,
  author  = {Yang, G. and Zhou, Y. and Chen, X. and Zhang, X.},
  title   = {CodeScore-R: An Automated Robustness Metric for Assessing the Functional Correctness of Code Synthesis},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2406.06902},
  doi     = {10.48550/arXiv.2406.06902}
}

@inproceedings{Zhang2024CodeFort,
  author    = {Zhang, Y. and Wang, S. and Qian, H. and Wang, Z. and Shang, M. and Liu, L. and Gouda, S. K. and Ray, B. and Ramanathan, M. K. and Ma, X. and Deoras, A.},
  title     = {CodeFort: Robust Training for Code Generation Models},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages     = {5262--5277},
  publisher = {Association for Computational Linguistics},
  year      = {2024},
  doi       = {10.18653/v1/2024.findings-emnlp.303}
}

@article{Bistarelli2025UsageLLMCode,
  author  = {Bistarelli, S. and Fiore, M. and Mercanti, I. and Mongiello, M.},
  title   = {Usage of Large Language Model for Code Generation Tasks: A Review},
  journal = {SN Computer Science},
  volume  = {6},
  number  = {6},
  year    = {2025},
  doi     = {10.1007/s42979-025-04241-5}
}

@article{Busch2025LLMCodeMigration,
  author  = {Busch, D. and Bainczyk, A. and Smyth, S. and Steffen, B.},
  title   = {LLM-based code generation and system migration in language-driven engineering},
  journal = {International Journal on Software Tools for Technology Transfer},
  volume  = {27},
  number  = {1},
  pages   = {137--147},
  year    = {2025},
  doi     = {10.1007/s10009-025-00798-x}
}

@article{Hemberg2024EvolvingCodeLLM,
  author  = {Hemberg, E. and Moskal, S. and O'Reilly, U.-M.},
  title   = {Evolving code with a large language model},
  journal = {Genetic Programming and Evolvable Machines},
  volume  = {25},
  number  = {2},
  year    = {2024},
  doi     = {10.1007/s10710-024-09494-2}
}

@article{Chen2021EvaluatingLLMCode,
  author  = {Chen, M. and Tworek, J. and Jun, H. and Yuan, Q. and Pinto, H. P. de O. and Kaplan, J. and Edwards, H. and Burda, Y. and Joseph, N. and Brockman, G. and Ray, A. and Puri, R. and Krueger, G. and Petrov, M. and Khlaaf, H. and Sastry, G. and Mishkin, P. and Chan, B. and Gray, S. and Zaremba, W.},
  title   = {Evaluating Large Language Models Trained on Code},
  journal = {arXiv},
  year    = {2021},
  note    = {arXiv:2107.03374},
  doi     = {10.48550/arXiv.2107.03374}
}

@article{Lu2021CodeXGLUE,
  author  = {Lu, S. and Guo, D. and Ren, S. and Huang, J. and Svyatkovskiy, A. and Blanco, A. and Clement, C. and Drain, D. and Jiang, D. and Tang, D. and Li, G. and Zhou, L. and Shou, L. and Zhou, L. and Tufano, M. and Gong, M. and Zhou, M. and Duan, N. and Sundaresan, N. and Liu, S.},
  title   = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  journal = {arXiv},
  year    = {2021},
  note    = {arXiv:2102.04664},
  doi     = {10.48550/arXiv.2102.04664}
}

@article{Zhang2024HumanEvalV,
  author  = {Zhang, F. and Wu, L. and Bai, H. and Lin, G. and Li, X. and Yu, X. and Wang, Y. and Chen, B. and Keung, J.},
  title   = {HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2410.12381},
  doi     = {10.48550/arXiv.2410.12381}
}

@article{Austin2021ProgramSynthesisLLM,
  author  = {Austin, J. and Odena, A. and Nye, M. and Bosma, M. and Michalewski, H. and Dohan, D. and Jiang, E. and Cai, C. and Terry, M. and Le, Q. and Sutton, C.},
  title   = {Program Synthesis with Large Language Models},
  journal = {arXiv},
  year    = {2021},
  note    = {arXiv:2108.07732},
  doi     = {10.48550/arXiv.2108.07732}
}

@article{Cassano2022MultiPLE,
  author  = {Cassano, F. and Gouwar, J. and Nguyen, D. and Nguyen, S. and Phipps-Costin, L. and Pinckney, D. and Yee, M.-H. and Zi, Y. and Anderson, C. J. and Feldman, M. Q. and Guha, A. and Greenberg, M. and Jangda, A.},
  title   = {MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation},
  journal = {arXiv},
  year    = {2022},
  note    = {arXiv:2208.08227},
  doi     = {10.48550/arXiv.2208.08227}
}

@article{Li2024EvoCodeBench,
  author  = {Li, J. and Li, G. and Zhang, X. and Dong, Y. and Jin, Z.},
  title   = {EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2404.00599},
  doi     = {10.48550/arXiv.2404.00599}
}

@article{Dong2023CodeScore,
  author  = {Dong, Y. and Ding, J. and Jiang, X. and Li, G. and Li, Z. and Jin, Z.},
  title   = {CodeScore: Evaluating Code Generation by Learning Code Execution},
  journal = {arXiv},
  year    = {2023},
  note    = {arXiv:2301.09043},
  doi     = {10.48550/arXiv.2301.09043}
}

@inproceedings{Niu2024EvaluatingEfficiency,
  author    = {Niu, C. and Zhang, T. and Li, C. and Luo, B. and Ng, V.},
  title     = {On Evaluating the Efficiency of Source Code Generated by LLMs},
  booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
  pages     = {103--107},
  publisher = {ACM},
  year      = {2024},
  doi       = {10.1145/3650105.3652295}
}

@article{Coello2024EffectivenessChatGPT,
  author  = {Coello, C. E. A. and Alimam, M. N. and Kouatly, R.},
  title   = {Effectiveness of ChatGPT in Coding: A Comparative Analysis of Popular Large Language Models},
  journal = {Digital},
  volume  = {4},
  number  = {1},
  pages   = {114--125},
  year    = {2024},
  doi     = {10.3390/digital4010005}
}

@inproceedings{Tong2024CodeJudge,
  author    = {Tong, W. and Zhang, T.},
  title     = {CodeJudge: Evaluating Code Generation with Large Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages     = {20032--20051},
  publisher = {Association for Computational Linguistics},
  year      = {2024},
  doi       = {10.18653/v1/2024.emnlp-main.1118}
}

@article{Liu2024EfficientCodeGeneration,
  author  = {Liu, J. and Xie, S. and Wang, J. and Wei, Y. and Ding, Y. and Zhang, L.},
  title   = {Evaluating Language Models for Efficient Code Generation},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2408.06450},
  doi     = {10.48550/arXiv.2408.06450}
}

@article{Nascimento2024LLM4DS,
  author  = {Nascimento, N. and Guimaraes, E. and Chintakunta, S. S. and Boominathan, S. A.},
  title   = {LLM4DS: Evaluating Large Language Models for Data Science Code Generation},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2411.11908},
  doi     = {10.48550/arXiv.2411.11908}
}

@inproceedings{Wang2021CodeT5,
  author    = {Wang, Y. and Wang, W. and Joty, S. and Hoi, S. C. H.},
  title     = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages     = {8696--8708},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  doi       = {10.18653/v1/2021.emnlp-main.685}
}

@article{Christopoulou2022PanGuCoder,
  author  = {Christopoulou, F. and Lampouras, G. and Gritta, M. and Zhang, G. and Guo, Y. and Li, Z. and Zhang, Q. and Xiao, M. and Shen, B. and Li, L. and Yu, H. and Yan, L. and Zhou, P. and Wang, X. and Ma, Y. and Iacobacci, I. and Wang, Y. and Liang, G. and Wei, J. and Liu, Q.},
  title   = {PanGu-Coder: Program Synthesis with Function-Level Language Modeling},
  journal = {arXiv},
  year    = {2022},
  note    = {arXiv:2207.11280},
  doi     = {10.48550/arXiv.2207.11280}
}

@article{Allal2023SantaCoder,
  author  = {Allal, L. B. and Li, R. and Kocetkov, D. and Mou, C. and Akiki, C. and Ferrandis, C. M. and Muennighoff, N. and Mishra, M. and Gu, A. and Dey, M. and Umapathi, L. K. and Anderson, C. J. and Zi, Y. and Poirier, J. L. and Schoelkopf, H. and Troshin, S. and Abulkhanov, D. and Romero, M. and Lappert, M. and von Werra, L.},
  title   = {SantaCoder: don't reach for the stars!},
  journal = {arXiv},
  year    = {2023},
  note    = {arXiv:2301.03988},
  doi     = {10.48550/arXiv.2301.03988}
}

@article{Zheng2023CodeGeeX,
  author  = {Zheng, Q. and Xia, X. and Zou, X. and Dong, Y. and Wang, S. and Xue, Y. and Wang, Z. and Shen, L. and Wang, A. and Li, Y. and Su, T. and Yang, Z. and Tang, J.},
  title   = {CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X},
  journal = {arXiv},
  year    = {2023},
  note    = {arXiv:2303.17568},
  doi     = {10.48550/arXiv.2303.17568}
}

@article{Zhu2022XLCoST,
  author  = {Zhu, M. and Jain, A. and Suresh, K. and Ravindran, R. and Tipirneni, S. and Reddy, C. K.},
  title   = {XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence},
  journal = {arXiv},
  year    = {2022},
  note    = {arXiv:2206.08474},
  doi     = {10.48550/arXiv.2206.08474}
}

@article{Xu2025LLMAgentsToolLearning,
  author  = {Xu, W. and Huang, C. and Gao, S. and Shang, S.},
  title   = {LLM-Based Agents for Tool Learning: A Survey},
  journal = {Data Science and Engineering},
  year    = {2025},
  doi     = {10.1007/s41019-025-00296-9}
}

@article{Wang2024AutonomousAgentsSurvey,
  author  = {Wang, L. and Ma, C. and Feng, X. and Zhang, Z. and Yang, H. and Zhang, J. and Chen, Z. and Tang, J. and Chen, X. and Lin, Y. and Zhao, W. X. and Wei, Z. and Wen, J.},
  title   = {A survey on large language model based autonomous agents},
  journal = {Frontiers of Computer Science},
  volume  = {18},
  number  = {6},
  year    = {2024},
  doi     = {10.1007/s11704-024-40231-1}
}

@article{Chen2024DLBasedSE,
  author  = {Chen, X. and Hu, X. and Huang, Y. and Jiang, H. and Ji, W. and Jiang, Y. and Jiang, Y. and Liu, B. and Liu, H. and Li, X. and Lian, X. and Meng, G. and Peng, X. and Sun, H. and Shi, L. and Wang, B. and Wang, C. and Wang, J. and Wang, T. and Zhang, L.},
  title   = {Deep learning-based software engineering: progress, challenges, and opportunities},
  journal = {Science China Information Sciences},
  volume  = {68},
  number  = {1},
  year    = {2024},
  doi     = {10.1007/s11432-023-4127-5}
}

@article{Rong2025LLMOptimizationEducation,
  author  = {Rong, Y. and Du, T. and Li, R. and Bao, W.},
  title   = {Integrating LLM-based code optimization with human-like exclusionary reasoning for computational education},
  journal = {Journal of King Saud University Computer and Information Sciences},
  volume  = {37},
  number  = {5},
  year    = {2025},
  doi     = {10.1007/s44443-025-00074-7}
}

@article{Le2024OneShotCorrection,
  author  = {Le, K. T. and Andrzejak, A.},
  title   = {Rethinking AI code generation: a one-shot correction approach based on user feedback},
  journal = {Automated Software Engineering},
  volume  = {31},
  number  = {2},
  year    = {2024},
  doi     = {10.1007/s10515-024-00451-y}
}

@article{Kumar2024LLMSurvey,
  author  = {Kumar, P.},
  title   = {Large language models (LLMs): survey, technical frameworks, and future challenges},
  journal = {Artificial Intelligence Review},
  volume  = {57},
  number  = {10},
  year    = {2024},
  doi     = {10.1007/s10462-024-10888-y}
}

@article{Qiu2025LoCoBench,
  author  = {Qiu, J. and Liu, Z. and Liu, Z. and Murthy, R. and Zhang, J. and Chen, H. and Wang, S. and Zhu, M. and Yang, L. and Tan, J. and Cen, Z. and Qian, C. and Heinecke, S. and Yao, W. and Savarese, S. and Xiong, C. and Wang, H.},
  title   = {LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering},
  journal = {arXiv},
  year    = {2025},
  note    = {arXiv:2509.09614},
  doi     = {10.48550/arXiv.2509.09614}
}

@article{Pena2025NonCodeSETasks,
  author  = {Peña, F. C. and Herbold, S.},
  title   = {Evaluating Large Language Models on Non-Code Software Engineering Tasks},
  journal = {arXiv},
  year    = {2025},
  note    = {arXiv:2506.10833},
  doi     = {10.48550/arXiv.2506.10833}
}

@article{Afreen2025BiasMitigation,
  author  = {Afreen, J. and Mohaghegh, M. and Doborjeh, M.},
  title   = {Systematic literature review on bias mitigation in generative AI},
  journal = {AI and Ethics},
  volume  = {5},
  number  = {5},
  pages   = {4789--4841},
  year    = {2025},
  doi     = {10.1007/s43681-025-00721-9}
}

@article{Shao2024LLMArchitecturesSurvey,
  author  = {Shao, M. and Basit, A. and Karri, R. and Shafique, M.},
  title   = {Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges},
  journal = {IEEE Access},
  volume  = {12},
  pages   = {188664--188706},
  year    = {2024},
  doi     = {10.1109/ACCESS.2024.3482107}
}

@article{Sagodi2024CodeSynthesisEvaluation,
  author  = {Ságodi, Z. and Siket, I. and Ferenc, R.},
  title   = {Methodology for Code Synthesis Evaluation of LLMs Presented by a Case Study of ChatGPT and Copilot},
  journal = {IEEE Access},
  volume  = {12},
  pages   = {72303--72316},
  year    = {2024},
  doi     = {10.1109/ACCESS.2024.3403858}
}

@article{Black2024LLMFuzzing,
  author  = {Black, G. and Vaidyan, V. Mathew and Comert, G.},
  title   = {Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation},
  journal = {IEEE Access},
  volume  = {12},
  pages   = {156065--156081},
  year    = {2024},
  doi     = {10.1109/ACCESS.2024.3484947}
}

@article{Ko2025CodingProficiency,
  author  = {Ko, E. and Kang, P.},
  title   = {Evaluating Coding Proficiency of Large Language Models: An Investigation Through Machine Learning Problems},
  journal = {IEEE Access},
  volume  = {13},
  pages   = {52925--52938},
  year    = {2025},
  doi     = {10.1109/ACCESS.2025.3553870}
}

@article{Woesle2025Hallucinations,
    title = {{A Systematic Literature Review of Hallucinations in Large Language Models}},
    author = {Woesle, C. and Fischer-Brandies, L. and Buettner, R.},
    journal = {IEEE Access},
    volume = {13},
    pages = {148231--148253},
    year = {2025},
    doi = {10.1109/ACCESS.2025.3601206},
    url = {https://doi.org/10.1109/ACCESS.2025.3601206}
}

@article{Li2025LLMSoftwareTesting,
  author  = {Li, Y. and Liu, P. and Wang, H. and Chu, J. and Wong, W. E.},
  title   = {Evaluating large language models for software testing},
  journal = {Computer Standards \& Interfaces},
  volume  = {93},
  pages   = {103942},
  year    = {2025},
  doi     = {10.1016/j.csi.2024.103942}
}

@inproceedings{Du2024ClassLevelCodeGen,
  author    = {Du, X. and Liu, M. and Wang, K. and Wang, H. and Liu, J. and Chen, Y. and Feng, J. and Sha, C. and Peng, X. and Lou, Y.},
  title     = {Evaluating Large Language Models in Class-Level Code Generation},
  booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages     = {1--13},
  year      = {2024},
  publisher = {ACM},
  doi       = {10.1145/3597503.3639219}
}

@article{Schafer2024UnitTestGeneration,
  author  = {Schäfer, M. and Nadi, S. and Eghbali, A. and Tip, F.},
  title   = {An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation},
  journal = {IEEE Transactions on Software Engineering},
  volume  = {50},
  number  = {1},
  pages   = {85--105},
  year    = {2024},
  doi     = {10.1109/TSE.2023.3334955}
}

@article{Alhanahnah2025FormalSpecRepair,
  author  = {Alhanahnah, M. and Hasan, M. R. and Xu, L. and Bagheri, H.},
  title   = {An empirical evaluation of pre-trained large language models for repairing declarative formal specifications},
  journal = {Empirical Software Engineering},
  volume  = {30},
  number  = {5},
  year    = {2025},
  doi     = {10.1007/s10664-025-10687-1}
}

@article{Guo2025HumanCentricEvaluation,
  author  = {Guo, Y. and Ji, K. and Zhu, X. and Wang, J. and Wen, F. and Li, C. and Zhang, Z. and Zhai, G.},
  title   = {Human-Centric Evaluation for Foundation Models},
  journal = {arXiv},
  year    = {2025},
  note    = {arXiv:2506.01793},
  doi     = {10.48550/arXiv.2506.01793}
}

@article{Stein2023MetricFlaws,
    title = {{Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models}},
    author = {Stein, G. and Cresswell, J. C. and Hosseinzadeh, R. and Sui, Y. and Ross, B. L. and Villecroze, V. and Liu, Z. and Caterini, A. L. and Taylor, J. E. T. and Loaiza-Ganem, G.},
    year = {2023},
    doi = {10.48550/arXiv.2306.04675},
    url = {https://doi.org/10.48550/arXiv.2306.04675},
    note = {arXiv}
}

@article{Mundhra2025IndustrialCaseStudy,
  author  = {Mundhra, Y. and Valk, M. and Izadi, M.},
  title   = {Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML},
  journal = {arXiv},
  year    = {2025},
  note    = {arXiv:2509.12395},
  doi     = {10.48550/arXiv.2509.12395}
}
