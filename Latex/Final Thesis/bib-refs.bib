@inproceedings{Lin2024SWC,
    author = {Lin, L. and Zhu, D. and Shang, J.},
    title = {{Overview of the Comprehensive Evaluation of Large Language Models}},
    booktitle = {2024 IEEE Smart World Congress (SWC)},
    year = {2024},
    pages = {1504--1512},
    doi = {10.1109/SWC62898.2024.00231},
    url = {https://doi.org/10.1109/SWC62898.2024.00231}
}

@misc{HuZhou2024LLMMetrics,
  title        = {Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions},
  author       = {Hu, T. and Zhou, X.-H.},
  year         = {2024},
  eprint       = {2404.09135},
  archivePrefix= {arXiv},
  note         = {Version 1},
  doi          = {10.48550/arXiv.2404.09135}
}

@misc{Bektas2025CriticalReview,
    title = {{Large Language Models in Software Engineering: A Critical Review of Evaluation Strategies}},
    author = {Bektas, Ali},
    year = {2025},
    institution = {Freie Universit{\"{a}}t Berlin},
    url = {https://elib.dlr.de/217570/1/Bektas_Ali_MA.pdf}
}

@misc{Liang2022HELM,
    title = {{Holistic Evaluation of Language Models}},
    author = {Liang, P. and Bommasani, R. and Lee, T. and Tsipras, D. and Soylu, D. and Yasunaga, M. and Zhang, Y. and Narayanan, D. and Wu, Y. and Kumar, A. and Newman, B. and Yuan, B. and Yan, B. and Zhang, C. and Cosgrove, C. and Manning, C. D. and R{\'{e}}, C. and Acosta-Navas, D. and Hudson, D. A. and ... and Koreeda, Y.},
    year = {2022},
    doi = {10.48550/arXiv.2211.09110},
    url = {https://doi.org/10.48550/arXiv.2211.09110},
    note = {arXiv}
}

@misc{Guo2023EvalLLMs,
    title = {{Evaluating Large Language Models: A Comprehensive Survey (Version 3)}},
    author = {Guo, Z. and Jin, R. and Liu, C. and Huang, Y. and Shi, D. and Supryadi and Yu, L. and Liu, Y. and Li, J. and Xiong, B. and Xiong, D.},
    year = {2023},
    doi = {10.48550/arXiv.2310.19736},
    url = {https://doi.org/10.48550/arXiv.2310.19736},
    note = {arXiv}
}

@misc{Hou2023LLMsSESLR,
    title = {{Large Language Models for Software Engineering: A Systematic Literature Review (Version 6)}},
    author = {Hou, X. and Zhao, Y. and Liu, Y. and Yang, Z. and Wang, K. and Li, L. and Luo, X. and Lo, D. and Grundy, J. and Wang, H.},
    year = {2023},
    doi = {10.48550/arXiv.2308.10620},
    url = {https://doi.org/10.48550/arXiv.2308.10620},
    note = {arXiv}
}

@misc{Hu2025BenchmarksSE,
    title = {{Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks (Version 4)}},
    author = {Hu, X. and Niu, F. and Chen, J. and Zhou, X. and Zhang, J. and He, J. and Xia, X. and Lo, D.},
    year = {2025},
    doi = {10.48550/arXiv.2505.08903},
    url = {https://doi.org/10.48550/arXiv.2505.08903},
    note = {arXiv}
}

@misc{Wang2023AMBER,
  title        = {AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation},
  author       = {Wang, J. and Wang, Y. and Xu, G. and Zhang, J. and Gu, Y. and Jia, H. and Wang, J. and Xu, H. and Yan, M. and Zhang, J. and Sang, J.},
  year         = {2023},
  eprint       = {2311.07397},
  archivePrefix= {arXiv},
  note         = {Version 2},
  doi          = {10.48550/arXiv.2311.07397}
}

@misc{Chang2023SurveyLLMs,
    title = {{A Survey on Evaluation of Large Language Models (Version 9)}},
    author = {Chang, Y. and Wang, X. and Wang, J. and Wu, Y. and Yang, L. and Zhu, K. and Chen, H. and Yi, X. and Wang, C. and Wang, Y. and Ye, W. and Zhang, Y. and Chang, Y. and Yu, P. S. and Yang, Q. and Xie, X.},
    year = {2023},
    doi = {10.48550/arXiv.2307.03109},
    url = {https://doi.org/10.48550/arXiv.2307.03109},
    note = {arXiv}
}

@misc{Chen2024SurveyCodeGen,
  title        = {A Survey on Evaluating Large Language Models in Code Generation Tasks},
  author       = {Chen, L. and Guo, Q. and Jia, H. and Zeng, Z. and Wang, X. and Xu, Y. and Wu, J. and Wang, Y. and Gao, Q. and Wang, J. and Ye, W. and Zhang, S.},
  year         = {2024},
  eprint       = {2408.16498},
  archivePrefix= {arXiv},
  note         = {Version 2},
  doi          = {10.48550/arXiv.2408.16498}
}

@misc{Zhou2025LLMAsJudge,
  title        = {An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks},
  author       = {Zhou, X. and Kim, K. and Zhang, T. and Weyssow, M. and Gomes, L. F. and Yang, G. and Liu, K. and Xia, X. and Lo, D.},
  year         = {2025},
  eprint       = {2505.20854},
  archivePrefix= {arXiv},
  note         = {Version 2},
  doi          = {10.48550/arXiv.2505.20854}
}

@misc{Zhuo2023ICEScore,
  title        = {ICE-Score: Instructing Large Language Models to Evaluate Code},
  author       = {Zhuo, T. Y.},
  year         = {2023},
  eprint       = {2304.14317},
  archivePrefix= {arXiv},
  note         = {Version 2},
  doi          = {10.48550/arXiv.2304.14317}
}

@misc{Li2024FullSDLC,
  title        = {Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study},
  author       = {Li, B. and Wu, W. and Tang, Z. and Shi, L. and Yang, J. and Li, J. and Yao, S. and Qian, C. and Hui, B. and Zhang, Q. and Yu, Z. and Du, H. and Yang, P. and Lin, D. and Peng, C. and Chen, K.},
  year         = {2024},
  eprint       = {2403.08604},
  archivePrefix= {arXiv},
  note         = {Version 3},
  doi          = {10.48550/arXiv.2403.08604}
}

@misc{Jimenez2023SWEBench,
  title        = {SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},
  author       = {Jimenez, C. E. and Yang, J. and Wettig, A. and Yao, S. and Pei, K. and Press, O. and Narasimhan, K.},
  year         = {2023},
  eprint       = {2310.06770},
  archivePrefix= {arXiv},
  note         = {Version 3},
  doi          = {10.48550/arXiv.2310.06770}
}

@misc{Srivastava2022BeyondImitation,
  title        = {Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models},
  author       = {Srivastava, A. and Rastogi, A. and Rao, A. and Shoeb, A. A. M. and Abid, A. and Fisch, A. and Brown, A. R. and Santoro, A. and Gupta, A. and Garriga-Alonso, A. and Kluska, A. and Lewkowycz, A. and Agarwal, A. and Power, A. and Ray, A. and Warstadt, A. and Kocurek, A. W. and Safaya, A. and Tazarv, A. and Wu, Z.},
  year         = {2022},
  eprint       = {2206.04615},
  archivePrefix= {arXiv},
  doi          = {10.48550/arXiv.2206.04615}
}

@article{Gallegos2024BiasFairness,
  title   = {Bias and Fairness in Large Language Models: A Survey},
  author  = {Gallegos, I. O. and Rossi, R. A. and Barrow, J. and Tanjim, M. M. and Kim, S. and Dernoncourt, F. and Yu, T. and Zhang, R. and Ahmed, N. K.},
  journal = {Computational Linguistics},
  volume  = {50},
  number  = {3},
  pages   = {1097--1179},
  year    = {2024},
  doi     = {10.1162/coli_a_00524}
}

@article{Ersoy2024BenchmarkingLlama3,
  author  = {Ersoy, P. and Erşahin, M.},
  title   = {Benchmarking Llama 3 70B for Code Generation: A Comprehensive Evaluation},
  journal = {Orclever Proceedings of Research and Development},
  volume  = {4},
  number  = {1},
  pages   = {52--58},
  year    = {2024},
  doi     = {10.56038/oprd.v4i1.444}
}

@incollection{Anand2024AnalysisLLMCode,
  author    = {Anand, A. and Chopra, S. and Arora, M.},
  title     = {Analysis of LLM Code Synthesis in Software Productivity},
  booktitle = {Applied Intelligence and Computing},
  pages     = {247--259},
  publisher = {Soft Computing Research Society},
  year      = {2024},
  doi       = {10.56155/978-81-955020-9-7-24}
}

@article{Paul2024BenchmarksMetricsCodeGen,
  author  = {Paul, D. G. and Zhu, H. and Bayley, I.},
  title   = {Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2406.12655},
  doi     = {10.48550/arXiv.2406.12655}
}

@article{Liu2023IsYourCodeCorrect,
  author  = {Liu, J. and Xia, C. S. and Wang, Y. and Zhang, L.},
  title   = {Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
  journal = {arXiv},
  year    = {2023},
  note    = {arXiv:2305.01210},
  doi     = {10.48550/arXiv.2305.01210}
}

@article{Yeo2024FrameworkEvaluatingCode,
  author  = {Yeo, S. and Ma, Y. and Kim, S. C. and Jun, H. and Kim, T.},
  title   = {Framework for evaluating code generation ability of large language models},
  journal = {ETRI Journal},
  volume  = {46},
  number  = {1},
  pages   = {106--117},
  year    = {2024},
  doi     = {10.4218/etrij.2023-0357}
}

@article{Li2024DevEval,
  author  = {Li, J. and Li, G. and Zhao, Y. and Li, Y. and Jin, Z. and Zhu, H. and Liu, H. and Liu, K. and Wang, L. and Fang, Z. and Wang, L. and Ding, J. and Zhang, X. and Dong, Y. and Zhu, Y. and Gu, B. and Yang, M.},
  title   = {DevEval: Evaluating Code Generation in Practical Software Projects},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2401.06401},
  doi     = {10.48550/arXiv.2401.06401}
}

@article{Wang2022ReCode,
  author  = {Wang, S. and Li, Z. and Qian, H. and Yang, C. and Wang, Z. and Shang, M. and Kumar, V. and Tan, S. and Ray, B. and Bhatia, P. and Nallapati, R. and Ramanathan, M. K. and Roth, D. and Xiang, B.},
  title   = {ReCode: Robustness Evaluation of Code Generation Models},
  journal = {arXiv},
  year    = {2022},
  note    = {arXiv:2212.10264},
  doi     = {10.48550/arXiv.2212.10264}
}

@article{Zhou2023CodeBERTScore,
  author  = {Zhou, S. and Alon, U. and Agarwal, S. and Neubig, G.},
  title   = {CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code},
  journal = {arXiv},
  year    = {2023},
  note    = {arXiv:2302.05527},
  doi     = {10.48550/arXiv.2302.05527}
}

@article{Evtikhiev2023OutOfBLEU,
  author  = {Evtikhiev, M. and Bogomolov, E. and Sokolov, Y. and Bryksin, T.},
  title   = {Out of the BLEU: How should we assess quality of the Code Generation models?},
  journal = {Journal of Systems and Software},
  volume  = {203},
  pages   = {111741},
  year    = {2023},
  doi     = {10.1016/j.jss.2023.111741}
}

@article{Yang2024CodeScoreR,
  author  = {Yang, G. and Zhou, Y. and Chen, X. and Zhang, X.},
  title   = {CodeScore-R: An Automated Robustness Metric for Assessing the Functional Correctness of Code Synthesis},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2406.06902},
  doi     = {10.48550/arXiv.2406.06902}
}

@inproceedings{Zhang2024CodeFort,
  author    = {Zhang, Y. and Wang, S. and Qian, H. and Wang, Z. and Shang, M. and Liu, L. and Gouda, S. K. and Ray, B. and Ramanathan, M. K. and Ma, X. and Deoras, A.},
  title     = {CodeFort: Robust Training for Code Generation Models},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages     = {5262--5277},
  publisher = {Association for Computational Linguistics},
  year      = {2024},
  doi       = {10.18653/v1/2024.findings-emnlp.303}
}

@article{Bistarelli2025UsageLLMCode,
  author  = {Bistarelli, S. and Fiore, M. and Mercanti, I. and Mongiello, M.},
  title   = {Usage of Large Language Model for Code Generation Tasks: A Review},
  journal = {SN Computer Science},
  volume  = {6},
  number  = {6},
  year    = {2025},
  doi     = {10.1007/s42979-025-04241-5}
}

@article{Busch2025LLMCodeMigration,
  author  = {Busch, D. and Bainczyk, A. and Smyth, S. and Steffen, B.},
  title   = {LLM-based code generation and system migration in language-driven engineering},
  journal = {International Journal on Software Tools for Technology Transfer},
  volume  = {27},
  number  = {1},
  pages   = {137--147},
  year    = {2025},
  doi     = {10.1007/s10009-025-00798-x}
}

@article{Hemberg2024EvolvingCodeLLM,
  author  = {Hemberg, E. and Moskal, S. and O'Reilly, U.-M.},
  title   = {Evolving code with a large language model},
  journal = {Genetic Programming and Evolvable Machines},
  volume  = {25},
  number  = {2},
  year    = {2024},
  doi     = {10.1007/s10710-024-09494-2}
}

@article{Chen2021EvaluatingLLMCode,
  author  = {Chen, M. and Tworek, J. and Jun, H. and Yuan, Q. and Pinto, H. P. de O. and Kaplan, J. and Edwards, H. and Burda, Y. and Joseph, N. and Brockman, G. and Ray, A. and Puri, R. and Krueger, G. and Petrov, M. and Khlaaf, H. and Sastry, G. and Mishkin, P. and Chan, B. and Gray, S. and Zaremba, W.},
  title   = {Evaluating Large Language Models Trained on Code},
  journal = {arXiv},
  year    = {2021},
  note    = {arXiv:2107.03374},
  doi     = {10.48550/arXiv.2107.03374}
}

@article{Lu2021CodeXGLUE,
  author  = {Lu, S. and Guo, D. and Ren, S. and Huang, J. and Svyatkovskiy, A. and Blanco, A. and Clement, C. and Drain, D. and Jiang, D. and Tang, D. and Li, G. and Zhou, L. and Shou, L. and Zhou, L. and Tufano, M. and Gong, M. and Zhou, M. and Duan, N. and Sundaresan, N. and Liu, S.},
  title   = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  journal = {arXiv},
  year    = {2021},
  note    = {arXiv:2102.04664},
  doi     = {10.48550/arXiv.2102.04664}
}

@article{Zhang2024HumanEvalV,
  author  = {Zhang, F. and Wu, L. and Bai, H. and Lin, G. and Li, X. and Yu, X. and Wang, Y. and Chen, B. and Keung, J.},
  title   = {HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2410.12381},
  doi     = {10.48550/arXiv.2410.12381}
}

@article{Austin2021ProgramSynthesisLLM,
  author  = {Austin, J. and Odena, A. and Nye, M. and Bosma, M. and Michalewski, H. and Dohan, D. and Jiang, E. and Cai, C. and Terry, M. and Le, Q. and Sutton, C.},
  title   = {Program Synthesis with Large Language Models},
  journal = {arXiv},
  year    = {2021},
  note    = {arXiv:2108.07732},
  doi     = {10.48550/arXiv.2108.07732}
}

@article{Cassano2022MultiPLE,
  author  = {Cassano, F. and Gouwar, J. and Nguyen, D. and Nguyen, S. and Phipps-Costin, L. and Pinckney, D. and Yee, M.-H. and Zi, Y. and Anderson, C. J. and Feldman, M. Q. and Guha, A. and Greenberg, M. and Jangda, A.},
  title   = {MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation},
  journal = {arXiv},
  year    = {2022},
  note    = {arXiv:2208.08227},
  doi     = {10.48550/arXiv.2208.08227}
}

@article{Li2024EvoCodeBench,
  author  = {Li, J. and Li, G. and Zhang, X. and Dong, Y. and Jin, Z.},
  title   = {EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2404.00599},
  doi     = {10.48550/arXiv.2404.00599}
}

@article{Dong2023CodeScore,
  author  = {Dong, Y. and Ding, J. and Jiang, X. and Li, G. and Li, Z. and Jin, Z.},
  title   = {CodeScore: Evaluating Code Generation by Learning Code Execution},
  journal = {arXiv},
  year    = {2023},
  note    = {arXiv:2301.09043},
  doi     = {10.48550/arXiv.2301.09043}
}

@inproceedings{Niu2024EvaluatingEfficiency,
  author    = {Niu, C. and Zhang, T. and Li, C. and Luo, B. and Ng, V.},
  title     = {On Evaluating the Efficiency of Source Code Generated by LLMs},
  booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
  pages     = {103--107},
  publisher = {ACM},
  year      = {2024},
  doi       = {10.1145/3650105.3652295}
}

@article{Coello2024EffectivenessChatGPT,
  author  = {Coello, C. E. A. and Alimam, M. N. and Kouatly, R.},
  title   = {Effectiveness of ChatGPT in Coding: A Comparative Analysis of Popular Large Language Models},
  journal = {Digital},
  volume  = {4},
  number  = {1},
  pages   = {114--125},
  year    = {2024},
  doi     = {10.3390/digital4010005}
}

@inproceedings{Tong2024CodeJudge,
  author    = {Tong, W. and Zhang, T.},
  title     = {CodeJudge: Evaluating Code Generation with Large Language Models},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages     = {20032--20051},
  publisher = {Association for Computational Linguistics},
  year      = {2024},
  doi       = {10.18653/v1/2024.emnlp-main.1118}
}

@article{Liu2024EfficientCodeGeneration,
  author  = {Liu, J. and Xie, S. and Wang, J. and Wei, Y. and Ding, Y. and Zhang, L.},
  title   = {Evaluating Language Models for Efficient Code Generation},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2408.06450},
  doi     = {10.48550/arXiv.2408.06450}
}

@article{Nascimento2024LLM4DS,
  author  = {Nascimento, N. and Guimaraes, E. and Chintakunta, S. S. and Boominathan, S. A.},
  title   = {LLM4DS: Evaluating Large Language Models for Data Science Code Generation},
  journal = {arXiv},
  year    = {2024},
  note    = {arXiv:2411.11908},
  doi     = {10.48550/arXiv.2411.11908}
}

@inproceedings{Wang2021CodeT5,
  author    = {Wang, Y. and Wang, W. and Joty, S. and Hoi, S. C. H.},
  title     = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages     = {8696--8708},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  doi       = {10.18653/v1/2021.emnlp-main.685}
}

@article{Christopoulou2022PanGuCoder,
  author  = {Christopoulou, F. and Lampouras, G. and Gritta, M. and Zhang, G. and Guo, Y. and Li, Z. and Zhang, Q. and Xiao, M. and Shen, B. and Li, L. and Yu, H. and Yan, L. and Zhou, P. and Wang, X. and Ma, Y. and Iacobacci, I. and Wang, Y. and Liang, G. and Wei, J. and Liu, Q.},
  title   = {PanGu-Coder: Program Synthesis with Function-Level Language Modeling},
  journal = {arXiv},
  year    = {2022},
  note    = {arXiv:2207.11280},
  doi     = {10.48550/arXiv.2207.11280}
}

@article{Allal2023SantaCoder,
  author  = {Allal, L. B. and Li, R. and Kocetkov, D. and Mou, C. and Akiki, C. and Ferrandis, C. M. and Muennighoff, N. and Mishra, M. and Gu, A. and Dey, M. and Umapathi, L. K. and Anderson, C. J. and Zi, Y. and Poirier, J. L. and Schoelkopf, H. and Troshin, S. and Abulkhanov, D. and Romero, M. and Lappert, M. and von Werra, L.},
  title   = {SantaCoder: don't reach for the stars!},
  journal = {arXiv},
  year    = {2023},
  note    = {arXiv:2301.03988},
  doi     = {10.48550/arXiv.2301.03988}
}

@article{Zheng2023CodeGeeX,
  author  = {Zheng, Q. and Xia, X. and Zou, X. and Dong, Y. and Wang, S. and Xue, Y. and Wang, Z. and Shen, L. and Wang, A. and Li, Y. and Su, T. and Yang, Z. and Tang, J.},
  title   = {CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X},
  journal = {arXiv},
  year    = {2023},
  note    = {arXiv:2303.17568},
  doi     = {10.48550/arXiv.2303.17568}
}

@article{Zhu2022XLCoST,
  author  = {Zhu, M. and Jain, A. and Suresh, K. and Ravindran, R. and Tipirneni, S. and Reddy, C. K.},
  title   = {XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence},
  journal = {arXiv},
  year    = {2022},
  note    = {arXiv:2206.08474},
  doi     = {10.48550/arXiv.2206.08474}
}

@article{Xu2025LLMAgentsToolLearning,
  author  = {Xu, W. and Huang, C. and Gao, S. and Shang, S.},
  title   = {LLM-Based Agents for Tool Learning: A Survey},
  journal = {Data Science and Engineering},
  year    = {2025},
  doi     = {10.1007/s41019-025-00296-9}
}

@article{Wang2024AutonomousAgentsSurvey,
  author  = {Wang, L. and Ma, C. and Feng, X. and Zhang, Z. and Yang, H. and Zhang, J. and Chen, Z. and Tang, J. and Chen, X. and Lin, Y. and Zhao, W. X. and Wei, Z. and Wen, J.},
  title   = {A survey on large language model based autonomous agents},
  journal = {Frontiers of Computer Science},
  volume  = {18},
  number  = {6},
  year    = {2024},
  doi     = {10.1007/s11704-024-40231-1}
}

@article{Chen2024DLBasedSE,
  author  = {Chen, X. and Hu, X. and Huang, Y. and Jiang, H. and Ji, W. and Jiang, Y. and Jiang, Y. and Liu, B. and Liu, H. and Li, X. and Lian, X. and Meng, G. and Peng, X. and Sun, H. and Shi, L. and Wang, B. and Wang, C. and Wang, J. and Wang, T. and Zhang, L.},
  title   = {Deep learning-based software engineering: progress, challenges, and opportunities},
  journal = {Science China Information Sciences},
  volume  = {68},
  number  = {1},
  year    = {2024},
  doi     = {10.1007/s11432-023-4127-5}
}

@article{Rong2025LLMOptimizationEducation,
  author  = {Rong, Y. and Du, T. and Li, R. and Bao, W.},
  title   = {Integrating LLM-based code optimization with human-like exclusionary reasoning for computational education},
  journal = {Journal of King Saud University Computer and Information Sciences},
  volume  = {37},
  number  = {5},
  year    = {2025},
  doi     = {10.1007/s44443-025-00074-7}
}

@article{Le2024OneShotCorrection,
  author  = {Le, K. T. and Andrzejak, A.},
  title   = {Rethinking AI code generation: a one-shot correction approach based on user feedback},
  journal = {Automated Software Engineering},
  volume  = {31},
  number  = {2},
  year    = {2024},
  doi     = {10.1007/s10515-024-00451-y}
}

@article{Kumar2024LLMSurvey,
  author  = {Kumar, P.},
  title   = {Large language models (LLMs): survey, technical frameworks, and future challenges},
  journal = {Artificial Intelligence Review},
  volume  = {57},
  number  = {10},
  year    = {2024},
  doi     = {10.1007/s10462-024-10888-y}
}

@article{Qiu2025LoCoBench,
  author  = {Qiu, J. and Liu, Z. and Liu, Z. and Murthy, R. and Zhang, J. and Chen, H. and Wang, S. and Zhu, M. and Yang, L. and Tan, J. and Cen, Z. and Qian, C. and Heinecke, S. and Yao, W. and Savarese, S. and Xiong, C. and Wang, H.},
  title   = {LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering},
  journal = {arXiv},
  year    = {2025},
  note    = {arXiv:2509.09614},
  doi     = {10.48550/arXiv.2509.09614}
}

@article{Pena2025NonCodeSETasks,
  author  = {Peña, F. C. and Herbold, S.},
  title   = {Evaluating Large Language Models on Non-Code Software Engineering Tasks},
  journal = {arXiv},
  year    = {2025},
  note    = {arXiv:2506.10833},
  doi     = {10.48550/arXiv.2506.10833}
}

@article{Afreen2025BiasMitigation,
  author  = {Afreen, J. and Mohaghegh, M. and Doborjeh, M.},
  title   = {Systematic literature review on bias mitigation in generative AI},
  journal = {AI and Ethics},
  volume  = {5},
  number  = {5},
  pages   = {4789--4841},
  year    = {2025},
  doi     = {10.1007/s43681-025-00721-9}
}

@article{Shao2024LLMArchitecturesSurvey,
  author  = {Shao, M. and Basit, A. and Karri, R. and Shafique, M.},
  title   = {Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges},
  journal = {IEEE Access},
  volume  = {12},
  pages   = {188664--188706},
  year    = {2024},
  doi     = {10.1109/ACCESS.2024.3482107}
}

@article{Sagodi2024CodeSynthesisEvaluation,
  author  = {Ságodi, Z. and Siket, I. and Ferenc, R.},
  title   = {Methodology for Code Synthesis Evaluation of LLMs Presented by a Case Study of ChatGPT and Copilot},
  journal = {IEEE Access},
  volume  = {12},
  pages   = {72303--72316},
  year    = {2024},
  doi     = {10.1109/ACCESS.2024.3403858}
}

@article{Black2024LLMFuzzing,
  author  = {Black, G. and Vaidyan, V. Mathew and Comert, G.},
  title   = {Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation},
  journal = {IEEE Access},
  volume  = {12},
  pages   = {156065--156081},
  year    = {2024},
  doi     = {10.1109/ACCESS.2024.3484947}
}

@article{Ko2025CodingProficiency,
  author  = {Ko, E. and Kang, P.},
  title   = {Evaluating Coding Proficiency of Large Language Models: An Investigation Through Machine Learning Problems},
  journal = {IEEE Access},
  volume  = {13},
  pages   = {52925--52938},
  year    = {2025},
  doi     = {10.1109/ACCESS.2025.3553870}
}

@article{Woesle2025Hallucinations,
    title = {{A Systematic Literature Review of Hallucinations in Large Language Models}},
    author = {Woesle, C. and Fischer-Brandies, L. and Buettner, R.},
    journal = {IEEE Access},
    volume = {13},
    pages = {148231--148253},
    year = {2025},
    doi = {10.1109/ACCESS.2025.3601206},
    url = {https://doi.org/10.1109/ACCESS.2025.3601206}
}

@article{Li2025LLMSoftwareTesting,
  author  = {Li, Y. and Liu, P. and Wang, H. and Chu, J. and Wong, W. E.},
  title   = {Evaluating large language models for software testing},
  journal = {Computer Standards \& Interfaces},
  volume  = {93},
  pages   = {103942},
  year    = {2025},
  doi     = {10.1016/j.csi.2024.103942}
}

@inproceedings{Du2024ClassLevelCodeGen,
  author    = {Du, X. and Liu, M. and Wang, K. and Wang, H. and Liu, J. and Chen, Y. and Feng, J. and Sha, C. and Peng, X. and Lou, Y.},
  title     = {Evaluating Large Language Models in Class-Level Code Generation},
  booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages     = {1--13},
  year      = {2024},
  publisher = {ACM},
  doi       = {10.1145/3597503.3639219}
}

@article{Schafer2024UnitTestGeneration,
  author  = {Schäfer, M. and Nadi, S. and Eghbali, A. and Tip, F.},
  title   = {An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation},
  journal = {IEEE Transactions on Software Engineering},
  volume  = {50},
  number  = {1},
  pages   = {85--105},
  year    = {2024},
  doi     = {10.1109/TSE.2023.3334955}
}

@article{Alhanahnah2025FormalSpecRepair,
  author  = {Alhanahnah, M. and Hasan, M. R. and Xu, L. and Bagheri, H.},
  title   = {An empirical evaluation of pre-trained large language models for repairing declarative formal specifications},
  journal = {Empirical Software Engineering},
  volume  = {30},
  number  = {5},
  year    = {2025},
  doi     = {10.1007/s10664-025-10687-1}
}

@article{Guo2025HumanCentricEvaluation,
  author  = {Guo, Y. and Ji, K. and Zhu, X. and Wang, J. and Wen, F. and Li, C. and Zhang, Z. and Zhai, G.},
  title   = {Human-Centric Evaluation for Foundation Models},
  journal = {arXiv},
  year    = {2025},
  note    = {arXiv:2506.01793},
  doi     = {10.48550/arXiv.2506.01793}
}

@article{Stein2023MetricFlaws,
    title = {{Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models}},
    author = {Stein, G. and Cresswell, J. C. and Hosseinzadeh, R. and Sui, Y. and Ross, B. L. and Villecroze, V. and Liu, Z. and Caterini, A. L. and Taylor, J. E. T. and Loaiza-Ganem, G.},
    year = {2023},
    doi = {10.48550/arXiv.2306.04675},
    url = {https://doi.org/10.48550/arXiv.2306.04675},
    note = {arXiv}
}

@article{Mundhra2025IndustrialCaseStudy,
  author  = {Mundhra, Y. and Valk, M. and Izadi, M.},
  title   = {Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML},
  journal = {arXiv},
  year    = {2025},
  note    = {arXiv:2509.12395},
  doi     = {10.48550/arXiv.2509.12395}
}

@inproceedings{10.3115/1073083.1073135,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: a method for automatic evaluation of machine translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}

@misc{ren2020codebleumethodautomaticevaluation,
      title={CodeBLEU: a Method for Automatic Evaluation of Code Synthesis}, 
      author={Shuo Ren and Daya Guo and Shuai Lu and Long Zhou and Shujie Liu and Duyu Tang and Neel Sundaresan and Ming Zhou and Ambrosio Blanco and Shuai Ma},
      year={2020},
      eprint={2009.10297},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2009.10297}, 
}

@inproceedings{10.1145/3551349.3556903,
author = {Eghbali, Aryaz and Pradel, Michael},
title = {CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556903},
doi = {10.1145/3551349.3556903},
abstract = {Recent years have brought a surge of work on predicting pieces of source code, e.g., for code completion, code migration, program repair, or translating natural language into code. All this work faces the challenge of evaluating the quality of a prediction w.r.t. some oracle, typically in the form of a reference solution. A common evaluation metric is the BLEU score, an n-gram-based metric originally proposed for evaluating natural language translation, but adopted in software engineering because it can be easily computed on any programming language and enables automated evaluation at scale. However, a key difference between natural and programming languages is that in the latter, completely unrelated pieces of code may have many common n-grams simply because of the syntactic verbosity and coding conventions of programming languages. We observe that these trivially shared n-grams hamper the ability of the metric to distinguish between truly similar code examples and code examples that are merely written in the same language. This paper presents CrystalBLEU, an evaluation metric based on BLEU, that allows for precisely and efficiently measuring the similarity of code. Our metric preserves the desirable properties of BLEU, such as being language-agnostic, able to handle incomplete or partially incorrect code, and efficient, while reducing the noise caused by trivially shared n-grams. We evaluate CrystalBLEU on two datasets from prior work and on a new, labeled dataset of semantically equivalent programs. Our results show that CrystalBLEU can distinguish similar from dissimilar code examples 1.9–4.5 times more effectively, when compared to the original BLEU score and a previously proposed variant of BLEU for code.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {28},
numpages = {12},
keywords = {BLEU, Evaluation, Metric},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3533767.3534405,
author = {Moussa, Rebecca and Sarro, Federica},
title = {On the use of evaluation measures for defect prediction studies},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534405},
doi = {10.1145/3533767.3534405},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {101–113},
numpages = {13},
keywords = {Software Defect Prediction, Evaluation Measures},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@article{10.1162/tacl_a_00675,
    author = {Opitz, Juri},
    title = {A Closer Look at Classification Evaluation Metrics and a Critical
                    Reflection of Common Evaluation Practice},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {12},
    pages = {820-836},
    year = {2024},
    month = {06},
    abstract = {Classification systems are evaluated in a countless number of papers. However, we
                    find that evaluation practice is often nebulous. Frequently, metrics are
                    selected without arguments, and blurry terminology invites misconceptions. For
                    instance, many works use so-called ‘macro’ metrics to rank systems
                    (e.g., ‘macro F1’) but do not clearly specify what they would
                    expect from such a ‘macro’ metric. This is problematic, since
                    picking a metric can affect research findings and thus any clarity in the
                    process should be maximized. Starting from the intuitive concepts of bias and prevalence, we perform an
                    analysis of common evaluation metrics. The analysis helps us understand the
                    metrics’ underlying properties, and how they align with expectations as
                    found expressed in papers. Then we reflect on the practical situation in the
                    field, and survey evaluation practice in recent shared tasks. We find that
                    metric selection is often not supported with convincing arguments, an issue that
                    can make a system ranking seem arbitrary. Our work aims at providing overview
                    and guidance for more informed and transparent metric selection, fostering
                    meaningful evaluation.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00675},
    url = {https://doi.org/10.1162/tacl_a_00675},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00675/2465598/tacl_a_00675.pdf},
}

@misc{lyu2024passimprovecodegeneration,
      title={Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking}, 
      author={Zhi-Cun Lyu and Xin-Ye Li and Zheng Xie and Ming Li},
      year={2024},
      eprint={2408.05715},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.05715}, 
}

@article{Hand2021,
  title = {F*: an interpretable transformation of the F-measure},
  volume = {110},
  ISSN = {1573-0565},
  url = {http://dx.doi.org/10.1007/s10994-021-05964-1},
  DOI = {10.1007/s10994-021-05964-1},
  number = {3},
  journal = {Machine Learning},
  publisher = {Springer Science and Business Media LLC},
  author = {Hand,  David J. and Christen,  Peter and Kirielle,  Nishadi},
  year = {2021},
  month = mar,
  pages = {451–456}
}

@article{https://doi.org/10.48550/arxiv.2003.01182,
  doi = {10.48550/ARXIV.2003.01182},
  url = {https://arxiv.org/abs/2003.01182},
  author = {Yao,  Jingxiu and Shepperd,  Martin},
  keywords = {Software Engineering (cs.SE),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Assessing Software Defection Prediction Performance: Why Using the Matthews Correlation Coefficient Matters},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2103.10201,
  doi = {10.48550/ARXIV.2103.10201},
  url = {https://arxiv.org/abs/2103.10201},
  author = {Yao,  Jingxiu and Shepperd,  Martin},
  keywords = {Software Engineering (cs.SE),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {The impact of using biased performance metrics on software defect prediction research},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{tantithamthavorn2018impactclassrebalancingtechniques,
      title={The Impact of Class Rebalancing Techniques on the Performance and Interpretation of Defect Prediction Models}, 
      author={Chakkrit Tantithamthavorn and Ahmed E. Hassan and Kenichi Matsumoto},
      year={2018},
      eprint={1801.10269},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/1801.10269}, 
}

@article{https://doi.org/10.48550/arxiv.2009.03612,
  doi = {10.48550/ARXIV.2009.03612},
  url = {https://arxiv.org/abs/2009.03612},
  author = {Wattanakriengkrai,  Supatsara and Thongtanunam,  Patanamon and Tantithamthavorn,  Chakkrit and Hata,  Hideaki and Matsumoto,  Kenichi},
  keywords = {Software Engineering (cs.SE),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Predicting Defective Lines Using a Model-Agnostic Technique},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2302.11370,
  doi = {10.48550/ARXIV.2302.11370},
  url = {https://arxiv.org/abs/2302.11370},
  author = {Diaz,  Fernando and Ekstrand,  Michael D. and Mitra,  Bhaskar},
  keywords = {Information Retrieval (cs.IR),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Recall,  Robustness,  and Lexicographic Evaluation},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{M2015,
  title = {A Review on Evaluation Metrics for Data Classification Evaluations},
  volume = {5},
  ISSN = {2230-9608},
  url = {http://dx.doi.org/10.5121/ijdkp.2015.5201},
  DOI = {10.5121/ijdkp.2015.5201},
  number = {2},
  journal = {International Journal of Data Mining \& Knowledge Management Process},
  publisher = {Academy and Industry Research Collaboration Center (AIRCC)},
  author = {M,  Hossin and M.N,  Sulaiman},
  year = {2015},
  month = mar,
  pages = {01–11}
}

@article{https://doi.org/10.48550/arxiv.2304.00059,
  doi = {10.48550/ARXIV.2304.00059},
  url = {https://arxiv.org/abs/2304.00059},
  author = {Beam,  Colin S.},
  keywords = {Methodology (stat.ME),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Resolving power: A general approach to compare the distinguishing ability of threshold-free evaluation metrics},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Kang2024,
  title = {ML-Based Software Defect Prediction in Embedded Software for Telecommunication Systems (Focusing on the Case of SAMSUNG ELECTRONICS)},
  volume = {13},
  ISSN = {2079-9292},
  url = {http://dx.doi.org/10.3390/electronics13091690},
  DOI = {10.3390/electronics13091690},
  number = {9},
  journal = {Electronics},
  publisher = {MDPI AG},
  author = {Kang,  Hongkoo and Do,  Sungryong},
  year = {2024},
  month = apr,
  pages = {1690}
}

@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    editor = "Goldstein, Jade  and
      Lavie, Alon  and
      Lin, Chin-Yew  and
      Voss, Clare",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909/",
    pages = "65--72"
}


@misc{Song2024Revisiting,
  doi = {10.48550/ARXIV.2404.08817},
  url = {https://arxiv.org/abs/2404.08817},
  author = {Song,  Yewei and Lothritz,  Cedric and Tang,  Daniel and Bissyandé,  Tegawendé F. and Klein,  Jacques},
  keywords = {Computation and Language (cs.CL),  Programming Languages (cs.PL),  Software Engineering (cs.SE),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{Montahaei2019Jointly,
  doi = {10.48550/ARXIV.1904.03971},
  url = {https://arxiv.org/abs/1904.03971},
  author = {Montahaei,  Ehsan and Alihosseini,  Danial and Baghshah,  Mahdieh Soleymani},
  keywords = {Machine Learning (cs.LG),  Computation and Language (cs.CL),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Jointly Measuring Diversity and Quality in Text Generation Models},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{Tevet2021Evaluating,
  doi = {10.48550/ARXIV.2004.02990},
  url = {https://arxiv.org/abs/2004.02990},
  author = {Tevet,  Guy and Berant,  Jonathan},
  keywords = {Computation and Language (cs.CL),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Evaluating the Evaluation of Diversity in Natural Language Generation},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@inproceedings{Sager2006Detecting,
author = {Sager, Tobias and Bernstein, Abraham and Pinzger, Martin and Kiefer, Christoph},
title = {Detecting similar Java classes using tree algorithms},
year = {2006},
isbn = {1595933972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1137983.1138000},
doi = {10.1145/1137983.1138000},
abstract = {Similarity analysis of source code is helpful during development to provide, for instance, better support for code reuse. Consider a development environment that analyzes code while typing and that suggests similar code examples or existing implementations from a source code repository. Mining software repositories by means of similarity measures enables and enforces reusing existing code and reduces the developing effort needed by creating a shared knowledge base of code fragments. In information retrieval similarity measures are often used to find documents similar to a given query document. This paper extends this idea to source code repositories. It introduces our approach to detect similar Java classes in software projects using tree similarity algorithms. We show how our approach allows to find similar Java classes based on an evaluation of three tree-based similarity measures in the context of five user-defined test cases as well as a preliminary software evolution analysis of a medium-sized Java project. Initial results of our technique indicate that it (1) is indeed useful to identify similar Java classes, (2)successfully identifies the ex ante and ex post versions of refactored classes, and (3) provides some interesting insights into within-version and between-version dependencies of classes within a Java project.},
booktitle = {Proceedings of the 2006 International Workshop on Mining Software Repositories},
pages = {65–71},
numpages = {7},
keywords = {tree similarity measures, software repositories, software evolution, change analysis},
location = {Shanghai, China},
series = {MSR '06}
}