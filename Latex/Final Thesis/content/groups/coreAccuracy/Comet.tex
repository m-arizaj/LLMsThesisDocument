\subsection{COMET}

\subsubsection*{Definition}
\textbf{COMET} (Crosslingual Optimized Metric for Evaluation of Translation) is an automatic evaluation metric for text generation. It is an \textbf{LLM-based metric}, meaning it leverages a pre-trained cross-lingual language model (like XLM-RoBERTa) to evaluate the quality of a generated text (hypothesis) by comparing it to a source text and a reference text.

Unlike traditional metrics that rely on n-gram overlap, COMET is trained to \textbf{predict human judgments} of quality, capturing semantic nuances more effectively.

\subsubsection*{Formula (General Idea)}
COMET does not use a simple, static statistical formula like BLEU. Instead, it is a neural framework.

The model takes a tuple $(s, h, r)$ consisting of:
\begin{itemize}
    \item $s$: Source text.
    \item $h$: Hypothesis (candidate text).
    \item $r$: Reference text.
\end{itemize}

These inputs are passed through a pre-trained encoder (e.g., XLM-R) to obtain vector representations. These embeddings are then combined and passed through a regression head to predict a quality score $\hat{y}$:

\begin{equation}
    \hat{y} = f_{\theta}(\text{Enc}(s), \text{Enc}(h), \text{Enc}(r))
\end{equation}

\noindent where $f_{\theta}$ is the trained estimator that minimizes the error against human quality scores.

\subsubsection*{Purpose}
The primary purpose is to provide an evaluation metric that correlates highly with human judgment, moving beyond surface-level matching to leverage the deep semantic understanding of LLMs.

\subsubsection*{Domains}
\begin{itemize}
    \item Machine Translation (MT)
    \item Natural Language Generation (NLG)
    \item LLM Evaluation
    \item Software Engineering (as a baseline)
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{High Correlation with Humans}: Specifically trained to replicate human assessments.
    \item \textbf{Semantic Understanding}: Captures nuances and meaning better than surface-level overlap metrics.
    \item \textbf{Cross-lingual}: Effective for evaluating translation due to its cross-lingual pre-training.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Reference Dependency}: Requires high-quality reference texts, which can be expensive or unavailable.
    \item \textbf{Poor Functional Correlation}: In code generation, it shows ``extremely weak correlation'' with functional correctness (e.g., only $r_p \approx 0.12$ on APPS-Eval).
    \item \textbf{Computational Cost}: More expensive to run than statistical metrics like BLEU.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Lin2024SWC,
Dong2023CodeScore,
rei2020comet,
}
\fussy

