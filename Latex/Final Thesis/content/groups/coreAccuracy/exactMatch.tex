\subsection{Exact Match}

\subsubsection{Introduction}

Exact Match (EM) is a discrete evaluation metric that measures the percentage of model outputs that exactly match a reference target. It is widely used in Software Engineering and code generation benchmarks as a strict indicator of functional or syntactic correctness. A prediction is considered correct only if it is identical to the ground truth, character by character or token by token.
In code generation, this metric is employed to quantify whether generated code reproduces the same solution as the reference implementation. Although EM provides high interpretability and objectivity, it is highly sensitive to formatting, tokenization, and naming variations that do not affect semantic meaning. \cite{Chen2024SurveyCodeGen}

\subsubsection{Mathematical Definition}

The metric is defined as:

\[
\text{Exact Match} = \frac{\text{Number of exactly matching predictions}}{\text{Total number of predictions}}
\]

That is, the ratio of predictions that are identical to the reference output among all evaluated samples.
It yields a binary outcome per instance ($1 =$ exact match, $0 =$ otherwise), averaged over the dataset.

\subsubsection{Variants}

Several extensions of Exact Match have emerged to handle structural or semantic differences in Software Engineering tasks:

\textbf{1. Exact Match (Standard).}
Used as the default accuracy metric for benchmarks like \textbf{HellaSwag}, \textbf{OpenBookQA}, \textbf{MMLU}, and \textbf{BLiMP}. In code completion contexts, it measures strict equality between predicted and reference code outputs, ensuring identity in syntax and structure. \cite{Liang2022HELM}

\textbf{2. Fuzzy Match.}
Allows small deviations like whitespace or minor tokenization changes. Applied in \emph{OpenAI Evals} for automatic evaluation of near-identical text outputs. \cite{Guo2023EvalLLMs}

\textbf{3. Quasi-Exact Match.}
This variant expands the strict exact match condition by allowing for identical strings up to some slight post-processing of the model generation, such as lower-casing, removing whitespace, punctuation, or articles. It serves as the default accuracy metric for benchmarks including \textbf{BoolQ}, synthetic reasoning (abstract symbols) and \textbf{DataImputation}. \cite{Liang2022HELM}

\textbf{4. Single-line Exact Match}
Introduced by Fried et al. (2022) for fill-in-the-middle evaluations, this metric assesses a model's ability to reconstruct a specific masked line of code within a function body. The process involves masking out a single line from the function body—explicitly excluding function descriptions, signatures, blank lines, and comments—and prompting the model to fill in that line. Success is measured by counting the frequency with which the model produces \textbf{exactly the masked-out line}. 
Notably, in benchmarks like \emph{MultiPL-E} where hand-written solutions may not be available for every problem, this metric often relies on reference solutions generated by high-performance models rather than human authors. \cite{Allal2023SantaCoder}

\textbf{5. Syntax Match.}
Identified as a core component of \emph{CodeBLEU} (Ren et al., 2020), this metric depicts \textbf{syntax similarity} rather than just surface-level n-gram matching. 
It is typically weighted alongside data-flow match (which captures semantic equivalence) to achieve a stronger correlation with human evaluation scores in code generation tasks. \cite{ren2020codebleumethodautomaticevaluation, Le2024OneShotCorrection}

\subsubsection{Applications in Software Engineering}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{|l|l|X|}
\hline
\textbf{Context} & \textbf{Example Benchmarks/Datasets} & \textbf{Purpose} \\ \hline
\textbf{Code Generation / Repair} & CodeXGLUE, MBPP, Bugs2Fix & Measures exact correctness of generated or repaired code \\ \hline
\textbf{Functional Correctness} & HumanEval, CONCODE & Evaluates logical equivalence under strict matching \\ \hline
\textbf{Reasoning / QA} & GSM8K, Dyck & Evaluates precision in reasoning and problem-solving LLMs \\ \hline
\textbf{Automatic Evaluation} & OpenAI Evals & Directly computes exact or fuzzy correctness without human intervention \\ \hline
\textbf{Cross-Modal Similarity} & Plot2Code & Links text-to-code matching quality \\ \hline
\end{tabularx}
\caption{Applications of Exact Match in Software Engineering and LLM Evaluation}
\end{table}

\subsubsection{Interpretation and Limitations}

While Exact Match is a clear and reproducible metric, it often underestimates functional performance in code generation since even a semantically equivalent solution with different variable names or formatting yields 0 score.
Therefore, EM is best used in combination with structural, semantic, or execution-based metrics such as \emph{CodeBLEU} and \emph{Pass@k}.
Its strict nature, however, makes it ideal for benchmark comparisons and regression testing in code generation pipelines.

\subsubsection{Additional References}

This metric and its variants are referenced and/or used in the following papers:

\cite{Bektas2025CriticalReview, Liang2022HELM, Guo2023EvalLLMs, Hou2023LLMsSESLR, Chang2023SurveyLLMs, Chen2024SurveyCodeGen, Srivastava2022BeyondImitation, Evtikhiev2023OutOfBLEU, Yang2024CodeScoreR, Lu2021CodeXGLUE, Wang2021CodeT5, Allal2023SantaCoder, Le2024OneShotCorrection}