\subsection{Edit Distance}

\subsubsection{Introduction}

Edit Distance (also known as Levenshtein Distance) is a string-based similarity metric that measures how many basic operations are required to transform one sequence into another. These operations typically include insertions, deletions, and substitutions of characters or tokens.
In Software Engineering contexts, this metric has evolved to capture structural and syntactic differences between code fragments, enabling its use in tasks like code similarity evaluation, bug repair, and program synthesis. The foundational concept was later expanded to Abstract Syntax Tree Edit Distance (AST-ED), which better models code transformations at the structural level rather than pure token sequences \cite{Song2024Revisiting}. 
This adaptation is described in detail in Song et al. (2024), showing its high correlation with human judgment in code similarity assessments \cite{Song2024Revisiting}.

\subsubsection{Formula}

For two strings (or token sequences) $a$ and $b$ with lengths $|a|$ and $|b|$, the Levenshtein Distance $D(i, j)$ is defined recursively as:

\[
D(i, j) =
\begin{cases}
0 & \text{if } i = 0 \text{ and } j = 0 \\
i & \text{if } j = 0 \\
j & \text{if } i = 0 \\
\min
\begin{cases}
D(i-1, j) + 1 \\
D(i, j-1) + 1 \\
D(i-1, j-1) + [a_i \neq b_j]
\end{cases}
\end{cases}
\]

Where:
\begin{itemize}
    \item $D(i, j)$ represents the minimal number of operations to convert the first $i$ tokens of $a$ into the first $j$ tokens of $b$.
    \item $[a_i \neq b_j]$ equals 0 if the tokens are identical and 1 otherwise.
\end{itemize}

To normalize the score for similarity, it is often converted into a similarity ratio \cite{Song2024Revisiting}:

\[
\text{Similarity} = 1 - \frac{D(a,b)}{\max(|a|, |b|)}
\]

This makes it easier to compare across code samples of different lengths.

\subsubsection{Variants and Adaptations}

\begin{table}[H]
\centering
\caption{Summary of Edit Distance variants and their applications in Software Engineering.}
\begin{tabularx}{\textwidth}{|X|X|X|}
\hline
\textbf{Variant} & \textbf{Description} & \textbf{Context of Use} \\ \hline
Levenshtein Distance & Measures token-level edit operations (insert, delete, substitute). & CodeXGLUE, Code Optimization / Education \\ \hline
AST Edit Distance (AST-ED) & Computes structural transformations between Abstract Syntax Trees to assess syntactic accuracy \cite{Song2024Revisiting}. & CodeXGLUE, AlignJudge, General SE Evaluation \\ \hline
Normalized Edit Distance & Converts raw edit distance into a similarity score between 0 and 1 \cite{Song2024Revisiting}. & Used for comparing outputs of different code generation systems. \\ \hline
Diversity (Levenshtein) & Measures the average pairwise edit distance between multiple generated outputs to assess output variety \cite{Tevet2021Evaluating, Montahaei2019Jointly}. & FuzzBench \\ \hline
\end{tabularx}
\end{table}

These adaptations improve the metric’s ability to assess syntactic correctness, structural similarity, and functional proximity in generated code.

\subsubsection{AST-based Extensions}

A major structural adaptation of Edit Distance for code evaluation is the Abstract Syntax Tree Edit Distance (AST-ED), which operates not at the token level but on syntactic structures represented as trees \cite{Song2024Revisiting}.
Instead of comparing raw text or sequences of tokens, AST-ED analyzes tree transformations required to convert one program’s structure into another, making it better suited for measuring semantic and structural correctness in code. Specifically, tree algorithms allow identifying similar classes or refactorings in projects \cite{Sager2006Detecting}.

Formally, if two programs are represented as trees $T_1$ and $T_2$, the AST Edit Distance is defined as:

\[
\text{AST-ED}(T_1, T_2) = \min_{\text{operations}} \sum_{k} \text{cost}(op_k)
\]

where each $op_k$ represents an edit operation (insert, delete, rename, or move) applied to nodes in the syntax tree.
The cost function may assign different weights depending on the operation type, reflecting its syntactic or semantic significance \cite{Sager2006Detecting}.

This approach captures both syntactic accuracy (structural alignment of code elements) and semantic similarity (functional equivalence at the statement or block level).
Because of this, AST-ED correlates strongly with human judgment when evaluating the similarity between code snippets that differ only in surface syntax but preserve functional behavior (Song et al., 2024) \cite{Song2024Revisiting}.

\textbf{Subtree and Structural Variants}
\begin{itemize}
    \item \textbf{AST Sub-tree Matching:} Compares substructures within the AST to assess partial alignment between code fragments, useful in evaluating output quality or localized code transformations \cite{Sager2006Detecting}.
    \item \textbf{AST-based Metrics (General):} Combine node-level and sub-tree comparisons to compute a weighted similarity score between program structures. Used to evaluate structural and semantic accuracy in code generation and translation benchmarks \cite{Song2024Revisiting}.
\end{itemize}

\textbf{Relevance in Software Engineering}
\begin{itemize}
    \item \textbf{Code Similarity Evaluation:} Identifying functionally similar code with different syntax or style \cite{Song2024Revisiting}.
    \item \textbf{Program Repair Assessment:} Quantifying the minimal syntactic change needed to fix a bug.
    \item \textbf{Refactoring and Migration Tasks:} Evaluating how structurally close transformed code is to the intended target \cite{Sager2006Detecting}.
    \item \textbf{Model Evaluation:} Measuring structure-preserving accuracy in LLM-generated code beyond textual overlap.
\end{itemize}

\subsubsection{Diversity (Levenshtein Distance)}

A specialized variant that quantifies how diverse model outputs are in generation tasks.
It is defined as the mean pairwise Levenshtein Distance among all generated samples. While metrics like Self-BLEU are often used, pairwise edit distance provides a direct measure of structural variation \cite{Montahaei2019Jointly, Tevet2021Evaluating}:

\[
\text{Diversity} = \frac{1}{N(N-1)} \sum_{i \neq j} \text{EditDistance}(\text{output}_i, \text{output}_j)
\]

This formulation reflects the Seed Quality or Output Variety, applicable in fuzzing and LLM evaluation contexts where establishing a rigorous evaluation of diversity is critical \cite{Tevet2021Evaluating}.
A higher diversity score indicates a broader exploration of the solution space.

\subsubsection{Interpretation}

In software engineering, Edit Distance quantifies how much modification is required for a generated code snippet to match a reference implementation. A lower distance (or higher similarity) indicates that the generated code closely aligns with the reference in both syntax and structure \cite{Song2024Revisiting}.

However, Edit Distance has known limitations:
\begin{itemize}
    \item It does not account for semantic equivalence, meaning two functionally identical programs might still differ syntactically \cite{Song2024Revisiting}.
    \item It is sensitive to code formatting or variable naming differences unless structural variants like AST-ED are used \cite{Sager2006Detecting}.
\end{itemize}

Despite this, Edit Distance remains a foundational metric for tasks such as code repair evaluation, plagiarism detection, and LLM-generated code assessment.

\textbf{Diversity as a Complementary Perspective}

A modern extension of this idea is Diversity, which shifts focus from code-to-reference similarity to code-to-code variety \cite{Montahaei2019Jointly}.
Instead of comparing a single model output against a ground truth, this variant measures the average pairwise edit distance among multiple generated outputs:
\begin{itemize}
    \item \textbf{High diversity} implies the model explores a wide range of syntactic and structural possibilities, producing varied code solutions from different seeds or prompts \cite{Tevet2021Evaluating}.
    \item \textbf{Low diversity} indicates repetitiveness or mode collapse, where outputs converge on nearly identical solutions.
\end{itemize}

In practice, Edit Distance and Diversity are often used together:
\begin{itemize}
    \item \textbf{Edit Distance} $\rightarrow$ measures closeness to a reference (accuracy, precision).
    \item \textbf{Diversity} $\rightarrow$ measures spread among generated outputs (creativity, exploration) \cite{Montahaei2019Jointly}.
\end{itemize}

\subsubsection{Additional References}

This metric and its variants are referenced and/or used in the following papers:

\cite{Bektas2025CriticalReview, Hou2023LLMsSESLR, Hu2025BenchmarksSE, Chen2024SurveyCodeGen, Paul2024BenchmarksMetricsCodeGen, Yang2024CodeScoreR, Xu2025LLMAgentsToolLearning, Rong2025LLMOptimizationEducation, Sagodi2024CodeSynthesisEvaluation, Black2024LLMFuzzing}