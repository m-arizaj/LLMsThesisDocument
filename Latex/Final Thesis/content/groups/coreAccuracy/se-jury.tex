\subsection{SE-Jury Score}

SE-Jury Score is an LLM-as-an-Ensemble-Judge evaluation metric introduced by Zhou et al. (2025) \cite{Zhou2025LLMAsJudge} to bridge the gap between human evaluation and automatic metrics in software engineering tasks such as code generation, automatic program repair, and code summarization. 
Unlike surface-level metrics like BLEU and ROUGE or execution-based ones like Pass@k, SE-Jury employs an ensemble of reasoning-based LLM evaluators to assess generated artifacts in terms of semantic correctness, functionality, and alignment with human expectations.

The SE-Jury score is defined as:

\begin{equation}
\text{SE-Jury}(x, y, r) = \frac{1}{|T|} \sum_{p_i \in T} LLM_{p_i}(x, y, r)
\end{equation}

Where:
\begin{itemize}
    \item $x$ is the task prompt or problem statement.
    \item $y$ is the generated output (e.g., code, summary).
    \item $r$ is the human reference implementation.
    \item $T$ is the subset of evaluation strategies selected through dynamic team formation.
    \item $LLM_{p_i}$ is the score returned by the $i$-th LLM-based judge.
\end{itemize}

Each judge outputs a normalized score (0–100), which is linearly rescaled to match dataset-specific ranges (e.g., 1–5 or 0–4) when required.

\subsubsection{Variants}

SE-Jury integrates five complementary evaluation strategies:

\begin{enumerate}
    \item \textbf{Direct Assess (S1):} The LLM directly rates the functional correctness of the generated code.
    \item \textbf{Direct Assess + Rethink (S2):} Adds a self-reflective reasoning pass before scoring.
    \item \textbf{Equivalence Assess (S3):} Judges semantic equivalence between generated and reference implementations.
    \item \textbf{Generate Tests and Assess (S4):} Produces test cases and verifies whether the generated code passes them.
    \item \textbf{Analyze Reference and Assess (S5):} Extracts core behavioral properties from the reference solution and checks their presence in the generated output.
\end{enumerate}

A \textit{Dynamic Team Formation} stage selects the subset of strategies that maximizes correlation with human ratings (using Kendall $\tau$, Spearman $\rho$, and Pearson $r$).
The final SE-Jury score is computed as the average of the selected judges.

\subsubsection{Interpretation}

In software engineering, SE-Jury provides a semantically grounded evaluation of code correctness and quality, aligning more closely with human assessment practices.

\begin{itemize}
    \item It captures functional and semantic equivalence beyond lexical similarity.
    \item It integrates diverse reasoning processes to approximate human judgment.
    \item It achieves strong correlation with human evaluators across code generation, repair, and summarization tasks.
    \item It remains applicable even when execution-based metrics cannot be used (e.g., documentation, static code, or non-runnable snippets).
\end{itemize}

However, it has certain limitations:
\begin{itemize}
    \item \textbf{Model dependence:} Results vary with the LLM used as judge (GPT-4o-mini in the original experiments).
    \item \textbf{Computational overhead:} Multiple reasoning passes increase cost.
    \item \textbf{Bias inheritance:} The LLM’s intrinsic biases may propagate into scores.
    \item \textbf{Limited non-functional assessment:} The metric focuses on correctness, not efficiency, readability, or maintainability.
\end{itemize}