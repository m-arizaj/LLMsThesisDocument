\subsection{BERTScore}

\subsubsection{Introduction}

BERTScore is an embedding-based evaluation metric that measures the similarity between a generated text and a reference by leveraging contextual embeddings from pretrained models like BERT \cite{Zhang2020BERTScore}.
Instead of relying on surface-level token overlap (as BLEU or ROUGE do), BERTScore computes semantic similarity between words or code tokens in their contextual vector space.

In Software Engineering, BERTScore and its specialized variant CodeBERTScore are widely used for evaluating code generation, summarization, and translation tasks. These metrics capture semantic correctness, contextual similarity, and embedding-based alignment between generated and reference code.

\subsubsection{Formula}

BERTScore is computed using cosine similarity between contextual embeddings of candidate and reference tokens \cite{Zhang2020BERTScore}:

\[
\text{BERTScore}_{P,R,F_1} = 
\begin{cases}
P = \frac{1}{|X|} \sum_{x \in X} \max_{y \in Y} \cos(e(x), e(y)) \\
R = \frac{1}{|Y|} \sum_{y \in Y} \max_{x \in X} \cos(e(x), e(y)) \\
F_1 = \frac{2PR}{P + R}
\end{cases}
\]

Where:
\begin{itemize}
    \item $X$ and $Y$ represent the sets of tokens in the candidate and reference texts, respectively.
    \item $e(x)$ and $e(y)$ are contextual embeddings (e.g., from BERT or CodeBERT).
    \item Cosine similarity measures semantic closeness between embeddings.
\end{itemize}

The metric produces Precision (P), Recall (R), and F1 variants, depending on whether the focus is on generated-token coverage, reference coverage, or their harmonic mean \cite{Zhang2020BERTScore}.

\subsubsection{Variants}

\textbf{BERTScore (Standard)}
The base version measures token-level embedding similarity using pretrained BERT models. It is often applied in natural language processing (NLP) and natural language generation (NLG) tasks, where meaning preservation is more important than exact lexical matching \cite{Zhang2020BERTScore}.

\textbf{CodeBERTScore}
An adaptation for programming languages proposed by Zhou et al. (2023) \cite{Zhou2023CodeBERTScore}.
It employs CodeBERT embeddings trained jointly on natural and programming languages, making it better suited for evaluating code generation, translation, and documentation synthesis tasks. Unlike BLEU or ROUGE, CodeBERTScore captures the semantic consistency and syntactic integrity of the generated code.

\textbf{CodeBERTScore (F1, F3)}
These weighted variants balance or prioritize recall:
\begin{itemize}
    \item \textbf{F1} provides a harmonic balance between precision and recall.
    \item \textbf{F3} emphasizes recall, rewarding outputs that capture more of the reference semantics even if syntactic precision is lower.
\end{itemize}
Such adjustments are particularly relevant in tasks where functional correctness is the main concern.

\textbf{Recall-weighted and LLM-based Extensions}
Recent studies in LLM evaluation have extended the metric to recall-weighted embedding similarity and LLM-based embedding matching, where embeddings are obtained from large models.
These approaches aim to align metric behavior more closely with human judgments of functionality and logical soundness.

\subsubsection{Applications in Software Engineering}

BERTScore and its extensions have been widely applied across SE-related evaluations:

\begin{itemize}
    \item In LLM evaluation tasks such as GEM, GLGE, and CLUE, BERTScore is used as an automatic semantic evaluation metric for text-based reasoning or natural language understanding.
    \item Within software engineering benchmarks such as CoNaLa, Card2Code, APR-Assess, and Summary-Assess, it quantifies embedding-based similarity between generated and reference code.
    \item In LLM-based functional correctness studies, BERTScore complements metrics like Exact Match by providing LLM-based embedding match (EM) scores that reflect semantic equivalence.
    \item CodeBERTScore, specifically, is used to assess the semantic and structural similarity of generated code in tasks like code translation, generation, and summarization \cite{Zhou2023CodeBERTScore}.
    \item Variants such as F1 and F3 have been evaluated on HumanEval and CoNaLa, demonstrating improved sensitivity to code semantics and recall-weighted meaning preservation.
\end{itemize}

Overall, BERTScore offers a flexible framework for evaluating embedding-based similarity, while CodeBERTScore extends this capability to cross-lingual code understanding and LLM evaluation scenarios.

\subsubsection{Interpretation}

In software engineering, BERTScore has become an essential bridge between surface-level metrics and semantic understanding.
Its primary advantages include:

\begin{itemize}
    \item \textbf{Contextual comprehension:} Goes beyond literal matching by analyzing token meaning in context, crucial for tasks like code summarization or translation \cite{Hanna2021FineGrained}.
    \item \textbf{Tolerance to variation:} Robust to stylistic or lexical changes that donâ€™t affect functionality like variable renaming or reordering.
    \item \textbf{High correlation with human evaluation:} Especially for code generation tasks when paired with CodeBERT or LLM-based embeddings \cite{Zhang2020BERTScore}.
\end{itemize}

However, BERTScore also has limitations, as noted in fine-grained analyses \cite{Hanna2021FineGrained}:
\begin{itemize}
    \item It can inflate scores for semantically incorrect but lexically similar code, being less sensitive to small but critical errors.
    \item It depends on the pretraining corpus of the chosen embedding model (e.g., BERT, CodeBERT, or GPT-based encoders).
    \item It is computationally heavier than token-based alternatives.
\end{itemize}

In practice, BERTScore and CodeBERTScore are most effective when combined with execution-based metrics like Pass@k or Test Pass Rate, providing a more holistic view of both semantic and functional model performance.

\subsubsection{Additional References}
This metric and its variants are referenced and/or used in the following papers:

\cite{Lin2024SWC, HuZhou2024LLMMetrics, Zhou2025LLMAsJudge, Zhuo2023ICEScore, Zhou2023CodeBERTScore, Evtikhiev2023OutOfBLEU, Yang2024CodeScoreR, Dong2023CodeScore, Tong2024CodeJudge}