\subsection{METEOR}

METEOR (Metric for Evaluation of Translation with Explicit ORdering) is an automatic evaluation metric originally proposed for machine translation to address weaknesses in BLEU, specifically its lack of recall and insufficient penalty for sentence structure anomalies \cite{banerjee-lavie-2005-meteor}.
Unlike BLEU, which focuses primarily on precision, METEOR relies on unigram matching (based on surface forms, stemming, and synonymy) to calculate a score that combines both precision and recall, demonstrating a higher correlation with human judgment \cite{banerjee-lavie-2005-meteor}.

In the context of Large Language Models (LLMs), METEOR has been adapted to evaluate various generation tasks, quantifying the semantic alignment between generated text and references \cite{Zhuo2023ICEScore}.

According to Banerjee \& Lavie (2005) \cite{banerjee-lavie-2005-meteor}, METEOR computes a parameterized harmonic mean of unigram precision and recall.

First, unigrams are matched between the candidate and the reference. Then, precision ($P$) and recall ($R$) are calculated:
\begin{itemize}
    \item $P = \frac{m}{w_t}$ (where $m$ is the number of matched unigrams and $w_t$ is the total unigrams in the candidate translation).
    \item $R = \frac{m}{w_r}$ (where $w_r$ is the total unigrams in the reference translation).
\end{itemize}

The harmonic mean $F_{mean}$ emphasizes recall through a weight parameter $\alpha$:

\begin{equation}
F_{mean} = \frac{P \cdot R}{\alpha \cdot P + (1 - \alpha) \cdot R}
\end{equation}

(Typically, $\alpha = 0.9$ is used, which makes recall 9 times more important than precision).

To account for word order and structure, a fragmentation penalty ($Pen$) is applied. This penalty is based on the number of "chunks" ($ch$)—contiguous sequences of matched unigrams—relative to the total number of matches ($m$):

\begin{equation}
Pen = \gamma \cdot \left(\frac{ch}{m}\right)^\beta
\end{equation}

(Default values are usually $\gamma = 0.5$ and $\beta = 3$).

The final METEOR score is calculated as \cite{banerjee-lavie-2005-meteor}:

\begin{equation}
\text{METEOR} = F_{mean} \cdot (1 - Pen)
\end{equation}

\subsubsection{Interpretation}

METEOR provides a rigorous measure of unigram overlap that is linguistically aware, making it more robust than metrics relying solely on exact string matching \cite{banerjee-lavie-2005-meteor}.
However, in the specific domain of LLM evaluation, while metrics like METEOR and BLEU are standard, they focus on lexical similarity. Recent surveys suggest that for tasks requiring deep semantic understanding or code generation, these n-gram based metrics should often be complemented by model-based evaluations to fully capture functional correctness \cite{Zhuo2023ICEScore}.

\subsubsection{Additional References}
This metric and its variants are referenced and/or used in the following papers:

\cite{Lin2024SWC, HuZhou2024LLMMetrics, Hou2023LLMsSESLR, Hu2025BenchmarksSE, Zhou2025LLMAsJudge, Zhuo2023ICEScore, Anand2024AnalysisLLMCode, Paul2024BenchmarksMetricsCodeGen, Evtikhiev2023OutOfBLEU}