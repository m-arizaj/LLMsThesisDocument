\subsection{CodeScore}

CodeScore is an execution-informed evaluation metric designed to measure the functional correctness of generated code without requiring direct code execution. It was introduced in the paper \textit{CodeScore: Evaluating Code Generation by Learning Code Execution} (Dong et al., 2023) \cite{Dong2023CodeScore} as a learned model-based proxy for traditional execution-based metrics. Unlike BLEU or CodeBLEU, which rely on textual overlap, CodeScore uses a trained model to infer how well generated code would perform on hidden test cases by learning representations of execution behavior \cite{Dong2023CodeScore}.

The metric outputs a continuous score between 0 and 1 that reflects both executability and behavioral similarity between generated and reference code. It has been widely adopted in software engineering evaluation benchmarks, offering a balance between reliability and computational efficiency.

The CodeScore model (implemented within the UniCE framework) predicts two key quantities \cite{Dong2023CodeScore}:
\begin{itemize}
    \item \textbf{Executability (E):} Whether the generated code can run successfully.
    \item \textbf{PassRatio (P):} The proportion of test cases passed.
\end{itemize}

The training loss is defined as:

\begin{equation}
L = L_C + L_E
\end{equation}

Where:
\begin{itemize}
    \item $L_C = (\text{CodeScore} - \text{PassRatio})^2$ penalizes deviation from observed test outcomes.
    \item $L_E = -\log p(E)$ is a binary cross-entropy loss predicting code executability.
\end{itemize}

The model outputs a scalar CodeScore value:

\begin{equation}
\text{CodeScore} = f(\text{generated\_code}, \text{reference\_code}, \text{task\_description})
\end{equation}

which approximates the true execution-based correctness without actually executing the code.

\subsubsection{Variants}

\begin{itemize}
    \item \textbf{CodeScore:} Standard model-based evaluation score predicting execution correctness across benchmarks like APPS-Eval and MBPP-Eval \cite{Dong2023CodeScore}.
    \item \textbf{CodeScore-R:} A robustness-oriented extension that evaluates consistency and resilience under perturbations (e.g., renamed variables, reordered code). It focuses on the stability of the model’s evaluation when faced with minor syntactic variations in the input \cite{Yang2024CodeScoreR}.
\end{itemize}

\subsubsection{Application in Software Engineering}

CodeScore and its variants are primarily used to assess LLM-generated code across multiple software engineering tasks such as code completion, repair, and migration. They are valuable when running large-scale test execution is impractical due to cost or sandboxing limitations.

Applications include:
\begin{itemize}
    \item \textbf{LLM-based Evaluation (CodeScore):} Estimating correctness of code generated by language models without executing test cases \cite{Dong2023CodeScore}.
    \item \textbf{Robustness / Functional Correctness (CodeScore-R):} Assessing reliability of model-generated code under code perturbations, ensuring that evaluations remain consistent even when surface syntax changes \cite{Yang2024CodeScoreR}.
\end{itemize}

In benchmarks like HumanEval, HumanEval-X, AVATAR, and UniCE’s evaluation suite, CodeScore-R has demonstrated stronger alignment with functional correctness than text-based metrics and higher robustness to input noise \cite{Yang2024CodeScoreR}.

\subsubsection{Interpretation}

\begin{itemize}
    \item \textbf{High CodeScore or CodeScore-R values (close to 1)} indicate code that is executable, semantically correct, and consistent across small perturbations \cite{Yang2024CodeScoreR}.
    \item \textbf{Moderate values} may correspond to partially correct or non-executable but syntactically plausible code.
    \item \textbf{Low scores} reflect code that fails execution or deviates significantly from the intended task behavior.
\end{itemize}

Overall, CodeScore offers an efficient and generalizable approach to evaluating code generation quality without relying solely on textual or exact-match metrics \cite{Dong2023CodeScore}.