\subsection{Security Analysis Score (SAS)}

\subsubsection*{Definition}
The \textbf{Security Analysis Score (SAS)} is a metric used to evaluate the security posture of code, typically generated by a Large Language Model (LLM). It provides a quantitative vulnerability assessment based on established industry frameworks, most notably the \textbf{OWASP Top 10} (Open Web Application Security Project).

The score specifically evaluates the presence or absence of common, critical security issues, such as:
\begin{itemize}
    \item SQL Injection
    \item Cross-Site Scripting (XSS)
    \item Buffer Overflows
    \item Insecure Cryptographic Practices
\end{itemize}

\subsubsection*{Calculation (General Idea)}
Unlike metrics with a simple arithmetic formula, the SAS is derived from the output of \textbf{Static Analysis Security Testing (SAST)} tools.

The calculation is generally based on the \textbf{identification, count, and severity classification} of vulnerabilities found in the code.
\begin{itemize}
    \item \textbf{Higher Score}: Indicates fewer or less severe vulnerabilities (better security).
    \item \textbf{Lower Score}: Indicates significant security risks.
\end{itemize}

It acts as a key component of the ``Code Quality Assessment'' dimension in modern software engineering benchmarks like LoCoBench.

\subsubsection*{Purpose}
The primary purpose of the SAS is to move evaluation beyond mere functional correctness and assess the \textbf{safety and robustness} of generated code. It quantifies a model's ability to produce code that is not only functional but also secure against common attack vectors, a critical aspect of real-world software engineering.

\subsubsection*{Domains}
\begin{itemize}
    \item Software Engineering
    \item Long-Context Code Evaluation
    \item Code Quality Assessment
    \item Security Vulnerability Assessment
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Real-World Relevance}: Measures a critical aspect of code quality (security) often overlooked by metrics focused only on correctness or similarity.
    \item \textbf{Standardized}: Basing the assessment on frameworks like OWASP provides a well-understood and widely accepted basis for evaluation.
    \item \textbf{Actionable}: Provides specific feedback on security flaws (e.g., ``SQL injection found''), enabling targeted model improvement.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Tool-Dependent}: The score's accuracy is highly dependent on the underlying static analysis tools used.
    \item \textbf{False Positives/Negatives}: Static analysis may misidentify issues or miss complex, logic-based vulnerabilities that require dynamic execution.
    \item \textbf{Scope}: Primarily focused on common vulnerabilities (OWASP Top 10) and may not cover novel or highly domain-specific flaws.
\end{itemize}

% a√±adir esta entrada al archivo .bib
% @article{qiu2025locobench,
%   title={LoCoBench: A benchmark for long-context large language models in complex software engineering},
%   author={Qiu, J. and Liu, Z. and Liu, Z. and Murthy, R. and Zhang, J. and Chen, H. and others},
%   journal={arXiv preprint arXiv:2509.09614},
%   year={2025},
%   doi={10.48550/arXiv.2509.09614}
% }