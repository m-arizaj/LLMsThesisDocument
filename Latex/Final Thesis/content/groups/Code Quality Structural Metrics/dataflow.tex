\subsection{Data-Flow Match}

Data-Flow Match evaluates how closely the semantic data-flow structure of generated code aligns with a reference implementation. Unlike surface-level similarity metrics (BLEU, ROUGE) or pure syntactic metrics (AST match), data-flow match analyzes how variables are defined, propagate, and interact across operations.

Loosely, it measures whether the generated program preserves the logical computation performed by the reference program.

A recent survey on LLM code evaluation highlights that data-flow analysis provides a deeper semantic view of similarity:

\begin{quote}
“Semantic data flow matching evaluates the semantic similarity of code by analyzing the data flow diagrams within the code… helping CodeBLEU deeply understand the internal logic and functionality.” \cite{Chen2024SurveyCodeGen}
\end{quote}

Additionally, the one-shot correction study emphasizes that correct repairs must preserve the data-flow of variables; failure to do so implies incorrect behavior even if the syntax looks valid:

\begin{quote}
“Enhancement… involves preserving the data-flow of each variable… and bridging the data-flow gap between code snippets.” \cite{Le2024OneShotCorrection}
\end{quote}

Together, these show that Data-Flow Match acts as a static semantic metric approximating functional correctness.

The CodeBLEU framework does not publish a single closed-form formula specifically named “Data-Flow Match,” but its computation is conceptually defined as comparing the sets of data-flow edges (definition–use relations) extracted from reference code $C_{\text{ref}}$ and generated code $C_{\text{gen}}$ \cite{Chen2024SurveyCodeGen}.

Let:
\begin{itemize}
    \item $DF(C)$ = set of data-flow edges in code $C$
    \item $e = (v_{\text{def}}, v_{\text{use}})$ represent a def–use pair
\end{itemize}

A common abstract formulation is:

\begin{equation}
\text{DataFlowMatch}(C_{\text{gen}}, C_{\text{ref}})
=
\frac{| DF(C_{\text{gen}}) \cap DF(C_{\text{ref}}) |}
{| DF(C_{\text{ref}}) |}
\end{equation}
        
This captures:
\begin{itemize}
    \item how many reference data-flow relations appear in generated code,
    \item normalized by the total number of data-flow relationships in the reference.
\end{itemize}

This representation is consistent with the survey’s description of “matching data-flow diagrams” as part of semantic similarity and with typical CodeBLEU-style implementations \cite{Chen2024SurveyCodeGen}.

\subsubsection{Variants}

\textbf{1. CodeBLEU Data-Flow Component} \\
CodeBLEU uses Data-Flow Match as one of its semantic submetrics:
\begin{itemize}
    \item matches variable dependencies
    \item compares definition–use chains
    \item checks whether generated programs preserve semantic execution paths
\end{itemize}
It is combined with n-gram match, AST match, and other components into a single CodeBLEU score \cite{Chen2024SurveyCodeGen}.

\textbf{2. General Data-Flow Similarity Metrics} \\
The survey notes that data-flow analysis is broadly applicable beyond CodeBLEU:
\begin{quote}
“Data flow analysis assesses code quality by comparing the similarity of data flows between generated and reference code… providing a deeper understanding of the code’s semantics.” \cite{Chen2024SurveyCodeGen}
\end{quote}
Variants include:
\begin{itemize}
    \item strict edge matching
    \item graph similarity between data-flow graphs
    \item variable-level flow preservation scores
    \item data-flow–aware clone/similarity measures
\end{itemize}

\textbf{3. Data-Flow Integrity for Repairs} \\
In the one-shot correction paper, data-flow is framed as a \textit{constraint on valid repair}:
\begin{itemize}
    \item correct repairs should preserve the data-flow of variables
    \item data-flow mismatches signal incorrect behavior, even when syntax is valid
\end{itemize}
While not formalized as a separate metric there, it supports using Data-Flow Match as a correctness-sensitive semantic signal \cite{Le2024OneShotCorrection}.

\subsubsection{Applications in Software Engineering}

\textbf{1. Code Generation Evaluation} \\
Data-Flow Match helps detect incorrect variable propagation, missing or extraneous assignments, swapped or inverted computation steps, and dead or unused variable patterns. This is especially important where two pieces of code may look similar (tokens/AST) but implement different logic \cite{Chen2024SurveyCodeGen}.

\textbf{2. Code Repair Validation} \\
Data-flow preservation ensures the fixed code maintains the intended logic, variables are neither dropped nor misused, and the repair does not introduce hidden semantic regressions. One-shot correction approaches explicitly call out bridging “data-flow gaps” as necessary for correct repairs \cite{Le2024OneShotCorrection}.

\textbf{3. Repository-Level Reasoning} \\
On larger projects, data-flow mismatch across functions or modules can cause integration bugs, wrong state propagation, or API misuse. Data-flow–based metrics can therefore support repository-level evaluation.

\textbf{4. Program Understanding \& Refactoring} \\
Data-flow analysis naturally supports semantic clone detection, behavior-preserving refactor checks, and semantic search (e.g., “find code that computes this thing similarly”). Data-Flow Match can underlie these evaluations as a quantitative metric.

\subsubsection{Interpretation}

\textbf{1. High Data-Flow Match} \\
Indicates:
\begin{itemize}
    \item strong semantic preservation
    \item behaviorally similar programs
    \item correct variable usage patterns
    \item faithful reproduction of the underlying logic
\end{itemize}

\textbf{2. Low Data-Flow Match} \\
Indicates:
\begin{itemize}
    \item incorrect semantics despite surface similarity
    \item broken or missing variable flows
    \item wrong dependencies or misordered logic
    \item logical errors hidden behind valid syntax
\end{itemize}

\textbf{3. Strengths}
\begin{itemize}
    \item Captures semantic similarity beyond tokens/AST \cite{Chen2024SurveyCodeGen}.
    \item Aligns more closely with human judgments of correctness than n-gram metrics.
    \item Language-general in principle, if parsers/data-flow tools exist.
\end{itemize}

\textbf{4. Limitations}
\begin{itemize}
    \item Requires reliable parsing and data-flow extraction (tooling heavy).
    \item Cannot be computed if code does not parse or is incomplete.
    \item Still only approximates full functional correctness.
    \item Dynamic features (reflection, dynamic typing) complicate analysis.
\end{itemize}