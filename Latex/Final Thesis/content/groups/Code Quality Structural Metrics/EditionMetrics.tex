\subsection{Edition Metrics (Patch Edit Size / Scope)}

Edition Metrics are a category of descriptive statistics used in software engineering benchmarks like \textbf{SWE-bench} to characterize and evaluate the code patches generated by Large Language Models (LLMs).

Instead of a single performance score, these metrics quantify the \textbf{size, scope, and complexity} of a generated patch (the proposed solution to a programming issue). They are used to compare the model-generated patches against the ``gold'' (human-written reference) patches.

These metrics are grouped into two main categories: \textit{Patch Edit Size} (lines) and \textit{Structural Scope} (files/functions).

\subsubsection*{1. Patch Edit Size (Lines)}

\textbf{Definition} \\
This category measures the \textbf{length} of the code patch by counting the number of lines affected. It is defined by three related metrics:
\begin{itemize}
    \item \textbf{Lines Added}: Refers to the number of new lines that are introduced by the patch.
    \item \textbf{Lines Removed}: Refers to the number of pre-existing lines taken out by the solution.
    \item \textbf{Lines Edited / Total Lines}: The sum of all lines added and removed to create the patch.
\end{itemize}

\textbf{Purpose} \\
The primary purpose is to quantify the size of a solution. In the SWE-bench benchmark, these metrics revealed that LLMs ``tend to generate shorter, simpler edits.'' Model-generated patches that applied correctly were, on average, less than half the total length of the gold reference patches.

\subsubsection*{2. Structural Scope (Files / Functions)}

\textbf{Definition} \\
This category measures the \textbf{breadth and complexity} of a patch by quantifying how many distinct structural components of the codebase are modified.
\begin{itemize}
    \item \textbf{Files Edited}: The number of unique files that the patch makes changes to.
    \item \textbf{Functions Edited}: The number of unique functions or methods that the patch modifies.
\end{itemize}

\textbf{Purpose} \\
The purpose is to measure a model's ability to handle complex tasks that are not self-contained. Resolving issues in SWE-bench ``frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously.'' For instance, reference solutions typically edit an average of 1.7 files and 3.0 functions per task.


\subsubsection{Applications}

In the context of the SWE-bench framework, Edition Metrics—such as line counts and file scope—serve a purpose far beyond simple descriptive statistics. They are effectively applied to gauge the true depth of a coding task and to draw meaningful, quantitative comparisons between Artificial Intelligence and human developers. These metrics allow researchers to diagnose three distinct aspects of automated software engineering:

\textbf{1. Defining Task Complexity and Realism}
Edition metrics are crucial for distinguishing substantial engineering problems from trivial coding exercises. Unlike traditional benchmarks that often rely on self-contained problems resolvable in a few lines, real-world software engineering involves complex, cross-context changes.
\begin{itemize}
    \item \textbf{Quantifying Scope:} By tracking the number of \textit{Files} and \textit{Functions Edited}, we can verify if a task demands a broad understanding of the system. For example, reference solutions in SWE-bench typically require editing an average of 1.7 files and 3.0 functions, verifying that these tasks require coordinating changes across multiple modules rather than just generating a single isolated snippet \cite{Jimenez2023SWEBench}.
    \item \textbf{Assessing Difficulty:} The variance in \textit{Lines Added} versus \textit{Lines Removed} helps categorize the nature of the work, distinguishing between minor bug patches and more involved structural refactoring \cite{Jimenez2023SWEBench}.
\end{itemize}

\textbf{2. Benchmarking the "Human-AI" Gap}
Perhaps the most telling application of these metrics is revealing the behavioral gap between Large Language Models (LLMs) and human engineers. The data exposes a tendency for models to under-deliver compared to their human counterparts.
\begin{itemize}
    \item \textbf{Tendency for Simplicity:} Analysis using these metrics shows that models generally produce "shorter, simpler edits" than humans. In fact, even when model-generated patches apply successfully, they are on average less than half the total length of the human-written "gold" patches (30.1 lines vs 74.5 lines), suggesting a lack of depth in the AI's approach \cite{Jimenez2023SWEBench}.
    \item \textbf{Scope Limitation:} While human developers frequently edit multiple files to ensure robust integration, the \textit{Files Edited} metric highlights that models rarely venture beyond a single file, indicating a struggle to navigate and modify the broader repository effectively \cite{Jimenez2023SWEBench}.
\end{itemize}

\textbf{3. Identifying Structural Reasoning Failures}
Finally, discrepancies in Edition Metrics are applied to diagnose specific failures in reasoning, particularly "greedy" or short-sighted coding patterns.
\begin{itemize}
    \item \textbf{Ignoring Dependencies:} When a model's \textit{Structural Scope} is significantly lower than the reference solution, it often signals that the AI has fixed the immediate error but neglected necessary updates to inter-file dependencies or third-party libraries \cite{Jimenez2023SWEBench}.
    \item \textbf{Lack of Defensive Coding:} Human engineers often include additional lines to handle edge cases or update tests. In contrast, low \textit{Lines Added} counts in model outputs typically reflect a failure to anticipate future issues or maintain code style standards, focusing solely on a minimal, "greedy" fix \cite{Jimenez2023SWEBench}.
\end{itemize}

\subsubsection*{Comparative Summary}
Below is the classification of edition metrics by category and function:

\begin{table}[H]
    \centering
    \caption{Classification of Edition Metrics by Category and Function}
    \label{tab:edition_metrics}
    \begin{tabular}{|l|l|p{5cm}|}
    \hline
    \textbf{Metric} & \textbf{Category} & \textbf{Measures...} \\
    \hline
    Lines Added & Patch Edit Size & The number of new lines introduced by the patch. \\
    \hline
    Lines Removed & Patch Edit Size & The number of pre-existing lines taken out by the patch. \\
    \hline
    Lines Edited & Patch Edit Size & The total lines added and removed; the overall length. \\
    \hline
    Files Edited & Structural Scope & The number of distinct files modified to solve the issue. \\
    \hline
    Functions Edited & Structural Scope & The number of distinct functions modified to solve the issue. \\
    \hline
    \end{tabular}
\end{table}

\subsubsection{Interpretation}

Interpreting edition metrics requires a direct comparison between the model-generated patch ($\hat{\delta}$) and the human reference solution ($\delta$). In the context of SWE-bench, discrepancies in these metrics serve as quantitative indicators of the model's depth of reasoning and adherence to software engineering best practices.

\textbf{Simplicity vs. Robustness (Line Count)}
A significant divergence in \textit{Lines Edited} is primarily interpreted as a difference in solution sophistication. Empirical results show that model-generated patches are often significantly shorter than reference solutions (averaging 30.1 lines versus 74.5 lines for gold patches) \cite{Jimenez2023SWEBench}.
\begin{itemize}
    \item \textbf{Lower Values:} A lower line count in the generated patch typically indicates a "greedy" approach, where the model solves the immediate problem exactly but neglects code style, formatting, or logical constraints \cite{Jimenez2023SWEBench}.
    \item \textbf{Higher Values:} Higher line counts in the reference solution often reflect "structural improvements" and defensive coding practices—such as anticipating future errors or maintaining backward compatibility—that models frequently fail to replicate \cite{Jimenez2023SWEBench}.
\end{itemize}

\textbf{Localization vs. Systemic Understanding (Scope)}
The \textit{Files Edited} and \textit{Functions Edited} metrics interpret the model's ability to reason about the codebase as a connected system.
\begin{itemize}
    \item \textbf{Narrow Scope (Single File):} Models rarely edit more than a single file, whereas reference solutions often span multiple files (averaging 1.7 files) \cite{Jimenez2023SWEBench}. A value of 1.0 in \textit{Files Edited} often signifies that the model has failed to identify inter-file dependencies or coordinate changes across different modules, limiting its ability to resolve complex, repository-scale issues \cite{Jimenez2023SWEBench}.
    \item \textbf{Primitive Implementation:} A reduced scope often correlates with the generation of "primitive Python code" that solves the issue in isolation rather than leveraging existing third-party libraries or utility functions present elsewhere in the repository \cite{Jimenez2023SWEBench}.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):

\sloppy
\cite{
Jimenez2023SWEBench,
}
\fussy
