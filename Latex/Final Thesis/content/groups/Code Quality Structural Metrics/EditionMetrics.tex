\subsection{Edition Metrics (Patch Edit Size / Scope)}

\subsubsection*{Introduction}
Edition Metrics are a category of descriptive statistics used in software engineering benchmarks like \textbf{SWE-bench} to characterize and evaluate the code patches generated by Large Language Models (LLMs).

Instead of a single performance score, these metrics quantify the \textbf{size, scope, and complexity} of a generated patch (the proposed solution to a programming issue). They are used to compare the model-generated patches against the ``gold'' (human-written reference) patches.

These metrics are grouped into two main categories: \textit{Patch Edit Size} (lines) and \textit{Structural Scope} (files/functions).

\subsubsection*{1. Patch Edit Size (Lines)}

\textbf{Definition} \\
This category measures the \textbf{length} of the code patch by counting the number of lines affected. It is defined by three related metrics:
\begin{itemize}
    \item \textbf{Lines Added}: Refers to the number of new lines that are introduced by the patch.
    \item \textbf{Lines Removed}: Refers to the number of pre-existing lines taken out by the solution.
    \item \textbf{Lines Edited / Total Lines}: The sum of all lines added and removed to create the patch.
\end{itemize}

\textbf{Purpose} \\
The primary purpose is to quantify the size of a solution. In the SWE-bench benchmark, these metrics revealed that LLMs ``tend to generate shorter, simpler edits.'' Model-generated patches that applied correctly were, on average, less than half the total length of the gold reference patches.

\subsubsection*{2. Structural Scope (Files / Functions)}

\textbf{Definition} \\
This category measures the \textbf{breadth and complexity} of a patch by quantifying how many distinct structural components of the codebase are modified.
\begin{itemize}
    \item \textbf{Files Edited}: The number of unique files that the patch makes changes to.
    \item \textbf{Functions Edited}: The number of unique functions or methods that the patch modifies.
\end{itemize}

\textbf{Purpose} \\
The purpose is to measure a model's ability to handle complex tasks that are not self-contained. Resolving issues in SWE-bench ``frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously.'' For instance, reference solutions typically edit an average of 1.7 files and 3.0 functions per task.

\subsubsection*{Domains \& Benchmarks}
\begin{itemize}
    \item \textbf{Domains}: Software Engineering, Program Repair.
    \item \textbf{Benchmarks}: SWE-bench.
\end{itemize}

\subsubsection*{Comparative Summary}
Below is the classification of edition metrics by category and function:

\begin{center}
\begin{tabular}{|l|l|p{5cm}|}
\hline
\textbf{Metric} & \textbf{Category} & \textbf{Measures...} \\
\hline
Lines Added & Patch Edit Size & The number of new lines introduced by the patch. \\
\hline
Lines Removed & Patch Edit Size & The number of pre-existing lines taken out by the patch. \\
\hline
Lines Edited & Patch Edit Size & The total lines added and removed; the overall length. \\
\hline
Files Edited & Structural Scope & The number of distinct files modified to solve the issue. \\
\hline
Functions Edited & Structural Scope & The number of distinct functions modified to solve the issue. \\
\hline
\end{tabular}
\end{center}

% a√±adir esta entrada a tu archivo .bib
% @article{jimenez2024swe,
%   title={SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},
%   author={Jimenez, C. E. and Yang, J. and Wettig, A. and others},
%   journal={arXiv preprint arXiv:2310.06770},
%   year={2024},
%   doi={10.48550/arXiv.2310.06770}
% }