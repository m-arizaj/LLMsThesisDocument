\subsection{Mutant Killings (Mutation Score)}

\subsubsection*{Definition}
\textbf{Mutant Killings}, also known as \textbf{Mutation Score}, is a metric used to precisely evaluate the effectiveness of a test suite. It is derived from \textbf{Mutation Testing} (or Mutation Analysis), a technique that involves:

\begin{enumerate}
    \item Creating a large number of artificial buggy versions of a program (the ``ground-truth solution''). Each version is called a \textbf{mutant} and contains one subtle, seeded bug (e.g., changing a $<$ to a $>$).
    \item Running the test suite against these mutants.
    \item If a test case causes a mutant to fail (i.e., it detects the bug), that mutant is considered ``killed.''
\end{enumerate}

The metric is the resulting score that measures what percentage of mutants the test suite successfully detected.

\subsubsection*{Formula}
The metric is formally known as the Mutation Score, defined as the ratio of killed mutants to the total number of non-equivalent mutants:

\begin{equation}
    \text{Mutation Score} = \frac{\text{Number of Killed Mutants}}{\text{Total Number of Mutants}}
\end{equation}

A higher score indicates a more effective test suite, as it can detect a larger percentage of the seeded bugs.

\subsubsection*{Purpose}
The primary purpose is to \textbf{quantify test effectiveness more precisely than code coverage}. While coverage measures execution, mutation testing measures the actual ability to detect faults.

In the context of evaluating LLM-generated code, it is notably used as a ``testing requirement'' for \textbf{test-suite reduction}. The goal is to create a minimal set of tests (e.g., in HumanEval+-Mini) that preserves the full effectiveness of the original suite.

\subsubsection*{Domains}
\begin{itemize}
    \item Software Engineering
    \item Test Effectiveness Evaluation
    \item Mutation Analysis
    \item Code Generation (Benchmark Creation)
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Precise Effectiveness Measure}: It is considered a rigorous way to evaluate test suite quality, superior to simple code coverage.
    \item \textbf{Focuses on Defect Detection}: It directly measures the ability to find faults rather than just counting executed lines.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Theoretical Basis}: It is a ``theoretical'' metric based on artificial bugs (mutants). This may not perfectly align with ``empirical'' effectiveness (detecting real, incorrect samples generated by LLMs).
    \item \textbf{Cost}: Generating and running tests against a large number of mutants can be computationally expensive.
\end{itemize}

% a√±adir esta entrada a tu archivo .bib
% @article{liu2023code,
%   title={Is your code generated by ChatGPT really correct? Rigorous evaluation of large language models for code generation},
%   author={Liu, J. and Xia, C. S. and Wang, Y. and Zhang, L.},
%   journal={arXiv preprint arXiv:2305.01210},
%   year={2023},
%   doi={10.48550/arXiv.2305.01210}
% }