\subsection{Mutant Killings (Mutation Score)}

\textbf{Mutant Killings}, also known as \textbf{Mutation Score}, is a metric used to precisely evaluate the effectiveness of a test suite. It is derived from \textbf{Mutation Testing} (or Mutation Analysis), a technique that involves:

\begin{enumerate}
    \item Creating a large number of artificial buggy versions of a program (the ``ground-truth solution''). Each version is called a \textbf{mutant} and contains one subtle, seeded bug (e.g., changing a $<$ to a $>$).
    \item Running the test suite against these mutants.
    \item If a test case causes a mutant to fail (i.e., it detects the bug), that mutant is considered ``killed.''
\end{enumerate}

The metric is the resulting score that measures what percentage of mutants the test suite successfully detected.

The metric is formally known as the Mutation Score, defined as the ratio of killed mutants to the total number of non-equivalent mutants:

\begin{equation}
    \text{Mutation Score} = \frac{\text{Number of Killed Mutants}}{\text{Total Number of Mutants}}
\end{equation}

A higher score indicates a more effective test suite, as it can detect a larger percentage of the seeded bugs.

\subsubsection*{Purpose}
The primary purpose is to \textbf{quantify test effectiveness more precisely than code coverage}. While coverage measures execution, mutation testing measures the actual ability to detect faults.

In the context of evaluating LLM-generated code, it is notably used as a ``testing requirement'' for \textbf{test-suite reduction}. The goal is to create a minimal set of tests (e.g., in HumanEval+-Mini) that preserves the full effectiveness of the original suite.

\subsubsection{Applications}
Based on the provided text, the application of Mutant Killings (Mutation Score) in software engineering focuses on the rigorous evaluation and optimization of test suites, particularly in the context of code generation benchmarks \cite{Liu2023IsYourCodeCorrect,}.

\begin{itemize}
    \item \textbf{Precise Evaluation of Test Effectiveness:} 
    The paper establishes that mutation analysis is used to evaluate the quality of a test suite more precisely than traditional metrics like code coverage. While coverage only measures the extent of code execution, mutant killings measure the test suite's actual ability to detect subtle, artificial faults (mutants) seeded into the ground-truth solutions. This effectively identifies "weak" test cases that execute code but fail to verify its logic \cite{Liu2023IsYourCodeCorrect,}.

    \item \textbf{Test-Suite Reduction (Minimization):} 
    A primary application detailed in the study is using mutant killings as a \textit{testing requirement} for reducing the size of test datasets (e.g., creating \textsc{HumanEval+-Mini}). The metric is used to solve the "set covering problem," where the goal is to select a minimal subset of tests that preserves the same "mutant killing" capacity as the full test suite. This allows for computationally efficient evaluation without sacrificing the rigor of the tests \cite{Liu2023IsYourCodeCorrect,}.

    \item \textbf{Benchmarking LLM Code Synthesis:} 
    The metric is applied to validate the robustness of benchmarks used for Large Language Models (LLMs). By ensuring a high mutation score, researchers can create benchmarks (like \textsc{HumanEval+}) that are capable of detecting incorrect code solutions generated by LLMs that might otherwise pass weaker, coverage-based tests \cite{Liu2023IsYourCodeCorrect,}.
\end{itemize}

\subsubsection{Interpretation}

The Mutant Killings metric (or Mutation Score) is interpreted as a precise measure of \textbf{test effectiveness}, distinct from and superior to standard code coverage \cite{Liu2023IsYourCodeCorrect,}. While code coverage interprets the \textit{extent} to which the code is executed, it does not guarantee that the execution path effectively validates the logic or detects defects \cite{Liu2023IsYourCodeCorrect,}.

Consequently, the mutation score should be interpreted as the \textbf{ratio of fault detection capability}. A higher score indicates that the test suite successfully identified a larger proportion of the "artificial buggy programs" (mutants) generated by seeding subtle bugs into the ground-truth solution \cite{Liu2023IsYourCodeCorrect,}. In the context of test-suite reduction, this metric serves as a \textbf{theoretical guarantee of test adequacy}, ensuring that a minimized set of tests retains the same rigor and ability to falsify incorrect code as the comprehensive original test suite \cite{Liu2023IsYourCodeCorrect,}.

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Liu2023IsYourCodeCorrect,
}
\fussy
