\subsection{Test Metrics}

\subsubsection{Introduction}

Test Metrics are used to evaluate the validity, reliability, and effectiveness of testing procedures applied to both software systems and AI-generated code. They quantify how accurately test cases assess functional correctness, regression prevention, and overall evaluation efficiency. In LLM-based software engineering benchmarks such as DevEval, SWE-bench, HumanEval+, and TESTPILOT, these metrics provide structured measures of testing rigor and consistency.

\subsubsection{Main Variants and Definitions}

\textbf{1. Oracle Test} \cite{Li2024FullSDLC, Jimenez2023SWEBench} \\
Measures how well the model-generated or benchmark-defined oracle (expected output) validates code correctness during execution.
Used in DevEval and SWE-bench to assess test validity and execution accuracy by verifying that outputs match the ground-truth behavior.

\textbf{2. Average Test Cases} \cite{Anand2024AnalysisLLMCode} \\
Represents the mean number of test cases generated or executed per evaluation instance. Used to quantify evaluation rigor and ensure coverage across varied problem types.

\textbf{3. Differential Testing Result} \cite{Liu2023IsYourCodeCorrect} \\
Compares the behavior of multiple code versions or model outputs under the same input conditions to detect discrepancies.
Applied in HumanEval+ to validate functional correctness through comparative testing.

\textbf{4. Fail-to-Pass / Pass-to-Pass Tests} \cite{Jimenez2023SWEBench} \\
Derived from SWE-bench, these indicators track whether a previously failing test now passes (fail-to-pass) or remains stable after fixes (pass-to-pass). They measure functional progress and regression prevention.

\textbf{5. Pass@Acceptance / Pass@Unit Test} \cite{Li2024FullSDLC} \\
Variants used in DevEval, indicating the proportion of models or systems that meet acceptance or unit-level test criteria.
Evaluates completeness and requirement satisfaction.

\textbf{6. Percentage of Passing Tests} \cite{Schafer2024UnitTestGeneration} \\
Used in TESTPILOT to assess reliability by computing the ratio of passed to total tests. Focuses on robustness across generated test cases.

\textbf{7. Test-Suite Reduction Ratio} \cite{Liu2023IsYourCodeCorrect} \\
Measures how efficiently a reduced subset of tests maintains coverage and evaluation accuracy. Applied in HumanEval+ to balance evaluation efficiency and testing completeness.

\subsubsection{Interpretation}

In software engineering and LLM-based code evaluation, Test Metrics bridge the gap between code generation performance and verification quality. They ensure that models are not only generating syntactically valid code but also functionally correct and testable solutions. These metrics are integral to modern evaluation frameworks, emphasizing execution-based validation, test coverage, and regression stability.