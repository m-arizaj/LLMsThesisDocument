\subsection{Sentiment Polarity Shift}

\textbf{Sentiment Polarity Shift} is not a single metric, but an evaluation category designed to measure \textbf{Social Bias}. It quantifies how the sentiment polarity (positive, negative, or neutral) of an LLM's response changes when conditioned on different social groups (e.g., by gender, race, or religion).

This approach evaluates whether the model associates certain groups with more negative or positive social connotations. Specific metrics that implement this concept include:
\begin{itemize}
    \item \textbf{Regard Score}: Measures ``polarity towards... social groups'' or the positive/negative social connotation.
    \item \textbf{Counterfactual Sentiment Bias}: Compares the sentiment distributions of two sentences generated from counterfactual prompts (e.g., where only the social group is changed).
    \item \textbf{Score Parity}: Measures the consistency with which a model generates language (evaluated by a sentiment classifier) with respect to a protected attribute.
\end{itemize}

\subsubsection*{Calculation (General Idea)}
These metrics generally rely on an \textbf{auxiliary classifier model} to score the generated text for sentiment or toxicity. The process typically involves:

\begin{enumerate}
    \item \textbf{Prompting}: The LLM is provided with prompts containing descriptors of social groups, often in counterfactual pairs (e.g., ``The [Group A] man was...'' vs. ``The [Group B] man was...'').
    \item \textbf{Generation}: The LLM generates text or continuations for each prompt.
    \item \textbf{Classification}: An external classifier (e.g., a sentiment or toxicity classifier) scores the polarity of each generated text.
    \item \textbf{Comparison}: The ``shift'' is calculated by comparing the sentiment scores or distributions between the different social groups.
\end{enumerate}

For example, \textit{Counterfactual Sentiment Bias} uses the Wasserstein-1 distance to measure the difference between the resulting sentiment distributions:

\begin{equation}
    \text{Shift}(G_A, G_B) = W_1(P(S|G_A), P(S|G_B))
\end{equation}

\noindent where $P(S|G_X)$ is the sentiment probability distribution conditioned on group $X$.

\subsubsection*{Purpose}
The main objective is to quantify social and cultural biases by identifying disparate sentiment or polarity in model outputs associated with different groups. This helps detect representational harms such as \textbf{stereotyping} (associating a group with negative attributes) or \textbf{misrepresentation}.

\subsection{Applications}

Although \textbf{Sentiment Polarity Shift} metrics originated in general NLP fairness research, they have crucial applications in the domain of Software Engineering (SE), particularly concerning the human and social aspects of SE (Recruitment, Professional Representation, and Developer Assistance).

Based on the provided literature, specific applications include:

\begin{itemize}
    \item \textbf{Bias Detection in Automated Technical Career Advice}:
    LLMs are increasingly used as coding assistants or career coaches. Research indicates that models may exhibit \textit{polarity shift} or disparate content when advising technical professionals based on demographics. For instance, \cite{Gallegos2024BiasFairness} highlights a case where a model conditioned on ``female data engineer'' focused on the need to \textit{develop} technical skills, whereas for a ``male data engineer'', it assumed competence and focused on \textit{networking} or data sources. This manifests as a negative shift in regard (social connotation) towards the technical competency of women in SE.

    \item \textbf{Evaluation of Stereotypes in Engineering Professions}:
    Metrics like the \textit{Regard Score} are applied to Open-Ended Language Generation to audit how models describe specific professions. The BOLD dataset \cite{dhamala2021bold} explicitly categorizes ``computer'' and ``engineering'' under the \textit{Science \& Technology} domain. Applying these metrics allows engineers to quantify if an LLM systematically generates text with lower sentiment or negative stereotypes when describing software engineers from marginalized groups compared to dominant groups.

    \item \textbf{Fairness in Recruitment and Occupational Descriptions}:
    Counterfactual evaluation techniques are used to measure sentiment bias in generated text about occupations. \cite{huang2020reducing} and \cite{sheng2019woman} demonstrate frameworks where the ``occupation'' slot in a prompt is perturbed (e.g., from ``The \textit{nurse}...'' to ``The \textit{programmer}...''). In SE, this application is critical for auditing automated resume screening tools or job description generators to ensure they do not associate technical roles (like ``programmer'' or ``developer'') with sentiment distributions that favor specific genders or races.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Measures Specific Harm}: Directly quantifies disparate sentiment, a well-known representational harm.
    \item \textbf{Black-Box Applicable}: Can be applied to any model that outputs text, as it relies on an external classifier.
    \item \textbf{Subtle Bias Detection}: Captures biases more subtle than overt toxicity, such as negative stereotypes or microaggressions.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Classifier Dependency}: The metric is highly dependent on the auxiliary classifier used, which may have its own biases.
    \item \textbf{Ambiguity}: In open-ended text, it can be difficult to attribute bias specifically to the group in the prompt versus a group mentioned in the continuation.
    \item \textbf{Sensitivity to Decoding}: Parameter choices (e.g., temperature) can drastically change the measured bias, leading to inconsistent results.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Gallegos2024BiasFairness,
dhamala2021bold,
sheng2019woman,
huang2020reducing,
}
\fussy
