\subsection{Hallucination}

The Hallucination metric quantifies the extent to which a model produces content that is factually incorrect, ungrounded, irrelevant, or logically inconsistent with respect to the input or known information \cite{Huang2023SurveyHallucination}.
In large language models and software engineering settings, hallucination typically refers to generated statements, code, or explanations that appear coherent but contain inaccuracies, fabricated details, or non-existent elements like invalid APIs, incorrect parameters, or invented functions.
This metric is essential for evaluating reliability and truthfulness in tasks such as code documentation, automated debugging assistance, requirement generation, and other contexts where factual correctness is critical.

Hallucination is commonly measured using discrete proportions rather than continuous scores.
A general formulation used across benchmarks is the \textit{hallucination rate} \cite{Huang2023SurveyHallucination}:

\begin{equation}
H_{rate} = \frac{N_{hallucinated}}{N_{total}}
\end{equation}

where:
\begin{itemize}
    \item $N_{hallucinated}$ is the number of outputs judged to contain hallucinations,
    \item $N_{total}$ is the number of evaluated outputs.
\end{itemize}

This instance-level formulation is consistent across classification-based, content-verification, and multimodal hallucination benchmarks.
Frameworks may apply automatic judgments, human annotation, or external model-based verification.

\textbf{FEWL-Based Factualness (Wei et al., 2024):} \\
Some evaluation settings use a model-weighted factualness score to assess hallucination without access to gold-standard answers \cite{Wei2024FEWL}.
The FEWL framework aggregates judgments from multiple trusted LLMs to estimate the likelihood that an output is factual, producing a continuous hallucination-related score that complements rate-based metrics \cite{Wei2024FEWL}.

\subsubsection{Variants}

\textbf{1. Hallucination Rate} \\
Used in surveys and evaluation frameworks such as HELM and general NLP/LLM assessments, hallucination rate measures the proportion of outputs containing incorrect or ungrounded content \cite{Huang2023SurveyHallucination}.
It is widely applied to factual QA, summarization, reasoning, and text generation.

\textbf{2. AMBER Hallucination Categories} \cite{Wang2023AMBER} \\
AMBER (2024) introduces a structured taxonomy for multimodal hallucination evaluation consisting of:
\begin{itemize}
    \item \textit{Existence Hallucination:} Claiming entities that do not exist or are not present.
    \item \textit{Attribute Hallucination:} Incorrect or fabricated attributes about existing entities.
    \item \textit{Relation Hallucination:} Incorrect relations or interactions between entities.
\end{itemize}
AMBER evaluates hallucination by classifying each generated response into correct or hallucinated categories across these dimensions.

\textbf{3. FEWL Model-Based Factualness} \\
FEWL provides a hallucination-oriented scoring mechanism without requiring reference answers \cite{Wei2024FEWL}.
It leverages multiple LLM judges and learned weighting to compute a factualness score, penalizing logically inconsistent or fabricated content.

\textbf{4. Domain-Specific Hallucination Detection} \\
Hallucination evaluation appears in specialized software engineering tasks such as:
\begin{itemize}
    \item API correctness verification,
    \item checking validity of code references,
    \item assessing generated documentation against real codebases.
\end{itemize}
These settings adapt the hallucination rate to code-related units such as statements or API calls.

\subsubsection{Application in Software Engineering}

Hallucination metrics are increasingly used to evaluate LLM-generated code, documentation, and technical explanations \cite{Woesle2025Hallucinations}.
Common manifestations include:
\begin{itemize}
    \item use of non-existent APIs or functions,
    \item incorrect library imports,
    \item fabricated parameter names or values,
    \item logically invalid reasoning in debugging explanations.
\end{itemize}

Measuring hallucination is essential for assessing robustness of LLM-based software assistants, improving code-generation correctness, and ensuring trustworthiness in automated development pipelines.

\subsubsection{Interpretation}

\begin{itemize}
    \item \textbf{Low hallucination rate} indicates high factual consistency and reliability.
    \item \textbf{High hallucination rate} reveals model tendencies to invent or distort information.
\end{itemize}

In both natural-language and software-engineering settings, hallucination metrics help quantify truthfulness and prevent deployment risks associated with incorrect or misleading content \cite{Huang2023SurveyHallucination}.
Model behavior is interpreted not only by the presence of factual errors but also by the structural type of hallucination (existence, attribute, relation), providing a more detailed understanding of failure modes \cite{Wang2023AMBER}.

\subsubsection{Additional References}

This metric and its variants are referenced and/or used in the following papers:

\cite{Chang2023SurveyLLMs, Wang2023AMBER, Xu2025LLMAgentsToolLearning, Li2025LLMSoftwareTesting}