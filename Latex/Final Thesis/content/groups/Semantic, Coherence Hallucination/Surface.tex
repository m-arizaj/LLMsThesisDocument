\subsection{Surface-Form Constraints}

\subsubsection*{Definition}
\textbf{Surface-Form Constraints} is a ``much-relaxed form of similarity metric'' used to evaluate generated code.

Instead of measuring functional correctness (i.e., execution and testing) or deep syntactic similarity (like AST matching), this metric simply checks if the generated code adheres to specific low-level textual requirements. It is primarily used in benchmarks to verify that the model's solution includes or excludes specific elements as defined by the problem instructions.

\subsubsection*{Formula (General Idea)}
This metric is not a complex calculation but rather a binary check for adherence to constraints. It can be formally expressed as an indicator function that verifies two conditions:

\begin{equation}
    \text{Pass} = \mathbb{I}\left( \left( \forall r \in R, r \in C_{\text{gen}} \right) \land \left( \forall f \in F, f \notin C_{\text{gen}} \right) \right)
\end{equation}

\noindent where:
\begin{itemize}
    \item $C_{\text{gen}}$: The generated code.
    \item $R$: The set of \textbf{required} elements (e.g., specific API calls, keywords like ``pandas'').
    \item $F$: The set of \textbf{forbidden} elements (e.g., restricted libraries).
    \item $\mathbb{I}$: Indicator function returning 1 (Pass) if all conditions are met, 0 otherwise.
\end{itemize}

\subsubsection*{Purpose}
The purpose is to provide a simple, automated check on the \textbf{structure or composition} of the generated code, rather than its full functional correctness. It helps measure whether a model can follow specific instructions about \textit{how} to solve a problem (e.g., ``solve this using the `Pandas` library'').

\subsubsection*{Domains}
\begin{itemize}
    \item Software Engineering
    \item Code Generation
    \item Data Science Code Generation
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Simplicity}: It is computationally much cheaper and faster than executing code or parsing an Abstract Syntax Tree (AST).
    \item \textbf{Method Verification}: It allows benchmarks to strictly test if a model uses a specific, required tool (like a specific library function) to solve a problem.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{No Guarantee of Correctness}: This metric is ``relaxed.'' Code can pass the surface-form constraints (e.g., it includes the word ``pandas'') but still be functionally incorrect, buggy, or incomplete.
    \item \textbf{Brittle}: It is a surface-level check and does not capture the semantic or logical correctness of the code.
\end{itemize}

\subsubsection*{Related Concepts}
\begin{itemize}
    \item \textbf{Surface Perturbation}: A benchmark design feature (used in DS-1000) where the problem's wording is changed to test robustness. This is distinct from Surface-Form Constraints, which is a metric for evaluating the output code.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
lai2022ds1000,
Paul2024BenchmarksMetricsCodeGen,
Anand2024AnalysisLLMCode,
}
\fussy

