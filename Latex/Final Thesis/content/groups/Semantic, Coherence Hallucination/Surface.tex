\subsection{Surface-Form Constraints}

\textbf{Surface-Form Constraints} is a ``much-relaxed form of similarity metric'' used to evaluate generated code.

Instead of measuring functional correctness (i.e., execution and testing) or deep syntactic similarity (like AST matching), this metric simply checks if the generated code adheres to specific low-level textual requirements. It is primarily used in benchmarks to verify that the model's solution includes or excludes specific elements as defined by the problem instructions.

\subsubsection*{Formula (General Idea)}
This metric is not a complex calculation but rather a binary check for adherence to constraints. It can be formally expressed as an indicator function that verifies two conditions:

\begin{equation}
    \text{Pass} = \mathbb{I}\left( \left( \forall r \in R, r \in C_{\text{gen}} \right) \land \left( \forall f \in F, f \notin C_{\text{gen}} \right) \right)
\end{equation}

\noindent where:
\begin{itemize}
    \item $C_{\text{gen}}$: The generated code.
    \item $R$: The set of \textbf{required} elements (e.g., specific API calls, keywords like ``pandas'').
    \item $F$: The set of \textbf{forbidden} elements (e.g., restricted libraries).
    \item $\mathbb{I}$: Indicator function returning 1 (Pass) if all conditions are met, 0 otherwise.
\end{itemize}

\subsubsection*{Purpose}
The purpose is to provide a simple, automated check on the \textbf{structure or composition} of the generated code, rather than its full functional correctness. It helps measure whether a model can follow specific instructions about \textit{how} to solve a problem (e.g., ``solve this using the `Pandas` library'').

\subsubsection{Applications in Software Engineering}

While \textbf{Surface-Form Constraints} are computationally simple compared to execution-based metrics, they serve critical roles in \textbf{Automated Software Quality Assurance} and \textbf{Code Synthesis Evaluation}. They are particularly useful for enforcing non-functional requirements such as efficiency and library compliance.

Based on the DS-1000 framework and related reviews, specific applications include:

\begin{itemize}
    \item \textbf{Enforcing Idiomatic and Efficient Implementations}:
    In scientific computing and data engineering, functional correctness is not enough; performance matters. A common application is rejecting ``brute force'' solutions in favor of vectorized ones. For example, \cite{lai2022ds1000} uses surface constraints to explicitly forbid loops (\texttt{for}, \texttt{while}) in the Abstract Syntax Tree (AST) when a solution requires high-performance libraries like \texttt{pandas} or \texttt{numpy}. This automates the detection of inefficient code that would otherwise pass unit tests.

    \item \textbf{Library and API Compliance Verification}:
    In modern software development, developers often require solutions within a specific technology stack. Surface-form constraints are applied to ensure that generated code strictly utilizes a requested library (e.g., \texttt{TensorFlow}) rather than a generic alternative or a different framework (e.g., \texttt{PyTorch}). This is essential for building \textbf{Domain-Specific Coding Assistants} that must adhere to existing project dependencies \cite{lai2022ds1000, Paul2024BenchmarksMetricsCodeGen}.

    \item \textbf{Robustness Analysis of Code Generators}:
    These constraints are used to evaluate the robustness of LLMs against ``memorization'' and shortcuts. By applying surface perturbations (e.g., changing keywords or requirements) and checking if the model updates its surface form accordingly, researchers can audit whether a model genuinely understands the software engineering task or is merely regurgitating training data \cite{Anand2024AnalysisLLMCode, lai2022ds1000}.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Simplicity}: It is computationally much cheaper and faster than executing code or parsing an Abstract Syntax Tree (AST).
    \item \textbf{Method Verification}: It allows benchmarks to strictly test if a model uses a specific, required tool (like a specific library function) to solve a problem.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{No Guarantee of Correctness}: This metric is ``relaxed.'' Code can pass the surface-form constraints (e.g., it includes the word ``pandas'') but still be functionally incorrect, buggy, or incomplete.
    \item \textbf{Brittle}: It is a surface-level check and does not capture the semantic or logical correctness of the code.
\end{itemize}


\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
lai2022ds1000,
Paul2024BenchmarksMetricsCodeGen,
Anand2024AnalysisLLMCode,
}
\fussy

