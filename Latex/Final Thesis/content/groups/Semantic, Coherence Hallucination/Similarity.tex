\subsection{Similarity Metrics}

\subsubsection*{Introduction}
In code generation and software engineering, ``Similarity'' is not a single metric but a broad category of evaluation techniques. Its purpose is to quantify how ``close'' a generated output is to a reference, especially when a simple \textbf{Exact Match} is too strict and fails to capture partially correct or semantically equivalent solutions.

Different tasks require different notions of similarity:
\begin{itemize}
    \item \textbf{Syntactic}: For code completion (minimizing typing effort).
    \item \textbf{Semantic}: For docstrings or robustness (preserving meaning).
    \item \textbf{Visual}: For data visualization (matching the rendered image).
\end{itemize}

\subsubsection*{1. Levenshtein Edit Similarity}
\textbf{Definition} \\
Also known as \textbf{Edit Distance}, this metric measures the ``edit effort'' between two strings. It is defined as the minimum number of single-character edits (insertions, deletions, or substitutions) required to change the candidate string into the reference string.

\textbf{Formula} \\
While the core calculation is the distance itself, a normalized similarity score is often expressed as:

\begin{equation}
    \text{Similarity} = 1 - \frac{\text{LevenshteinDistance}(str_1, str_2)}{\max(\text{len}(str_1), \text{len}(str_2))}
\end{equation}

\textbf{Purpose} \\
It is a ``critical evaluation metric'' in code evaluation because it directly measures how much effort a developer would need to manually correct a model's generated code to match the ground truth.

\textbf{Applications} \\
\begin{itemize}
    \item Code Completion (line-level or token-level suggestions).
    \item Structural Distance (assessing syntactic closeness).
\end{itemize}

\subsubsection*{2. Sentence Cosine Similarity}
\textbf{Definition} \\
This is a \textbf{semantic similarity} metric that measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. In NLP and code analysis, these vectors are typically ``sentence embeddings'' generated by models like SBERT.

\begin{itemize}
    \item \textbf{1.0}: Vectors point in the same direction (semantically identical).
    \item \textbf{0.0}: Vectors are orthogonal (unrelated).
\end{itemize}

\textbf{Purpose} \\
To verify that a generated piece of text (like a docstring) or a code element (like a function name) preserves its original \textbf{meaning}, even if the surface-level text (words, syntax) is different.

\textbf{Applications} \\
\begin{itemize}
    \item \textbf{Robustness Benchmarks} (e.g., ReCode): Confirming semantic equivalence after perturbation.
    \item \textbf{Code Generation}: Assessing docstring quality.
\end{itemize}

\subsubsection*{3. Visualization Similarity Scores}
\textbf{Definition} \\
This metric evaluates the \textbf{output quality} of data science code by comparing the \textbf{rendered image} of a generated plot against an expected reference plot. The calculation is often based on image processing techniques (e.g., Structural Similarity Index - SSIM, or Mean Squared Error - MSE).

\textbf{Purpose} \\
To evaluate the correctness of visualization tasks where the code itself is a means to an end. Text-based metrics on the code are poor indicators of visual correctness (different code can produce identical plots).

\textbf{Applications} \\
\begin{itemize}
    \item Data Science Code Generation (matplotlib, seaborn, plotly).
\end{itemize}

\subsubsection*{Comparative Summary}
\begin{center}
\begin{tabular}{|p{3cm}|p{2.5cm}|p{3.5cm}|p{3cm}|}
\hline
\textbf{Metric} & \textbf{Based on} & \textbf{Measures} & \textbf{Typical Domain} \\
\hline
Levenshtein Edit Similarity & Character Edits & Syntactic closeness / Edit effort & Code Completion \\
\hline
Sentence Cosine Similarity & Vector Embeddings & Semantic meaning & Code/Docstring Robustness \\
\hline
Visualization Similarity & Image Comparison & Visual output quality & Data Science Visualization \\
\hline
\end{tabular}
\end{center}


\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Lu2021CodeXGLUE,
Wang2022ReCode,
Nascimento2024LLM4DS,
}
\fussy
