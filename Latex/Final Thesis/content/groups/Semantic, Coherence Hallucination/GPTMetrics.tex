\subsection{GPT-based Metrics (LLM-as-a-Judge)}

\subsubsection*{Introduction}
A recent paradigm in evaluating Large Language Models (LLMs) involves using an LLM itself as the evaluator. This approach, often called ``LLM-as-a-judge,'' leverages the advanced reasoning and language understanding capabilities of models like GPT to assess the quality of generated text or code.

These metrics work by crafting a specific prompt that instructs a powerful model (e.g., GPT-4) to compare a candidate output against a reference or a set of criteria and provide a score. This method has gained prominence because its results often show a \textbf{higher correlation with human judgments} than traditional metrics.

Two prominent examples of this approach are \textbf{GPTScore} (for general text evaluation) and \textbf{GPT-4V Overall Rating} (for multimodal code evaluation).

\subsubsection*{1. GPTScore}

\textbf{Definition} \\
GPTScore is an automatic evaluation method that uses an LLM (like GPT-3 or GPT-4) to score the quality of generated text. It operates by providing the ``judge'' LLM with the context, a reference text, and the candidate (model-generated) text. It prompts the LLM to provide:
\begin{itemize}
    \item A \textbf{final score} (e.g., on a 1-5 scale).
    \item A \textbf{detailed evaluation} (a textual explanation) for its assessment.
\end{itemize}

\textbf{Purpose} \\
To provide a new automatic evaluation metric for text generation that achieves a higher correlation with human judgments than previous metrics like BLEU or ROUGE.

\textbf{Applications} \\
\begin{itemize}
    \item LLM Evaluation / NLP
    \item General Text Generation
\end{itemize}

\textbf{Advantages}
\begin{itemize}
    \item \textbf{High Human Correlation}: Has been shown to correlate more strongly with human evaluation.
    \item \textbf{Explainable}: Unlike traditional metrics that output only a number, GPTScore generates a detailed textual explanation for its score.
\end{itemize}

\subsubsection*{2. GPT-4V Overall Rating}

\textbf{Definition} \\
The GPT-4V Overall Rating is a metric used within the \textbf{Plot2Code} benchmark to evaluate multimodal code generation (i.e., generating code from an image of a plot). It specifically uses the GPT-4V (Vision) model to analyze the generated code alongside the source plot image and assign an ``overall quality rating... on a scale of 1-5.''

\textbf{Purpose} \\
To serve as a ``key metric for assessing the overall quality'' of generated code in a complex, multimodal, image-to-code task. It acts as a scalable, automated alternative to expensive human evaluation for this specific domain.

\subsubsection{Applications}

Based on recent surveys and evaluation frameworks \cite{Lin2024SWC, Chen2024SurveyCodeGen}, the application of GPT-based metrics (LLM-as-a-Judge) has expanded into Software Engineering (SE) to address the limitations of static analysis and functional testing metrics (e.g., Pass@k).

\begin{itemize}
    \item \textbf{Automated Evaluation of Instruction Tuning (PandaLM):} 
    Traditional metrics cannot easily evaluate if a model followed complex, non-functional constraints in coding tasks (e.g., "write efficient python code"). \textbf{PandaLM} \cite{Chen2024SurveyCodeGen} is a specialized judge model trained to distinguish superior instruction-following capabilities in LLMs, providing a reproducible and automated alternative to human evaluation for tuning code models.
    
    \item \textbf{Multi-turn Coding Dialogue Assessment (MT-Bench):}
    In interactive software engineering scenarios (e.g., debugging or iterative code refinement), static metrics fail to capture context retention. Benchmarks like \textbf{MT-Bench} \cite{Chen2024SurveyCodeGen} utilize GPT-4 as a judge to grade multi-turn conversations, specifically assessing the model's ability to maintain logic and correct errors across sequential coding prompts.
    
    \item \textbf{Reference-free Quality Estimation (GPTScore):}
    For code explanation and documentation tasks, ground truth references are often unavailable or varied. \textbf{GPTScore} \cite{Lin2024SWC} enables reference-free evaluation by using the probabilities of a pre-trained model to assess the fluency, coherence, and consistency of generated technical text, offering a proxy for "readability" and "explainability" in software documentation.
    
    \item \textbf{Subjective Code Quality Analysis:}
    Unlike functional correctness (which is binary), code quality attributes such as \textit{conciseness}, \textit{clarity}, and \textit{modularity} are subjective. LLM-as-a-judge approaches allow for the evaluation of these subjective qualities by prompting the judge model with specific rubrics, bridging the gap between automated metrics and human code review standards \cite{Chen2024SurveyCodeGen}.
\end{itemize}

\subsubsection*{Comparative Summary}
Below is a comparison of these LLM-as-a-Judge approaches:

\begin{table}[H]
    \centering
    \caption{Comparison of LLM-as-a-Judge Approaches}
    \label{tab:llm_judge_approaches}
    \begin{tabular}{|p{3cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|}
    \hline
    \textbf{Metric} & \textbf{Base Model} & \textbf{Input Modality} & \textbf{Task Domain} \\
    \hline
    GPTScore & LLM (e.g., GPT-3, GPT-4) & Text-only (Context, Reference, Candidate) & General Text Generation (NLP) \\
    \hline
    GPT-4V Overall Rating & Multimodal LLM (GPT-4V) & Multimodal (Image + Code) & Code Generation from Plots \\
    \hline
    \end{tabular}
\end{table}


\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Lin2024SWC,
Chen2024SurveyCodeGen,
}
\fussy
