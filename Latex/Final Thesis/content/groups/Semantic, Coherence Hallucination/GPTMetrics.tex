\subsection{GPT-based Metrics (LLM-as-a-Judge)}

\subsubsection*{Introduction}
A recent paradigm in evaluating Large Language Models (LLMs) involves using an LLM itself as the evaluator. This approach, often called ``LLM-as-a-judge,'' leverages the advanced reasoning and language understanding capabilities of models like GPT to assess the quality of generated text or code.

These metrics work by crafting a specific prompt that instructs a powerful model (e.g., GPT-4) to compare a candidate output against a reference or a set of criteria and provide a score. This method has gained prominence because its results often show a \textbf{higher correlation with human judgments} than traditional metrics.

Two prominent examples of this approach are \textbf{GPTScore} (for general text evaluation) and \textbf{GPT-4V Overall Rating} (for multimodal code evaluation).

\subsubsection*{1. GPTScore}

\textbf{Definition} \\
GPTScore is an automatic evaluation method that uses an LLM (like GPT-3 or GPT-4) to score the quality of generated text. It operates by providing the ``judge'' LLM with the context, a reference text, and the candidate (model-generated) text. It prompts the LLM to provide:
\begin{itemize}
    \item A \textbf{final score} (e.g., on a 1-5 scale).
    \item A \textbf{detailed evaluation} (a textual explanation) for its assessment.
\end{itemize}

\textbf{Purpose} \\
To provide a new automatic evaluation metric for text generation that achieves a higher correlation with human judgments than previous metrics like BLEU or ROUGE.

\textbf{Applications} \\
\begin{itemize}
    \item LLM Evaluation / NLP
    \item General Text Generation
\end{itemize}

\textbf{Advantages}
\begin{itemize}
    \item \textbf{High Human Correlation}: Has been shown to correlate more strongly with human evaluation.
    \item \textbf{Explainable}: Unlike traditional metrics that output only a number, GPTScore generates a detailed textual explanation for its score.
\end{itemize}

\subsubsection*{2. GPT-4V Overall Rating}

\textbf{Definition} \\
The GPT-4V Overall Rating is a metric used within the \textbf{Plot2Code} benchmark to evaluate multimodal code generation (i.e., generating code from an image of a plot). It specifically uses the GPT-4V (Vision) model to analyze the generated code alongside the source plot image and assign an ``overall quality rating... on a scale of 1-5.''

\textbf{Purpose} \\
To serve as a ``key metric for assessing the overall quality'' of generated code in a complex, multimodal, image-to-code task. It acts as a scalable, automated alternative to expensive human evaluation for this specific domain.

\textbf{Applications} \\
\begin{itemize}
    \item Software Engineering / Code Generation
    \item Human Evaluation / Multimodal Quality
\end{itemize}

\subsubsection*{Comparative Summary}
Below is a comparison of these LLM-as-a-Judge approaches:

\begin{center}
\begin{tabular}{|p{3cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|}
\hline
\textbf{Metric} & \textbf{Base Model} & \textbf{Input Modality} & \textbf{Task Domain} \\
\hline
GPTScore & LLM (e.g., GPT-3, GPT-4) & Text-only (Context, Reference, Candidate) & General Text Generation (NLP) \\
\hline
GPT-4V Overall Rating & Multimodal LLM (GPT-4V) & Multimodal (Image + Code) & Code Generation from Plots \\
\hline
\end{tabular}
\end{center}


\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Lin2024SWC,
Chen2024SurveyCodeGen,
}
\fussy
