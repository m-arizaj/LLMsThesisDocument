\subsection{Rate-based Metrics}

\subsubsection{Introduction}

Rate-based metrics capture how frequently a model achieves success, correctness, or completion relative to the total number of attempts or evaluations. They provide a clear, interpretable measure of task-level performance across diverse evaluation contexts in software engineering, including functional correctness, vulnerability detection, compilation, and efficiency.

In LLM and software engineering evaluation, these metrics serve as general-purpose indicators of both accuracy and reliability in generated artifacts or agentic workflows. Because rates are normalized (ranging from 0 to 1, or 0–100\%), they allow direct comparisons across tasks, datasets, and experimental setups.

\subsubsection{Formula and General Definition}

The general formulation for a rate metric is:

\[
\text{Rate} = \frac{N_{\text{successful outcomes}}}{N_{\text{total outcomes}}}
\]

where:
\begin{itemize}
    \item $N_{\text{successful outcomes}}$ is the count of outputs meeting a given criterion (e.g., passing all tests, producing valid code, completing a task).
    \item $N_{\text{total outcomes}}$ is the total number of evaluated outputs or attempts.
\end{itemize}

This formulation underlies a family of specialized metrics, each adapting the definition of “success” to its specific evaluation dimension.

\subsubsection{Main Variants and Interpretations}

\textbf{1. Success Rate} \\
Measures the proportion of model outputs that meet functional or syntactic correctness criteria. It is one of the most universal metrics in both LLM and software-engineering contexts.

\[
\text{Success Rate} = \frac{N_{\text{passed tasks}}}{N_{\text{total tasks}}}
\]

\begin{itemize}
    \item \textit{Interpretation:} Reflects general correctness and execution reliability.
    \item \textit{Usage:} Common in code generation, autonomous agents, and benchmark evaluations to assess whether outputs compile or execute successfully.
    \item \textit{Example:} Measuring how many generated Python solutions pass all unit tests.
    \item \textit{Used in:} \cite{Nascimento2024LLM4DS, Wang2024AutonomousAgentsSurvey}
\end{itemize}

\textbf{2. Acceptance Rate} \\
Quantifies the proportion of generated outputs that are \textit{accepted} by a downstream validator, reviewer, or automated test pipeline.

\[
\text{Acceptance Rate} = \frac{N_{\text{accepted outputs}}}{N_{\text{submitted outputs}}}
\]

\begin{itemize}
    \item \textit{Interpretation:} Serves as a proxy for productivity and quality of code suggestions or patches.
    \item \textit{Application:} Often used in software generation systems or IDE-based evaluations where human or system approval defines success.
    \item \textit{Used in:} \cite{Paul2024BenchmarksMetricsCodeGen}
\end{itemize}

\textbf{3. Insecure Code Detection Rate} \\
Evaluates how effectively a system identifies insecure or vulnerable code snippets.

\[
\text{Detection Rate} = \frac{N_{\text{correctly detected insecure samples}}}{N_{\text{total insecure samples}}}
\]

\begin{itemize}
    \item \textit{Interpretation:} Analogous to recall for the “insecure” class; high values indicate strong detection coverage.
    \item \textit{Relevance:} Fundamental for LLMs designed for \textit{security evaluation, static analysis, and vulnerability repair} tasks.
    \item \textit{Used in:} \cite{Anand2024AnalysisLLMCode}
\end{itemize}

\textbf{4. Parsing Success Rate} \\
Determines the proportion of generated code files that compile or parse successfully.

\[
\text{Parsing Success Rate} = \frac{N_{\text{syntactically valid files}}}{N_{\text{generated files}}}
\]

\begin{itemize}
    \item \textit{Interpretation:} A structural correctness measure capturing syntactic validity independently of semantic correctness.
    \item \textit{Used For:} Assessing model robustness to syntax generation errors, especially in large code completion or generation tasks \cite{Zhang2024HumanEvalV}.
\end{itemize}

\textbf{5. Speedup Rate} \\
Measures performance improvement in execution time or resource usage relative to a baseline model or method.

\[
\text{Speedup Rate} = \frac{T_{\text{baseline}}}{T_{\text{method}}}
\]

Alternatively expressed as percentage improvement:

\[
\text{Improvement Rate} = \frac{T_{\text{baseline}} - T_{\text{method}}}{T_{\text{baseline}}}
\]

\begin{itemize}
    \item \textit{Interpretation:} Quantifies runtime efficiency gains; higher values indicate faster or more efficient computation.
    \item \textit{Context:} Useful in evaluating optimization algorithms, code compilation efficiency, or hardware-specific performance.
    \item \textit{Used in:} \cite{Niu2024EvaluatingEfficiency}
\end{itemize}

\textbf{6. Synthesis Success Rate} \\
Applies primarily to hardware or compiled code generation tasks, measuring how often generated artifacts successfully synthesize or compile.

\[
\text{Synthesis Success Rate} = \frac{N_{\text{synthesized and valid outputs}}}{N_{\text{attempted outputs}}}
\]

\begin{itemize}
    \item \textit{Interpretation:} Combines both functional and hardware correctness; crucial for evaluating domain-specific synthesis tasks (e.g., VerilogEval).
    \item \textit{Significance:} Ensures generated solutions are not only syntactically correct but executable and operational.
    \item \textit{Used in:} \cite{Chen2024SurveyCodeGen}
\end{itemize}

\textbf{7. Win Rate} \\
Quantifies how often an agent or model outperforms a baseline or achieves a successful outcome in interactive or competitive tasks.

\[
\text{Win Rate} = \frac{N_{\text{wins}}}{N_{\text{total interactions}}}
\]

\begin{itemize}
    \item \textit{Interpretation:} Used in multi-agent and tool-use settings to measure task completion, decision accuracy, or reward-based performance.
    \item \textit{Context:} Commonly applied in tool-learning and autonomous LLM agent evaluations where success is goal-oriented (e.g., task solved vs. failed).
    \item \textit{Used in:} \cite{Xu2025LLMAgentsToolLearning}
\end{itemize}

\subsubsection{Broader Interpretation}

Rate metrics are among the most interpretable and transferable across domains because they quantify event frequency, not magnitude or ranking.
They form the foundation of \textit{task-level functional evaluation}, allowing researchers to express performance as simple ratios of success versus failure.

However:
\begin{itemize}
    \item They do not capture the degree of partial correctness (only binary outcomes).
    \item High rates may obscure subtle quality differences among correct outputs.
    \item Complementary metrics like F1, Precision, or Execution Time can add nuance in such cases.
\end{itemize}