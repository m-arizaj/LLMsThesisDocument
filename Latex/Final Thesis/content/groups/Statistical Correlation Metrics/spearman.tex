\subsection{Spearman's \texorpdfstring{$\rho$}{rho}}

Spearman's $\rho$ (rho), also referred to as Spearman's rank correlation coefficient ($r_s$), is a non-parametric statistical measure used to evaluate the strength and direction of a monotonic relationship between two ranked variables. Unlike Pearson's $r$, it does not assume linearity, normality, or equal variances, making it suitable for ordinal data and non-linear relationships.
In software engineering and LLM evaluation, Spearman's $\rho$ is widely used to measure how consistently automatic metrics rank model outputs compared with human judgments. A high $\rho$ value indicates that two rankings (human preference vs. metric-based ordering) follow the same relative order.

For data without significant ties, Spearman's $\rho$ is computed as:

\begin{equation}
\rho = 1 - \frac{6 \sum_{i=1}^{n} d_i^2}{n(n^2 - 1)}
\end{equation}

Where:
\begin{itemize}
    \item $d_i$ is the rank difference of the $i$-th pair,
    \item $n$ is the number of paired observations.
\end{itemize}

When ties are present, Spearman's $\rho$ is defined as the Pearson correlation coefficient between the \textit{rank-transformed variables}:

\begin{equation}
\rho = \frac{\mathrm{cov}(rg_X,\, rg_Y)}{\sigma_{rg_X}\,\sigma_{rg_Y}}
\end{equation}

This formulation, also discussed in Xu et al. (2010), generalizes Spearman's $\rho$ to datasets where ranks are not strictly unique \cite{Xu2010SpearmanKendall}.

\subsubsection{Variants}

\begin{itemize}
    \item \textbf{Spearman's $r_s$:} Traditional notation for the coefficient based on ranked data.
    \item \textbf{Spearman's $\rho$:} Formal statistical notation used interchangeably with $r_s$ in empirical evaluation studies.
\end{itemize}

Both formulations refer to the same underlying measure.

\subsubsection{Applications in Software Engineering}

Spearman's $\rho$ has been used across multiple SE and LLM evaluation benchmarks to examine the \textit{rank-level agreement} between automated metrics and human evaluators.
Within the broader SE literature \cite{Zhou2025LLMAsJudge, Zhuo2023ICEScore, Zhou2023CodeBERTScore, Dong2023CodeScore, Tong2024CodeJudge}, it is applied to:

\begin{itemize}
    \item \textbf{Human–metric correlation:} Measuring how well a metric's rankings of generated code or summaries align with human preferences.
    \item \textbf{Metric comparison:} Comparing the ordering induced by metrics such as BLEU, CodeBLEU, or Pass@k against human correctness ratings.
    \item \textbf{Benchmark-level ranking stability:} Evaluating whether different metrics rank LLMs consistently across tasks such as code generation, summarization, and explanation.
\end{itemize}

Although the Xu et al. (2010) paper focuses on theoretical properties of correlation coefficients (bias and MSE in normal/contaminated models), Spearman's $\rho$ is frequently adopted in SE evaluation settings for its robustness to non-linear relationships between predicted metric scores and human judgments \cite{Xu2010SpearmanKendall}.

\subsubsection{Interpretation}

For software engineering evaluation:

\begin{itemize}
    \item \textbf{$\rho$ close to +1:} Strong positive monotonic relationship — metric and human rankings agree.
    \item \textbf{$\rho$ around 0:} No monotonic relationship — rankings are unrelated.
    \item \textbf{$\rho$ close to -1:} Strong negative monotonic relationship — rankings move in opposite directions.
\end{itemize}

Because it does not assume linearity, Spearman's $\rho$ is often preferred over Pearson's $r$ when dealing with ordinal human ratings, non-linear relationships between metric scores and correctness, or heterogeneous metric scales.