\subsection{Helpfulness}

Helpfulness is a key category of \textbf{human-centered evaluation} for Large Language Models (LLMs). It is a qualitative assessment that measures how well an LLM's response satisfies a user's request, answers their question, or accomplishes the instructed task.

It is the primary metric for evaluating a model's utility and capability, often balanced against ``Harmlessness'' to ensure that responses are not only safe but also functional and useful.

Helpfulness is defined as the measure of a model's ability to provide high-quality, relevant, and accurate answers that align with the user's intent. It quantifies the practical utility of the system.

\subsubsection*{Measurement Methodology}
Helpfulness is measured primarily through \textbf{Human Evaluation}, often using a ``battle'' or pairwise comparison format:

\begin{itemize}
    \item \textbf{Chatbot Arena}: Human users interact with two different anonymous models, ask them the same question, and vote for which model provided the ``better'' (i.e., more helpful and thorough) response.
    \item \textbf{HELM}: Uses human evaluations to score model outputs on various criteria, including accuracy and helpfulness for specific tasks.
\end{itemize}

In these evaluations, raters score based on relevance, accuracy, completeness, and overall quality.

\subsubsection*{Purpose}
The purpose of measuring helpfulness is to \textbf{quantify the utility and performance} of an LLM from a user's perspective. It serves as the main metric for determining if a model is proficient at following instructions, which is the primary goal of assistant-style models.

\subsubsection{Applications}

In the context of Software Engineering, \textit{helpfulness} translates directly to the model's capability to assist developers in writing, debugging, and understanding code. Unlike general conversation, helpfulness here is strictly bound by functional correctness and technical precision.

\begin{itemize}
    \item \textbf{Functional Code Synthesis}: The primary application of helpfulness is the generation of executable and functionally correct code. Benchmarks like HumanEval and APPS are used to evaluate whether the model can produce code that passes unit tests based on a natural language description. A ``helpful'' model must produce logic that compiles and solves the problem without hallucinating non-existent libraries \cite{Liang2022HELM}.

    \item \textbf{Multi-turn Debugging and Reasoning}: In real-world engineering, helpfulness requires sustaining a technical context over multiple turns. Models are evaluated on their ability to diagnose errors in provided code, explain the root cause, and iteratively refine the solution based on user feedback. This capability is critical for complex tasks where the first attempt may not be perfect \cite{zheng2023judging}.

    \item \textbf{Technical Instruction Following}: Helpfulness in software engineering often involves strict adherence to constraints (e.g., ``use only the Pandas library'' or ``output in JSON format''). Evaluators assess the model's ability to follow these specific technical requirements, as failing to do so renders the code useless for integration into existing systems \cite{zheng2023judging}.

    \item \textbf{Code Explanation and Documentation}: Beyond generating code, helpfulness is measured by the model's ability to explain complex algorithms or generate documentation for legacy codebases. This aids developers in maintaining and understanding large systems, serving as an interactive knowledge base \cite{Chang2023SurveyLLMs}.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Aligns with User Goals}: Directly measures what most users care aboutâ€”getting a good answer.
    \item \textbf{Captures Nuance}: Human evaluators can reward responses that are not just factually correct but also well-written, comprehensive, and appropriately toned.
    \item \textbf{Gold Standard for Utility}: It serves as the most reliable ``ground truth'' for a model's practical capability, often revealing gaps that automated metrics miss.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Subjectivity}: What one user finds ``helpful,'' another might find overly verbose or simplistic. The metric is inherently subjective.
    \item \textbf{Scalability}: Collecting human judgments is expensive and slow compared to running automated computational metrics.
    \item \textbf{Evaluator Bias}: Ratings can be influenced by the knowledge, preferences, and potential biases of the human evaluators.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Chang2023SurveyLLMs,
Liang2022HELM,
zheng2023judging,
}
\fussy
