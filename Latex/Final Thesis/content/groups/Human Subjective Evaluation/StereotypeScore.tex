\subsection{Stereotype Score (ss)}

The \textbf{Stereotype Score (ss)} is an automatic evaluation metric used in the \textbf{StereoSet} benchmark to measure stereotypical bias in large language models.

It is not a standalone metric but rather a key component of the StereoSet evaluation framework. It is designed to quantify a model's preference for text containing a social stereotype versus text that is ``anti-stereotype'' (i.e., neutral or positive).

The Stereotype Score is calculated based on a model's performance on the StereoSet dataset, where it chooses between a stereotypical option and an anti-stereotypical option given a context.

\begin{equation}
    ss = \frac{\text{Count}(\text{Model prefers stereotype})}{\text{Total Instances}} \times 100
\end{equation}

\textbf{Ideal Score}: An ideal, unbiased model chooses the stereotype and anti-stereotype options with equal probability, resulting in a score of:
\begin{equation}
    ss_{\text{ideal}} = 50
\end{equation}

This score is used to calculate the benchmark's main metric, the \textbf{Idealized CAT (iCAT) Score}, combining it with the language modeling score ($lms$):
\begin{equation}
    \text{iCAT} = lms \cdot \frac{\min(ss, 100-ss)}{50}
\end{equation}

\subsubsection*{Purpose}
The purpose of the Stereotype Score is to explicitly measure a model's stereotypical bias. By presenting a direct choice between a stereotypical association and a non-stereotypical one, it quantifies the model's tendency to select the harmful stereotype, providing a clear signal of its learned biases.

\subsubsection{Applications in Software Engineering}

The Stereotype Score is applied in Software Engineering primarily as a quantitative metric for **AI Safety and Fairness Testing**, ensuring that Large Language Models (LLMs) are suitable for deployment in user-facing software systems.

\begin{itemize}
    \item \textbf{Model Selection and Benchmarking}:
    The metric allows software engineers to compare different pretrained models (e.g., BERT, GPT-2, RoBERTa) based on their bias levels before integrating them into a system. By using the \textbf{StereoSet Leaderboard}, developers can select models that strike the best balance between language capability (Language Modeling Score) and fairness (Stereotype Score), avoiding models that exhibit "strong stereotypical biases" \cite{nadeem2021stereoset}.

    \item \textbf{Risk Assessment for Deployment}:
    Since LLMs are widely deployed as services (e.g., on Google Cloud or Amazon AWS) to serve millions of users, the Stereotype Score functions as a **release criterion**. It helps quantify the potential "adverse effects" of a model, allowing engineers to assess the risk of the model amplifying harmful biases regarding gender, race, religion, or profession in production environments \cite{nadeem2021stereoset}.

    \item \textbf{Diagnostic Testing for Downstream Tasks}:
    The score is used to predict and diagnose bias propagation in downstream software applications. If a foundational model has a high Stereotype Score, it is likely to introduce bias into dependent tasks such as **coreference resolution** (e.g., assuming "doctor" is male) or **sentiment analysis** (e.g., associating certain demographics with negative sentiment). The metric serves as an early indicator of these "extrinsic" failures \cite{nadeem2021stereoset}.

    \item \textbf{Continuous Monitoring of Model Evolution}:
    The framework supports the continuous evaluation of new language models through a hidden test set. This allows the software engineering community to "track the bias of future language models" over time, ensuring that newer iterations of software components do not regress in terms of fairness standards \cite{nadeem2021stereoset}.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Contextual}: Measures bias within a sentence or discourse context, which is more nuanced than simple word-embedding associations.
    \item \textbf{Interpretable}: The score is a simple percentage (0-100), with a clear target (50) for unbiased behavior.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Validity of ``Anti-stereotype''}: The concept of an anti-stereotype option is controversial. It may not reflect real-world power dynamics, making the 50/50 target a questionable indicator of true fairness.
    \item \textbf{Ambiguity}: The dataset has been criticized for ambiguities regarding which stereotypes are actually captured.
    \item \textbf{Ranking vs. Generation}: It measures preference by ranking pre-written sentences, which may not fully capture the model's tendency to \textit{produce} stereotypes in open-ended generation.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
nadeem2021stereoset,
}
\fussy
