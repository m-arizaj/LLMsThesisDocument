\subsection{Toxicity}

\subsubsection{Introduction}

Toxicity measures the extent to which text generated by a model contains harmful, offensive, abusive, or otherwise inappropriate content. In Large Language Models (LLMs) and Software Engineering (SE), toxicity metrics help evaluate the safety and ethical reliability of generated code comments, commit messages, documentation, and natural-language responses. They are essential for ensuring professional communication quality and preventing the propagation of harmful language in development workflows.

Traditional toxicity evaluations, such as \textit{RealToxicityPrompts} (Gehman et al., 2020) \cite{Gehman2020RealToxicityPrompts} and \textit{ToxiGen} (Hartvigsen et al., 2022) \cite{Hartvigsen2022ToxiGen}, use curated datasets of toxic and non-toxic text to score model outputs using classifier-based toxicity probabilities.
More recent frameworks like \textit{LATTE} (Koh et al., 2024) introduce LLM-based toxicity evaluators that contextualize and score toxicity along multiple dimensions, reflecting a shift toward more nuanced, value-aligned evaluation \cite{Koh2024LATTE}.

\subsubsection{Formula and Structure}

Toxicity metrics are commonly defined using classifier probabilities or aggregated toxicity rates.

\textbf{1. Toxicity Probability}
A classifier or evaluator assigns a toxicity score \cite{Gehman2020RealToxicityPrompts}:
\[
T(x) = P(\text{toxic} \mid x)
\]
where $x$ is the generated text.
This matches classifier outputs such as the Perspective API score used in RealToxicityPrompts \cite{Gehman2020RealToxicityPrompts}.

\textbf{2. Toxicity Rate}
The proportion of generated outputs classified as toxic:
\[
\text{Toxicity Rate} = \frac{N_{\text{toxic}}}{N_{\text{total}}}
\]

\textbf{3. Expected Toxicity}
The average toxicity across $N$ generations \cite{Gehman2020RealToxicityPrompts}:
\[
E[T] = \frac{1}{N} \sum_{i=1}^{N} T(x_i)
\]

These formulations quantify how often toxicity occurs (rate) and how severe it is (probability). LLM-based evaluators like LATTE extend this with multi-dimensional scores that account for contextual meaning, intent, and ethical alignment \cite{Koh2024LATTE}.

\subsubsection{Variants}

\begin{itemize}
    \item \textbf{Toxicity (Binary):} Classifier-based categorization of text as toxic or non-toxic, common in early safety benchmarks \cite{Gehman2020RealToxicityPrompts}.
    \item \textbf{Toxicity Rate:} A frequency-based measure of how often a model generates harmful content.
    \item \textbf{Contextual Toxicity (LLM-Based):} Introduced in LATTE, where LLM evaluators score toxicity across \textit{evaluation dimensions} (e.g., harmful intent, demeaning phrasing, stereotyping) \cite{Koh2024LATTE}. These scores incorporate semantic context rather than relying solely on surface-level lexical cues.
\end{itemize}

Variants can be calibrated to different cultural norms, bias sensitivities, or ethical guidelines \cite{Hartvigsen2022ToxiGen}.

\subsubsection{Applications in Software Engineering}

In SE contexts, toxicity metrics are used to evaluate:
\begin{itemize}
    \item Generated \textit{code comments},
    \item Automated \textit{documentation},
    \item \textit{Commit messages},
    \item SE-related natural-language explanations.
\end{itemize}

Frameworks such as \textit{HELM} apply toxicity scoring for safety analysis, while benchmarks like \textit{RealToxicityPrompts} and \textit{ToxiGen} quantify toxicity robustness under varied prompts \cite{Gehman2020RealToxicityPrompts, Hartvigsen2022ToxiGen}. These evaluations help ensure that AI-assisted development tools produce safe, respectful, and workplace-appropriate language.

\subsubsection{Interpretation}

A high toxicity score indicates stronger presence of harmful, abusive, or discriminatory content, while a low score signals safer and more ethically aligned outputs.

However, toxicity detection is context-dependent:
\begin{itemize}
    \item Classifier-based systems may encode dataset or cultural biases \cite{Hartvigsen2022ToxiGen}.
    \item Scoring can over-penalize minority dialects or reclaimed terms (documented in RealToxicityPrompts and ToxiGen) \cite{Gehman2020RealToxicityPrompts, Hartvigsen2022ToxiGen}.
    \item LLM-based evaluators (LATTE) address these limitations by applying contextual reasoning and multi-dimension judgment \cite{Koh2024LATTE}.
\end{itemize}

In SE environments, monitoring toxicity supports ethical AI adoption and helps prevent harmful language from propagating into technical ecosystems.

\subsubsection{Additional References}
This metric and its variants are referenced and/or used in the following papers:

\cite{Chang2023SurveyLLMs, Liang2022HELM, Gallegos2024BiasFairness}