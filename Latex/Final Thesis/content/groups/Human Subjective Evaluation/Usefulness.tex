\subsection{Usefulness Score}

\subsubsection*{Definition}
The \textbf{Usefulness Score} is a human-centric, subjective evaluation metric designed to assess the quality of generated code snippets based on their helpfulness to a human developer.

It is not an automated metric but rather a rubric for human annotation, where experienced developers grade a generated code snippet on a 0-4 discrete scale. This score is a key metric for measuring human preference alignment.

\subsubsection*{Grading Scale}
The metric is defined by the following 5-point scale, as detailed in the \textbf{ICE-Score} paper's appendix:

\begin{description}
    \item[Score 0 (Totally Useless)] ``Snippet is not at all helpful, it is irrelevant to the problem.''
    \item[Score 1 (Slightly Helpful)] ``Snippet is slightly helpful, it contains information relevant to the problem, but it is easier to write the solution from scratch.''
    \item[Score 2 (Somewhat Helpful)] ``Snippet is somewhat helpful, it requires significant changes (compared to the size of the snippet), but is still useful.''
    \item[Score 3 (Helpful / Almost Useful)] ``Snippet is helpful, but needs to be slightly changed to solve the problem.''
    \item[Score 4 (Very Helpful)] ``Snippet is very helpful, it solves the problem.''
\end{description}

\subsubsection*{Purpose}
The purpose of the Usefulness Score is to capture a human developer's judgment of a code snippet's quality, often missed by traditional token-matching metrics like BLEU. It directly measures how well a generated snippet satisfies a user's requirements and determines how much effort is saved (or wasted) by using it. It serves as a ground truth for training automated metrics (like ICE-Score).

\subsubsection*{Domains}
\begin{itemize}
    \item Software Engineering
    \item Code Generation
    \item Human Preference Alignment
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Human-Aligned}: By definition, this metric has a high correlation with human judgment, which token-matching metrics often lack.
    \item \textbf{Captures Semantics}: It assesses the \textbf{semantic logic} and \textbf{practical value} of the code, not just its textual similarity to a single reference answer.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Cost and Scalability}: Relies on manual grading from ``experienced software developers,'' making it expensive and difficult to scale compared to automated metrics.
    \item \textbf{Subjectivity}: As a human-based score, it is subject to variability. (e.g., even human-written reference code in CoNaLa only achieved an average score of 3.4/4).
\end{itemize}

% a√±adir estas entradas al archivo .bib


\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Zhuo2023ICEScore,
Evtikhiev2023OutOfBLEU,
yin2018learning,
}
\fussy
