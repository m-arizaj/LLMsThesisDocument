\subsection{Human Metrics}

\subsubsection{Introduction}

Human metrics refer to evaluation methods that rely on human judgment to assess the quality, correctness, and usefulness of model-generated outputs. They are considered the gold standard for evaluating complex aspects that automatic metrics cannot fully capture, such as semantic adequacy, clarity, usefulness, and naturalness \cite{Liu2022RevisitingGoldStandard}.

In software engineering, these metrics are used to evaluate LLM-generated code, explanations, summaries, and documentation, ensuring that automated systems align with human standards of reasoning and readability.

\subsubsection{Formula and Structure}

Unlike quantitative metrics, human metrics are qualitative or semi-quantitative. They generally rely on rating scales, comparative judgments, or ranking procedures.

A typical evaluation setup involves $n$ human raters providing scores for one or more criteria \cite{Liu2022RevisitingGoldStandard}:

\[
H = \frac{1}{n} \sum_{i=1}^{n} s_i
\]

Where:
\begin{itemize}
    \item $H$ is the aggregated human score.
    \item $s_i$ is the score given by rater $i$ (usually on a 1–5 or 1–10 scale).
\end{itemize}

Human ratings may also be normalized or aggregated using statistical measures such as mean, median, or inter-rater reliability coefficients like Cohen's $\kappa$ or Krippendorff's $\alpha$ to assess consistency among raters \cite{Liu2022RevisitingGoldStandard}.

\subsubsection{Variants}

\begin{itemize}
    \item \textbf{Human Evaluation:} The general framework of human-centered evaluation applied to generated code, documentation, or text \cite{Bektas2025CriticalReview, Hu2025BenchmarksSE}.
    \item \textbf{Human Judgment Score:} Aggregated score from human annotators based on correctness, style, or usefulness to compare with other metrics \cite{Evtikhiev2023OutOfBLEU}.
    \item \textbf{Human Rater Score (mean / max):} Considers either the average or best (maximum) judgment among annotators \cite{Srivastava2022BeyondImitation}.
    \item \textbf{Naturalness (Human Evaluation):} Assesses whether generated code or text appears natural and human-like, often in readability and code synthesis studies \cite{Wang2022ReCode}.
\end{itemize}

\subsubsection{Interpretation}

Human metrics are essential for validating the performance of LLMs in real-world software engineering contexts.
They serve as a bridge between quantitative model outputs and qualitative human perception, offering an interpretable benchmark for tasks where correctness alone is insufficient \cite{Liu2022RevisitingGoldStandard}.

Key considerations:
\begin{itemize}
    \item High human evaluation scores indicate that the model's outputs align closely with human expectations in clarity, coherence, and utility.
    \item Inter-rater reliability ensures the consistency of judgments, validating the robustness of human evaluations.
    \item Human-centered evaluation frameworks combine these scores with automatic metrics to achieve a balanced understanding of model quality \cite{Liu2022RevisitingGoldStandard}.
\end{itemize}

While human evaluation is expensive and time-consuming, it remains the most reliable reference point for validating automatic evaluation frameworks.

\subsubsection{Additional References}
This metric and its variants are referenced and/or used in the following papers:

\cite{Bektas2025CriticalReview, Hu2025BenchmarksSE, Srivastava2022BeyondImitation, Wang2022ReCode, Evtikhiev2023OutOfBLEU}