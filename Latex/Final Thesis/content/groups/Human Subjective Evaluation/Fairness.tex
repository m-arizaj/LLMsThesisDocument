\subsection{Fairness}

Fairness is a critical category of ethical evaluation for Large Language Models (LLMs). It is concerned with assessing whether a model's outputs and behaviors result in disparate treatment or outcomes between different social groups.

This evaluation is essential as LLMs, typically trained on massive, uncurated Internet-scale data, can learn, perpetuate, and even amplify harmful social biases. These biases include stereotypes, misrepresentations, and derogatory language that disproportionately affect marginalized communities.

In the context of LLMs, fairness is often operationalized into two main concepts:

\begin{itemize}
    \item \textbf{Group Fairness}: Requires that a statistical outcome measure (like accuracy or toxicity score) has approximate parity across different social groups.
    \item \textbf{Individual Fairness}: Requires that individuals who are similar with respect to a specific task are treated similarly by the model.
\end{itemize}

\subsubsection*{Measurement Approaches}
Fairness is not a single metric but a broad category. The metrics used to measure it are typically organized by the part of the model they operate on:

\begin{itemize}
    \item \textbf{Embedding-Based Metrics}: Measure bias by computing distances in the model's vector space. They check the association between neutral words (e.g., ``doctor'') and identity-related words (e.g., ``man,'' ``woman'').
    \begin{itemize}
        \item \textit{Examples}: WEAT, SEAT.
    \end{itemize}
    
    \item \textbf{Probability-Based Metrics}: Use model-assigned token probabilities to estimate bias, often by comparing the likelihood of ``counterfactual'' sentences (identical except for a perturbed social group attribute).
    \begin{itemize}
        \item \textit{Examples}: CrowS-Pairs Score, DisCo, Log-Probability Bias Score.
    \end{itemize}
    
    \item \textbf{Generated Text-Based Metrics}: Analyze the final free-text output generated by the model.
    \begin{itemize}
        \item \textit{Distribution}: Comparing co-occurrence counts of group terms with stereotypes.
        \item \textit{Classifiers}: Using auxiliary classifiers (e.g., toxicity) on outputs.
        \item \textit{Lexicons}: Comparing output words against lists of harmful terms.
    \end{itemize}
\end{itemize}

\subsubsection*{Purpose}
The primary purpose is to \textbf{identify, measure, and mitigate harmful social biases}. This enables researchers to understand and prevent the propagation of stereotypes and derogatory language that disproportionately harm marginalized communities.

\subsubsection*{Domains}
\begin{itemize}
    \item General LLM Evaluation (Ethical Evaluation)
    \item Text Generation \& Question-Answering
    \item Classification (e.g., Toxicity Detection)
    \item Machine Translation \& Information Retrieval
\end{itemize}

\subsubsection*{Benchmarks}
Major benchmarks used for fairness evaluation include:
\begin{itemize}
    \item \textbf{General}: HELM, Chatbot Arena, HolisticBias.
    \item \textbf{Specific}: BBQ (Bias Benchmark for QA), CrowS-Pairs, StereoSet, BOLD, RealToxicityPrompts, Winogender, WinoBias.
\end{itemize}

\subsection{Applications}

Fairness metrics are increasingly integrated into the software development lifecycle (SDLC) to ensure that systems do not perpetuate historical power asymmetries or inflict harm on specific social groups. These metrics are applied primarily to mitigate two types of harms: \textbf{allocational harms} (unfair distribution of resources or opportunities) and \textbf{representational harms} (stereotyping, erasure, or demeaning portrayals) \cite{blodgett2020language, Gallegos2024BiasFairness}.

Key domains where fairness metrics are actively applied include:

\begin{itemize}
    \item \textbf{Algorithmic Hiring and Recruitment}: Fairness metrics are used to audit resume screening tools and automated interview systems. The goal is to prevent \textit{allocational harms}, such as the disparate exclusion of candidates based on gender, ethnicity, or names, ensuring that qualified individuals are not systematically denied employment opportunities \cite{Gallegos2024BiasFairness, blodgett2020language}.

    \item \textbf{Content Moderation and Safety}: In the development of toxicity detection systems, fairness metrics are essential to ensure that dialectal variations (e.g., African-American English) are not unfairly flagged as toxic or offensive. This prevents the silencing of marginalized voices and ensures equitable user safety \cite{blodgett2020language, Gallegos2024BiasFairness}.

    \item \textbf{Healthcare and Clinical Decision Support}: Developers apply fairness evaluations to medical diagnostic models and chatbots to prevent bias in patient care. This helps ensure that medical advice or resource allocation is not skewed by demographic proxies, which could lead to severe health disparities \cite{Chang2023SurveyLLMs, Gallegos2024BiasFairness}.

    \item \textbf{Machine Translation}: Metrics are applied to detect and mitigate gender bias in translation software, specifically to prevent systems from defaulting to masculine forms for professional roles (e.g., translating "doctor" as male) and feminine forms for others, thereby enforcing exclusionary norms \cite{Gallegos2024BiasFairness, blodgett2020language}.

    \item \textbf{Search and Information Retrieval}: Fairness metrics evaluate ranking algorithms to ensure that the visibility of information does not reinforce stereotypes or erase specific social groups from search results \cite{Chang2023SurveyLLMs, Gallegos2024BiasFairness}.
\end{itemize}


\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Identifies Harms}: Allows precise definition and measurement of specific social harms.
    \item \textbf{Enables Mitigation}: Provides necessary feedback to develop bias mitigation techniques.
    \item \textbf{Promotes Equity}: Empowers developers to prevent the amplification of historical power asymmetries.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Weak Downstream Correlation}: Embedding and probability metrics often show a weak relationship with actual bias observed in downstream applications.
    \item \textbf{Dataset Validity}: Widely-used datasets (e.g., StereoSet, CrowS-Pairs) suffer from ambiguities, raising questions about whether they accurately measure real-world stereotypes.
    \item \textbf{Biased Classifiers}: Metrics relying on auxiliary classifiers (e.g., for toxicity) can be unreliable if the classifier itself is biased (e.g., against African-American English).
    \item \textbf{Subjectivity}: Fairness is ``highly subjective, value-dependent, and non-static,'' making a universal benchmark difficult to establish.
\end{itemize}


\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Chang2023SurveyLLMs,
Gallegos2024BiasFairness,
blodgett2020language,
parrish2022bbq,
nangia2020crows,
nadeem2021stereoset,
}
\fussy
