\subsection{Fairness}

\subsubsection*{Introduction}
Fairness is a critical category of ethical evaluation for Large Language Models (LLMs). It is concerned with assessing whether a model's outputs and behaviors result in disparate treatment or outcomes between different social groups.

This evaluation is essential as LLMs, typically trained on massive, uncurated Internet-scale data, can learn, perpetuate, and even amplify harmful social biases. These biases include stereotypes, misrepresentations, and derogatory language that disproportionately affect marginalized communities.

\subsubsection*{Definition}
In the context of LLMs, fairness is often operationalized into two main concepts:

\begin{itemize}
    \item \textbf{Group Fairness}: Requires that a statistical outcome measure (like accuracy or toxicity score) has approximate parity across different social groups.
    \item \textbf{Individual Fairness}: Requires that individuals who are similar with respect to a specific task are treated similarly by the model.
\end{itemize}

\subsubsection*{Measurement Approaches}
Fairness is not a single metric but a broad category. The metrics used to measure it are typically organized by the part of the model they operate on:

\begin{itemize}
    \item \textbf{Embedding-Based Metrics}: Measure bias by computing distances in the model's vector space. They check the association between neutral words (e.g., ``doctor'') and identity-related words (e.g., ``man,'' ``woman'').
    \begin{itemize}
        \item \textit{Examples}: WEAT, SEAT.
    \end{itemize}
    
    \item \textbf{Probability-Based Metrics}: Use model-assigned token probabilities to estimate bias, often by comparing the likelihood of ``counterfactual'' sentences (identical except for a perturbed social group attribute).
    \begin{itemize}
        \item \textit{Examples}: CrowS-Pairs Score, DisCo, Log-Probability Bias Score.
    \end{itemize}
    
    \item \textbf{Generated Text-Based Metrics}: Analyze the final free-text output generated by the model.
    \begin{itemize}
        \item \textit{Distribution}: Comparing co-occurrence counts of group terms with stereotypes.
        \item \textit{Classifiers}: Using auxiliary classifiers (e.g., toxicity) on outputs.
        \item \textit{Lexicons}: Comparing output words against lists of harmful terms.
    \end{itemize}
\end{itemize}

\subsubsection*{Purpose}
The primary purpose is to \textbf{identify, measure, and mitigate harmful social biases}. This enables researchers to understand and prevent the propagation of stereotypes and derogatory language that disproportionately harm marginalized communities.

\subsubsection*{Domains}
\begin{itemize}
    \item General LLM Evaluation (Ethical Evaluation)
    \item Text Generation \& Question-Answering
    \item Classification (e.g., Toxicity Detection)
    \item Machine Translation \& Information Retrieval
\end{itemize}

\subsubsection*{Benchmarks}
Major benchmarks used for fairness evaluation include:
\begin{itemize}
    \item \textbf{General}: HELM, Chatbot Arena, HolisticBias.
    \item \textbf{Specific}: BBQ (Bias Benchmark for QA), CrowS-Pairs, StereoSet, BOLD, RealToxicityPrompts, Winogender, WinoBias.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Identifies Harms}: Allows precise definition and measurement of specific social harms.
    \item \textbf{Enables Mitigation}: Provides necessary feedback to develop bias mitigation techniques.
    \item \textbf{Promotes Equity}: Empowers developers to prevent the amplification of historical power asymmetries.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Weak Downstream Correlation}: Embedding and probability metrics often show a weak relationship with actual bias observed in downstream applications.
    \item \textbf{Dataset Validity}: Widely-used datasets (e.g., StereoSet, CrowS-Pairs) suffer from ambiguities, raising questions about whether they accurately measure real-world stereotypes.
    \item \textbf{Biased Classifiers}: Metrics relying on auxiliary classifiers (e.g., for toxicity) can be unreliable if the classifier itself is biased (e.g., against African-American English).
    \item \textbf{Subjectivity}: Fairness is ``highly subjective, value-dependent, and non-static,'' making a universal benchmark difficult to establish.
\end{itemize}

% añadir estas entradas a tu archivo .bib

% @inproceedings{blodgett2020language,
%   title={Language (technology) is power: A critical survey of “bias” in NLP},
%   author={Blodgett, S. L. and Barocas, S. and Daumé III, H. and Wallach, H.},
%   booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
%   pages={5454--5476},
%   year={2020},
%   doi={10.18653/v1/2020.acl-main.485}
% }

% @inproceedings{parrish2022bbq,
%   title={BBQ: A hand-built bias benchmark for question answering},
%   author={Parrish, A. and Chen, A. and Nangia, N. and others},
%   booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
%   pages={2086--2105},
%   year={2022},
%   doi={10.18653/v1/2022.findings-acl.165}
% }

% @inproceedings{nangia2020crows,
%   title={CrowS-Pairs: A challenge dataset for measuring social biases in masked language models},
%   author={Nangia, N. and Vania, C. and Bhalerao, R. and Bowman, S. R.},
%   booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
%   pages={1953--1967},
%   year={2020},
%   doi={10.18653/v1/2020.emnlp-main.154}
% }

% @article{nadeem2021stereoset,
%   title={StereoSet: Measuring stereotypical bias in pretrained language models},
%   author={Nadeem, M. and Bethke, A. and Reddy, S.},
%   journal={arXiv preprint arXiv:2004.09456},
%   year={2021},
%   doi={10.48550/arXiv.2004.09456}
% }