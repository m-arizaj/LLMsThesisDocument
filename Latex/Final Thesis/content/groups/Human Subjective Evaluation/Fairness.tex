\subsection{Fairness}

\subsubsection*{Introduction}
Fairness is a critical category of ethical evaluation for Large Language Models (LLMs). It is concerned with assessing whether a model's outputs and behaviors result in disparate treatment or outcomes between different social groups.

This evaluation is essential as LLMs, typically trained on massive, uncurated Internet-scale data, can learn, perpetuate, and even amplify harmful social biases. These biases include stereotypes, misrepresentations, and derogatory language that disproportionately affect marginalized communities.

\subsubsection*{Definition}
In the context of LLMs, fairness is often operationalized into two main concepts:

\begin{itemize}
    \item \textbf{Group Fairness}: Requires that a statistical outcome measure (like accuracy or toxicity score) has approximate parity across different social groups.
    \item \textbf{Individual Fairness}: Requires that individuals who are similar with respect to a specific task are treated similarly by the model.
\end{itemize}

\subsubsection*{Measurement Approaches}
Fairness is not a single metric but a broad category. The metrics used to measure it are typically organized by the part of the model they operate on:

\begin{itemize}
    \item \textbf{Embedding-Based Metrics}: Measure bias by computing distances in the model's vector space. They check the association between neutral words (e.g., ``doctor'') and identity-related words (e.g., ``man,'' ``woman'').
    \begin{itemize}
        \item \textit{Examples}: WEAT, SEAT.
    \end{itemize}
    
    \item \textbf{Probability-Based Metrics}: Use model-assigned token probabilities to estimate bias, often by comparing the likelihood of ``counterfactual'' sentences (identical except for a perturbed social group attribute).
    \begin{itemize}
        \item \textit{Examples}: CrowS-Pairs Score, DisCo, Log-Probability Bias Score.
    \end{itemize}
    
    \item \textbf{Generated Text-Based Metrics}: Analyze the final free-text output generated by the model.
    \begin{itemize}
        \item \textit{Distribution}: Comparing co-occurrence counts of group terms with stereotypes.
        \item \textit{Classifiers}: Using auxiliary classifiers (e.g., toxicity) on outputs.
        \item \textit{Lexicons}: Comparing output words against lists of harmful terms.
    \end{itemize}
\end{itemize}

\subsubsection*{Purpose}
The primary purpose is to \textbf{identify, measure, and mitigate harmful social biases}. This enables researchers to understand and prevent the propagation of stereotypes and derogatory language that disproportionately harm marginalized communities.

\subsubsection*{Domains}
\begin{itemize}
    \item General LLM Evaluation (Ethical Evaluation)
    \item Text Generation \& Question-Answering
    \item Classification (e.g., Toxicity Detection)
    \item Machine Translation \& Information Retrieval
\end{itemize}

\subsubsection*{Benchmarks}
Major benchmarks used for fairness evaluation include:
\begin{itemize}
    \item \textbf{General}: HELM, Chatbot Arena, HolisticBias.
    \item \textbf{Specific}: BBQ (Bias Benchmark for QA), CrowS-Pairs, StereoSet, BOLD, RealToxicityPrompts, Winogender, WinoBias.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Identifies Harms}: Allows precise definition and measurement of specific social harms.
    \item \textbf{Enables Mitigation}: Provides necessary feedback to develop bias mitigation techniques.
    \item \textbf{Promotes Equity}: Empowers developers to prevent the amplification of historical power asymmetries.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Weak Downstream Correlation}: Embedding and probability metrics often show a weak relationship with actual bias observed in downstream applications.
    \item \textbf{Dataset Validity}: Widely-used datasets (e.g., StereoSet, CrowS-Pairs) suffer from ambiguities, raising questions about whether they accurately measure real-world stereotypes.
    \item \textbf{Biased Classifiers}: Metrics relying on auxiliary classifiers (e.g., for toxicity) can be unreliable if the classifier itself is biased (e.g., against African-American English).
    \item \textbf{Subjectivity}: Fairness is ``highly subjective, value-dependent, and non-static,'' making a universal benchmark difficult to establish.
\end{itemize}


\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Chang2023SurveyLLMs,
Gallegos2024BiasFairness,
blodgett2020language,
parrish2022bbq,
nangia2020crows,
nadeem2021stereoset,
}
\fussy
