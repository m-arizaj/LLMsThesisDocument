\subsection{Honesty}

Honesty is a core principle used in the \textbf{Human Evaluation} of Large Language Models (LLMs). It is part of the ``3H rule'' — \textbf{Helpfulness, Honesty, and Harmlessness} — which serves as a foundational concept for developing detailed human assessment criteria.

Human evaluation itself is defined as a method to assess the quality and accuracy of a model's generated results through human participation. This approach is considered more comprehensive than automated evaluation because it reflects real-world application scenarios.

Honesty is not a quantitative metric with a single mathematical formula. It is a \textbf{qualitative principle} assessed by human evaluators (such as experts, researchers, or ordinary users).

In evaluation surveys, the principle of ``Honesty'' is elaborated into specific, measurable criteria such as \textbf{Accuracy}, which is defined as scrutinizing:

\begin{quote}
``The extent to which the language model produces information that aligns with factual knowledge, avoiding errors and inaccuracies.''
\end{quote}

\subsubsection*{Purpose}
The purpose of evaluating for Honesty is to assess an LLM's adherence to truthfulness and factual correctness. It is a fundamental component of the ``3H rule'' for human alignment and trustworthiness. This principle helps ensure that models are reliable and avoid generating factually inaccurate information or ``hallucinations.''

\subsubsection*{Applications}

In the domain of Software Engineering, the principle of Honesty is applied to ensure the reliability and security of generated code and technical analysis. Unlike open-ended chat, "untruthful" outputs here result in functional errors or security risks.

\begin{itemize}
    \item \textbf{Code Correctness and Verification}: Honesty is evaluated through the model's ability to distinguish between valid and invalid code. This involves binary discrimination tasks (e.g., does the code pass unit tests?) to ensure the model does not "hallucinate" non-existent libraries or syntax \cite{askell2021general}.
    \item \textbf{Vulnerability Detection}: LLMs are applied to tasks such as \textit{code vulnerability detection} and \textit{test prioritization}. Evaluations indicate that while models perform well in generation, they may fail to provide accurate (honest) assessments in these safety-critical tasks, often yielding incorrect answers regarding security flaws \cite{Chang2023SurveyLLMs}.
    \item \textbf{Calibration of Capabilities}: An honest software assistant must be calibrated to its own limitations, refusing to generate code for tasks beyond its capability rather than producing plausible but non-functional solutions. This is critical to avoid "misleading human users" in technical contexts \cite{askell2021general}.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{More Reliable}: Human evaluation based on principles like Honesty is considered ``more reliable'' than automated metrics, especially for open-ended generation tasks.
    \item \textbf{Real-World Scenarios}: This evaluation method is ``closer to the actual application scenario'' and ``can provide more comprehensive and accurate feedback.''
    \item \textbf{Fundamental Assessment}: It assesses a core pillar of model trustworthiness that automated metrics (like F1 or ROUGE) may not capture.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Subjectivity and Variance}: Human evaluation can have ``high variance and instability,'' which may be ``due to cultural and individual differences'' among the human evaluators.
    \item \textbf{Requires Human Labor}: By definition, it is not an automated process and requires inviting human evaluators to assess the model's outputs.
    \item \textbf{Requires Clear Rubrics}: The effectiveness of the evaluation depends heavily on the quality of the ``evaluation criteria'' and the ``evaluator's expertise level.''
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
askell2021general,
Chang2023SurveyLLMs,
}
\fussy
