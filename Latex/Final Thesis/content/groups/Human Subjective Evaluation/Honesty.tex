\subsection{Honesty}

\subsubsection*{Introduction}
Honesty is a core principle used in the \textbf{Human Evaluation} of Large Language Models (LLMs). It is part of the ``3H rule'' — \textbf{Helpfulness, Honesty, and Harmlessness} — which serves as a foundational concept for developing detailed human assessment criteria.

Human evaluation itself is defined as a method to assess the quality and accuracy of a model's generated results through human participation. This approach is considered more comprehensive than automated evaluation because it reflects real-world application scenarios.

\subsubsection*{Definition}
Honesty is not a quantitative metric with a single mathematical formula. It is a \textbf{qualitative principle} assessed by human evaluators (such as experts, researchers, or ordinary users).

In evaluation surveys, the principle of ``Honesty'' is elaborated into specific, measurable criteria such as \textbf{Accuracy}, which is defined as scrutinizing:

\begin{quote}
``The extent to which the language model produces information that aligns with factual knowledge, avoiding errors and inaccuracies.''
\end{quote}

\subsubsection*{Purpose}
The purpose of evaluating for Honesty is to assess an LLM's adherence to truthfulness and factual correctness. It is a fundamental component of the ``3H rule'' for human alignment and trustworthiness. This principle helps ensure that models are reliable and avoid generating factually inaccurate information or ``hallucinations.''

\subsubsection*{Domains}
\begin{itemize}
    \item Human Evaluation
    \item Human-centered Evaluation
    \item LLM Evaluation (Trustworthiness, Ethics)
\end{itemize}

\subsubsection*{Benchmarks}
Honesty, as a human evaluation criterion, is applied within broader evaluation frameworks and benchmarks:
\begin{itemize}
    \item \textbf{HELM} (Holistic Evaluation of Language Models)
    \item \textbf{Chatbot Arena}
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{More Reliable}: Human evaluation based on principles like Honesty is considered ``more reliable'' than automated metrics, especially for open-ended generation tasks.
    \item \textbf{Real-World Scenarios}: This evaluation method is ``closer to the actual application scenario'' and ``can provide more comprehensive and accurate feedback.''
    \item \textbf{Fundamental Assessment}: It assesses a core pillar of model trustworthiness that automated metrics (like F1 or ROUGE) may not capture.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Subjectivity and Variance}: Human evaluation can have ``high variance and instability,'' which may be ``due to cultural and individual differences'' among the human evaluators.
    \item \textbf{Requires Human Labor}: By definition, it is not an automated process and requires inviting human evaluators to assess the model's outputs.
    \item \textbf{Requires Clear Rubrics}: The effectiveness of the evaluation depends heavily on the quality of the ``evaluation criteria'' and the ``evaluator's expertise level.''
\end{itemize}

% añadir esta entrada a tu archivo .bib
% @article{askell2021general,
%   title={A general language assistant as a laboratory for alignment},
%   author={Askell, A. and Bai, Y. and Chen, A. and others},
%   journal={arXiv preprint arXiv:2112.00861},
%   year={2021},
%   doi={10.48550/arXiv.2112.00861}
% }