\subsection{Hit-N}

\textbf{Hit-N} (also referred to as Hit-N (multi-defects)) is an evaluation metric used in fault localization. It is specifically ``designed for multi-defects.''

The metric measures the number of bugs (out of a set of known bugs) for which a fault localization tool's predicted set of suspicious code elements (e.g., the top-ranked statements) successfully ``contains at least N faulty statements.''

The metric is defined qualitatively based on a counting mechanism. It can be expressed as:

\begin{equation}
    \text{Hit-N} = \sum_{b \in B} \mathbb{1}(|S_{\text{predicted}}(b) \cap S_{\text{faulty}}(b)| \geq N)
\end{equation}

\noindent where:
\begin{itemize}
    \item $B$: The set of bugs in the multi-bug benchmark.
    \item $S_{\text{predicted}}(b)$: The set of suspicious statements predicted by the tool for bug $b$.
    \item $S_{\text{faulty}}(b)$: The set of actual faulty statements for bug $b$.
    \item $N$: The threshold number of faulty statements required for a ``hit.''
\end{itemize}

\subsubsection*{Purpose}
The primary purpose of Hit-N is to evaluate the effectiveness of fault localization techniques in scenarios involving multiple defects. Unlike metrics that might only measure the rank of the \textit{first} fault found (like Mean First Rank - MFR), Hit-N assesses a tool's ability to identify faulty statements across multiple, potentially interacting, bugs within the same project.

\subsubsection{Applications}
In the context of software engineering, particularly within fault localization and deep learning-based approaches, the \textbf{Hit-N} metric is applied in the following scenarios:

\begin{itemize}
    \item \textbf{Evaluation of Co-change Fixing Locations}: Hit-N is primarily applied to evaluate the capability of Fault Localization (FL) tools to identify multiple dependent faulty statements that must be fixed simultaneously (referred to as ``co-change fixing locations''). This is crucial for validating tools designed to support Automated Program Repair (APR) in multi-hunk bug scenarios, where identifying a single fault is insufficient \cite{li2022fault}.
    
    \item \textbf{Benchmarking Deep Learning-based FL Tools}: It serves as a rigorous benchmark for comparing the performance of advanced deep learning models (e.g., FIXLOCATOR) against traditional or other learning-based baselines (like DeepFL or CNN-FL). By requiring $N$ successful hits, it distinguishes tools that can capture complex fault patterns from those that only find simple, isolated bugs \cite{li2022fault}.
    
    \item \textbf{Systematic Review of FL Metrics}: In broader surveys of deep learning-based software engineering, Hit-N is categorized as a specialized metric for assessing model performance on multi-defect datasets, highlighting the progression from single-fault assumptions to more realistic multi-fault evaluations in the field \cite{Chen2024DLBasedSE}.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Designed for Multi-Defect Scenarios}: Its main advantage is its specific design for evaluating performance against multiple bugs, which is often a more realistic scenario than the single-bug assumption.
    \item \textbf{Measures Set-Based Success}: It evaluates the \textit{set} of predicted locations rather than just the single highest-ranked location.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Infrequent Use}: Survey literature indicates that this metric is not commonly used (listing only one use case in reviewed literature).
    \item \textbf{Niche Application}: Its utility is limited to multi-defect fault localization; it is not a general-purpose metric. Most existing fault localization research still assumes only one bug is present.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Chen2024DLBasedSE,
li2022fault,
}
\fussy
