\subsection{ELO Score}

The ELO Score (or Elo rating) is a human evaluation metric used to assess the performance of Large Language Models (LLMs), particularly in comparative chatbot benchmarks. This method was adopted by the \textit{Chatbot Arena} platform as a way to ``reduce the cost of human evaluation'' while still capturing human preferences.

Instead of absolute scoring, this method relies on crowdsourced, pairwise comparisons where users vote for the ``better'' response from two anonymous models. The ELO score is then used to rank the models on a leaderboard.

The ELO Score is a method for calculating the relative skill levels of players (in this case, LLMs) in competitor-versus-competitor matchups. In the context of \textit{Chatbot Arena}, users pose questions to two anonymous models simultaneously and vote for the better response.

The ranking process follows these principles:
\begin{itemize}
    \item All models initially start with the same ELO score.
    \item With each ``matchup,'' the ELO score of the favored (winning) LLM increases, while the score of the other model decreases.
    \item Over time, as more comparisons are accumulated, the ``relative abilities of LLMs can be discerned through their respective Elo scores.''
\end{itemize}

\subsubsection*{Purpose}
The primary purpose of the ELO score is to serve as the core metric for \textbf{Human Evaluation} in a crowdsourced, competitive environment. It provides a scalable and reliable way to rank models based on aggregated human preferences \cite{zheng2023judging}.

\subsubsection*{Applications}
\begin{itemize}
    \item It is the main evaluation metric for \textit{Chatbot Arena}, a platform designed to benchmark LLMs through anonymous battles \cite{zheng2023judging}.
    \item It is used to evaluate an LLM's alignment with \textbf{Human Preference}, serving as a proxy for utility in open-ended conversation tasks \cite{zheng2023judging}.
    \item \textbf{Software Engineering (Code Generation)}: The metric is applied to evaluate and rank LLMs specifically on their ``coding'' capabilities. It helps differentiate models based on their ability to handle programming tasks, reasoning, and math, where some models (like Vicuna-13B) perform noticeably worse than others (like GPT-4) \cite{zheng2023judging}.
\end{itemize}


\subsubsection*{Limitations}
Despite its utility, the use of LLM-based evaluation and this scoring method has identified limitations:
\begin{itemize}
    \item \textbf{Position Bias}: Judges (especially LLMs acting as judges) exhibit a propensity to favor certain positions, often favoring the first answer presented \cite{zheng2023judging}.
    \item \textbf{Verbosity Bias}: There is a tendency to favor longer responses, even if they are not as clear, high-quality, or accurate as smaller alternatives \cite{zheng2023judging}.
    \item \textbf{Self-Enhancement Bias}: Some LLM judges may favor answers generated by themselves (e.g., GPT-4 favoring GPT-4 outputs), although studies on this are limited \cite{zheng2023judging}.
    \item \textbf{Limited Reasoning Capability}: LLM judges have limited capability in grading math and reasoning questions; they can be misled by incorrect answers or fail to identify arithmetic mistakes \cite{zheng2023judging}.
    \item \textbf{Neglect of Safety}: The current evaluation largely emphasizes helpfulness but ``largely neglects safety,'' such as honesty and harmlessness, combining multiple dimensions (accuracy, creativity) into a single metric \cite{zheng2023judging}.
\end{itemize}


\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Lin2024SWC,
Guo2023EvalLLMs,
zheng2023judging,
}
\fussy