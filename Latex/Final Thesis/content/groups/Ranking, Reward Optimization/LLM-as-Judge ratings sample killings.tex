\subsection{LLM-as-Judge and Sample Killings}

\subsubsection*{Introduction}
This document covers two distinct but related evaluation techniques that use Large Language Models (LLMs) as part of the evaluation process itself.

\begin{itemize}
    \item \textbf{LLM Sample Killings}: A strategy to measure and improve \textit{test effectiveness}. It identifies which test cases are most valuable by measuring their ability to ``kill'' (i.e., detect and falsify) incorrect code samples generated by a set of LLMs.
    \item \textbf{LLM-as-Judge Ratings}: A \textit{human-aligned evaluation} method. It uses a powerful LLM to act as a ``judge'' and assign a qualitative score to another model's output, assessing its quality against human-defined criteria.
\end{itemize}

These methods represent a shift from traditional evaluation, leveraging LLMs' own outputs or reasoning capabilities to create more robust benchmarks and assessments.

\subsubsection*{1. LLM Sample Killings}

\textbf{Definition} \\
LLM Sample Killings is a testing requirement used for test-suite reduction, particularly in the \textbf{HumanEval+} benchmark. It is defined as the set of incorrect LLM-generated code samples that a specific test case can successfully detect and falsify.

This empirical metric is used to find the most effective test cases: a test is considered valuable if it ``kills'' many incorrect LLM solutions that other, simpler tests might miss.

\textbf{Purpose} \\
The primary purpose is \textbf{test-suite reduction} and ensuring \textbf{test effectiveness}. By identifying the tests that ``kill'' the most diverse set of empirically wrong LLM samples, a large test suite can be ``distilled'' into a much smaller one (e.g., HumanEval+-MINI) while preserving its rigorous evaluation capability.

\textbf{Applications} \\
\begin{itemize}
    \item Software Engineering / Code Generation
    \item Test Effectiveness / Model Robustness
    \item Benchmark creation and test-suite minimization
\end{itemize}

\textbf{Advantages}
\begin{itemize}
    \item \textbf{Highly Effective}: Found to be the ``most effective'' strategy for test-suite reduction, preserving the benchmark's ability to find errors.
    \item \textbf{Practical}: It is an empirical measure of a test case's value, based on real-world LLM failures.
\end{itemize}

\textbf{Limitations}
\begin{itemize}
    \item \textbf{Requires Cross-Validation}: To evaluate a new LLM, one must use the ``sample kills'' data gathered from \textit{other} LLMs (a leave-one-out cross-validation approach).
\end{itemize}

\subsubsection*{2. LLM-as-Judge Ratings (1-3 Scale)}

\textbf{Definition} \\
LLM-as-Judge Ratings is an evaluation method used in the \textbf{HumanEval-V} benchmark where a capable LLM (e.g., GPT-4o) acts as a ``judge'' to rate the quality of problem specifications (PS) generated by other LMMs.

The judge rates the generated text on a \textbf{1-3 scale} (1 = severe error, 3 = near perfect) across three distinct dimensions:
\begin{itemize}
    \item \textbf{Basic-Level Perception}: Identifying basic visual elements (shapes, text, colors).
    \item \textbf{High-Level Comprehension}: Understanding relationships, patterns, constraints, and operations.
    \item \textbf{Contextual Interpretation}: The clarity of the description, ensuring it is free of vagueness or hallucinations.
\end{itemize}

\textbf{Purpose} \\
The goal is to provide a scalable, \textbf{human-aligned evaluation} of an LMM's \textbf{visual understanding} capabilities, separate from its coding proficiency. It assesses the quality of the intermediate textual description generated from a visual diagram.

\textbf{Applications} \\
\begin{itemize}
    \item Multimodal Code Generation
    \item Visual Reasoning
    \item Evaluating intermediate textual representations.
\end{itemize}

\textbf{Advantages}
\begin{itemize}
    \item \textbf{Decouples Skills}: Allows for the isolated evaluation of visual comprehension without penalizing a model for poor coding ability.
    \item \textbf{Scalable \& Qualitative}: Offers nuanced, human-like qualitative feedback at scale without expensive human annotation.
\end{itemize}

\textbf{Limitations}
\begin{itemize}
    \item \textbf{Limited Robustness}: Studies found ``limitations of using LLM-as-judge as an evaluation tool.''
    \item \textbf{Poor Correlation}: Ratings did not strongly correlate with downstream functional correctness (minimal difference between passed and failed tasks).
    \item \textbf{Rigid Comparisons}: The method may suffer from rigid comparisons to human-annotated ground truth.
\end{itemize}

\subsubsection*{Comparative Summary}
Below is a comparison of these two LLM-based evaluation strategies:

\begin{center}
\begin{tabular}{|p{3cm}|p{3cm}|p{3.5cm}|p{3cm}|}
\hline
\textbf{Metric} & \textbf{Core Concept} & \textbf{Evaluation Target} & \textbf{Primary Goal} \\
\hline
LLM Sample Killings & Using failed LLM outputs & Test cases & To build an efficient, robust test suite \\
\hline
LLM-as-Judge Ratings & Using an LLM's evaluative ability & LMM-generated text & To provide qualitative, human-aligned ratings \\
\hline
\end{tabular}
\end{center}

% a√±adir estas entradas al archivo .bib

% @article{liu2023code,
%   title={Is your code generated by ChatGPT really correct? Rigorous evaluation of large language models for code generation},
%   author={Liu, J. and Xia, C. S. and Wang, Y. and Zhang, L.},
%   journal={arXiv preprint arXiv:2305.01210},
%   year={2023},
%   doi={10.48550/arXiv.2305.01210}
% }

% @article{zhang2025humaneval,
%   title={HumanEval-V: Benchmarking high-level visual reasoning with complex diagrams in coding tasks},
%   author={Zhang, F. and Wu, L. and Bai, H. and Lin, G. and Li, X. and Yu, X. and Wang, Y. and Chen, B. and Keung, J.},
%   journal={arXiv preprint arXiv:2410.12381},
%   year={2025},
%   doi={10.48550/arXiv.2410.12381}
% }