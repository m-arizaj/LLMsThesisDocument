\subsection{Reward Score}

\subsubsection*{Definition}
The \textbf{Reward Score} (also referred to as reward/score) is an objective, quantitative metric used in the evaluation of LLM-based autonomous agents. It is a primary metric in the category of \textbf{Task Success}, which measures how well an agent can successfully complete its goals.

This metric is typically defined by the environment or benchmark itself, where the agent receives a score based on its performance during or after the interaction.

\subsubsection*{Formula (General Idea)}
The specific formula for the Reward Score is \textbf{task-dependent} and defined by the benchmark being used. It is a quantitative value where higher values indicate greater task completion ability.

For example, in a benchmark like WebShop, the reward score ($R$) could be a function of the attributes matched in a purchase task:

\begin{equation}
    R = f(\text{Task Completion}, \text{Constraints Met}, \text{Efficiency})
\end{equation}

\subsubsection*{Purpose}
The purpose of the Reward Score is to provide a ``concrete, measurable insight'' into an agent's performance and its ability to achieve its defined objectives. It is the standard method for quantifying an agent's task completion ability in reinforcement learning and agentic workflows.

\subsubsection*{Domains}
\begin{itemize}
    \item LLM Agents / Autonomous Agents
    \item Objective Evaluation of Agents
    \item Reinforcement Learning from Human Feedback (RLHF)
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Objective}: Provides concrete, measurable insights into performance, minimizing subjective interpretation.
    \item \textbf{Direct Measurement}: Directly assesses the agent's success in completing the assigned task.
    \item \textbf{Comparable}: As a quantitative score, it allows for direct comparison and tracking of performance over time between different agent architectures.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Scope}: While objective, it may not capture qualitative aspects of performance such as ``intelligence'' or ``user-friendliness,'' which are often better assessed via subjective evaluation.
    \item \textbf{Reward Hacking}: Agents may optimize for the reward metric without actually learning the intended behavior (Goodhart's Law).
\end{itemize}

% a√±adir estas entradas al archivo .bib

% @article{wang2024survey,
%   title={A survey on large language model based autonomous agents},
%   author={Wang, L. and Ma, C. and Feng, X. and others},
%   journal={Frontiers of Computer Science},
%   volume={18},
%   number={6},
%   pages={186345},
%   year={2024},
%   doi={10.1007/s11704-024-40231-1}
% }

% @article{zhang2024building,
%   title={Building cooperative embodied agents modularly with large language models},
%   author={Zhang, H. and Du, W. and Shan, J. and others},
%   journal={arXiv preprint arXiv:2307.02485},
%   year={2024},
%   doi={10.48550/arXiv.2307.02485}
% }

% @article{yao2023react,
%   title={ReAct: Synergizing reasoning and acting in language models},
%   author={Yao, S. and Zhao, J. and Yu, D. and others},
%   journal={arXiv preprint arXiv:2210.03629},
%   year={2023},
%   doi={10.48550/arXiv.2210.03629}
% }

% @article{mehta2024improving,
%   title={Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback},
%   author={Mehta, N. and Teruel, M. and Sanz, P. F. and others},
%   journal={arXiv preprint arXiv:2304.10750},
%   year={2024},
%   doi={10.48550/arXiv.2304.10750}
% }