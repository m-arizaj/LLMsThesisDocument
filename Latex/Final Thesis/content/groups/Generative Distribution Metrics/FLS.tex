\subsection{FLS (Feature Likelihood Score)}

\subsubsection*{Introduction}
FLS (Feature Likelihood Score) is a density estimation metric designed to evaluate the generalization performance of generative models. Introduced by Jiralerspong et al. (2023), its primary goal is to approximate a likelihood evaluation, even for models where a likelihood value is not readily available.

The method works by fitting a Kernel Density Estimate (KDE) to the feature representations (e.g., from an Inception-V3 encoder) of the \textit{generated} samples. The bandwidth for this KDE is optimized to maximize the log-likelihood of the \textit{training} data. The final FLS score is then derived from the log-likelihood of the \textit{test} data under this fitted KDE.

FLS is used as a metric for ranking the overall quality of models, but it also produces a diagnostic sub-metric, \textbf{FLS-POG}, specifically designed to measure memorization.

\subsubsection*{1. FLS (Feature Likelihood Score)}

\textbf{Definition} \\
FLS is a density estimation method that requires access to training, test, and generated samples. It fits a Kernel Density Estimate (KDE) to the encoded features of the generated samples. The KDE's bandwidths are chosen by maximizing the log-likelihood of the training data. The final FLS score is an affine transformation of the log-likelihood of the test data under this KDE.

\textbf{Purpose} \\
To provide a ``Ranking / Overall Quality'' score for generative models by approximating the likelihood of real test data under the model's generated distribution.

\textbf{Benchmarks} \\
Commonly used on: CIFAR10, ImageNet1k, FFHQ, LSUN-Bedroom.

\textbf{Limitations}
\begin{itemize}
    \item FLS is not strongly correlated with human evaluations of image realism, particularly on complex datasets.
    \item The metric can be sensitive to the choice of encoder used for feature extraction.
\end{itemize}

\subsubsection*{2. FLS-POG (Percentage of Overfit Gaussians)}

\textbf{Definition} \\
FLS-POG is a metric derived from the FLS calculation, designed specifically to measure \textbf{memorization}. It is defined as the ``percentage of generated samples that had a higher log-likelihood under the training set than the test set.'' A high FLS-POG score suggests that the generated samples are more ``typical'' of the training set than the test set, indicating overfitting.

\textbf{Purpose} \\
To automatically detect memorization in generative models, claiming to isolate it from other phenomena like mode collapse.

\textbf{Limitations}
\begin{itemize}
    \item FLS-POG has been shown to be an \textbf{unreliable metric} for memorization.
    \item It correlates poorly with the actual, measured percentage of memorized samples.
    \item In controlled experiments, it was found to be the \textit{weakest detector of memorization} compared to other metrics like AuthPct and CT.
    \item It also fails to flag mode shrinkage correctly.
\end{itemize}

\subsubsection*{Comparative Summary}
Below is a comparison of the main score and its diagnostic variant:

\begin{table}[H]
    \centering
    \caption{Comparison of FLS and FLS-POG Metrics}
    \label{tab:fls_metrics}
    \begin{tabular}{|p{2cm}|p{1.5cm}|p{3.5cm}|p{5cm}|}
    \hline
    \textbf{Metric} & \textbf{Based on} & \textbf{Goal} & \textbf{Key Feature(s)} \\
    \hline
    FLS & KDE & Rank models based on density estimation. & Fits a KDE to generated data; evaluates log-likelihood on test data. \\
    \hline
    FLS-POG & FLS & Detect memorization / overfitting. & Compares train vs. test log-likelihood for each generated sample. \\
    \hline
    \end{tabular}
\end{table}

\subsubsection{Applications}

FLS was introduced to fill a critical gap in the engineering of generative systems: the inability of standard perceptual metrics (like FID) to distinguish between a model that creates novel data and one that simply memorizes its training set.

\begin{itemize}
    \item \textbf{Generalization vs. Overfitting Diagnosis}: In the development of generative models, ensuring that the system synthesizes \textit{novel} samples rather than retrieving training data is a key engineering requirement (particularly for privacy and copyright compliance). Jiralerspong et al. (2023) apply FLS as a proxy for the "generalization gap" found in discriminative models. By comparing the likelihood of the training set versus a held-out test set under the generator's distribution, engineers can mathematically identify overfitting checkpoints that would otherwise achieve high fidelity scores on perceptual metrics \cite{jiralerspong2023feature}.

    \item \textbf{Trichotomic Model Tuning}: Traditional evaluation pipelines often require separate metrics for fidelity (Precision), diversity (Recall), and novelty. Jiralerspong et al. (2023) propose FLS as a unified objective for hyperparameter tuning. It serves as a single summary statistic that penalizes models for failing in any of the three pillars: generating unrealistic samples (low fidelity), dropping modes (low diversity), or copying training data (lack of novelty) \cite{jiralerspong2023feature}.

    \item \textbf{Benchmarking Reliability Studies}: In the broader context of metric validation, Stein et al. (2023) apply FLS (specifically the FLS-POG variant) to audit the reliability of memorization detectors. Their application reveals a critical failure mode in automated evaluation: they demonstrate that FLS-POG tends to conflate "mode shrinkage" (generating a limited variety of valid samples) with true memorization. This application serves as a negative control in engineering, warning developers that density-based memorization checks may trigger false positives when the model distribution simply collapses to a subset of the data \cite{Stein2023MetricFlaws}.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):

\sloppy
\cite{
Stein2023MetricFlaws,
jiralerspong2023feature,
}
\fussy
