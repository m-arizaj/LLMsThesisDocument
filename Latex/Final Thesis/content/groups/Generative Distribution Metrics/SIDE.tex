\subsection{SIDE}

\textbf{SIDE} is an \textbf{embedding-based metric} specifically designed to evaluate the quality of code summaries. It differentiates itself from other embedding-based metrics like BERTScore by applying \textbf{contrastive learning}.

Instead of just measuring direct similarity, SIDE enhances the evaluation by learning to pull similar (positive) code-summary pairs closer together in the embedding space while pushing dissimilar (negative) pairs apart. This approach allows it to better capture the semantic adequacy of a generated summary.

SIDE works by encoding both the generated artifact (the code summary) and a reference summary into high-dimensional vectors (embeddings) and then measuring the similarity between them.

While the core calculation relies on similarity, the embeddings $\mathbf{e}$ are optimized via contrastive learning. The metric can be conceptually represented as:

\begin{equation}
    \text{SIDE}(S_{gen}, S_{ref}) \sim \cos(\mathbf{e}_{gen}, \mathbf{e}_{ref})
\end{equation}

\noindent where:
\begin{itemize}
    \item $\mathbf{e}_{gen}$ and $\mathbf{e}_{ref}$ are the vector representations of the generated and reference summaries, respectively.
    \item The embedding space is trained to minimize the distance between semantic equivalents and maximize the distance between dissimilar pairs.
\end{itemize}

\subsubsection*{Purpose}
The primary purpose of SIDE is to assess the quality of machine-generated code summaries. It is used as an automatic metric to compare a candidate summary against a human-written reference summary, aiming for a closer correlation with human judgment than traditional n-gram metrics.


\subsubsection*{Applications}
SIDE is explicitly designed to address challenges in the software development lifecycle, particularly in documentation and program comprehension:

\begin{itemize}
    \item \textbf{Automated Documentation Evaluation}: SIDE is applied to evaluate the quality of AI-generated code summaries (e.g., from tools like Copilot or CodeT5). It correlates significantly better with human judgment than traditional NLP metrics because it detects if the summary semantically matches the code, even if the wording differs entirely from the reference.

    \item \textbf{Documentation Consistency Checking}: A key application of SIDE is the automated identification of \textit{inconsistent comments} in software repositories. By computing the semantic alignment score between existing methods and their Javadoc/docstrings, SIDE can flag outdated or misleading comments that no longer reflect the code's behavior, facilitating code maintenance.

    \item \textbf{Filtering Training Data}: In the development of Large Language Models for code (Code LLMs), SIDE can be used to curate training datasets by filtering out low-quality code-comment pairs, ensuring that models are trained only on semantically aligned documentation.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{State-of-the-Art}: It is considered a state-of-the-art metric specifically designed for the code summarization task.
    \item \textbf{Task-Specific}: Unlike general text-based metrics (like BLEU or ROUGE), SIDE's use of contrastive learning is tailored specifically for the nuances of code documentation.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Similarity vs. Correctness}: Like other embedding-based metrics, SIDE ``approximates correctness through similarity.'' This is a limitation because ``similarity does not always align with correctness.'' A summary could be syntactically different but semantically correct, or vice versa, leading to potential scoring discrepancies.
\end{itemize}



\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
mastropaolo2024evaluating,
Zhou2025LLMAsJudge,
}
\fussy
