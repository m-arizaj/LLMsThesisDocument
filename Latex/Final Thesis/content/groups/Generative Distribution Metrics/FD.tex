\subsection{Fréchet Distance (FD)}

The \textbf{Fréchet Distance (FD)} is one of the most widely used metrics for evaluating the quality of generative models, particularly in the generation of images. It is designed to measure the similarity between the distribution of generated samples and the distribution of real samples.

The metric follows a two-step process:
\begin{enumerate}
    \item \textbf{Feature Extraction}: Samples from both the real and generated datasets are passed through a feature extractor (an ``encoder'') to get lower-dimensional representations.
    \item \textbf{Distance Calculation}: The Fréchet distance (also known as the Wasserstein-2 distance) is calculated between two multivariate Gaussian distributions that are fitted to the mean and covariance of these extracted features.
\end{enumerate}

This metric is most commonly known as \textbf{FID (Fréchet Inception Distance)}, which specifically refers to using the Inception-V3 network as the feature extractor. While FID is a standard, distinct variants like \textbf{FD$\infty$} and \textbf{sFID} have been developed to address specific limitations.

\subsubsection*{1. Standard Fréchet Distance (FD / FID)}

\textbf{Definition} \\
The standard Fréchet Distance computes the Wasserstein-2 distance between two Gaussians fitted to the sample mean ($\mu$) and covariance ($\Sigma$) of the real ($r$) and generated ($g$) feature representations. When the Inception-V3 encoder is used, this is referred to as \textbf{FID}.

\begin{equation}
    \text{FD} = ||\mu_r - \mu_g||_2^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)
\end{equation}

\textbf{Purpose} \\
FD (and FID) is used as a primary metric for ranking the overall quality of generative models. It groups both the \textbf{fidelity} (realism) and \textbf{diversity} of the generated samples into a single score. A lower score indicates better performance.

\textbf{Limitations}
\begin{itemize}
    \item \textbf{Encoder Bias (FID)}: The standard Inception-V3 encoder is biased towards textures over shapes and features relevant to its 1k ImageNet classes.
    \item \textbf{Poor Human Correlation}: FID scores often do not correlate strongly with human evaluations of perceptual realism (especially for diffusion models).
    \item \textbf{Sample Bias}: The calculation is biased by the finite number of samples (e.g., 50k); the true value would be lower with infinite samples.
    \item \textbf{Conflates Metrics}: Combining fidelity and diversity into one score makes it difficult to diagnose why a model is failing.
\end{itemize}

\subsubsection*{2. FD$\infty$ (Fréchet Distance Infinite)}

\textbf{Definition} \\
A variant of FD designed to correct for the inherent bias introduced by using a finite number of samples. FD$\infty$ is calculated by evaluating the standard FD at multiple sample sizes (e.g., 15 intervals from 5k to 50k), fitting a linear trend, and extrapolating the value to an infinite number of samples ($N=\infty$).

\textbf{Advantages}
\begin{itemize}
    \item Provides a less biased estimate of the Fréchet Distance compared to the standard calculation.
    \item More stable with respect to the number of samples used.
\end{itemize}

\textbf{Limitations}
\begin{itemize}
    \item While it reduces bias, it does not completely solve the problem, and values still tend to decrease as more samples are used.
    \item It relies on the same (potentially flawed) encoder as the underlying FD metric.
\end{itemize}

\subsubsection*{3. sFID (Spatial FID)}

\textbf{Definition} \\
A variant of FID that computes the Fréchet Distance using representations from an \textit{intermediate layer} of the Inception-V3 network (specifically, the \texttt{mixed\_6/conv} layer) instead of the final \texttt{pool3} layer.

\textbf{Purpose} \\
Developed to capture more spatial or textural features rather than the high-level semantic features from the final layer.

\textbf{Limitations}
\begin{itemize}
    \item It still relies on the Inception-V3 network and its associated biases.
    \item It is not widely reported in modern papers compared to the standard FID.
\end{itemize}

\subsubsection*{Comparative Summary}
Below is a comparison of the key FD variants:

\begin{table}[H]
    \centering
    \caption{Comparison of Fréchet Distance (FD) Variants}
    \label{tab:fid_variants}
    \begin{tabular}{|p{2cm}|p{2cm}|p{4cm}|p{4cm}|}
    \hline
    \textbf{Metric} & \textbf{Based on} & \textbf{Extension Goal} & \textbf{Key Feature(s)} \\
    \hline
    FD / FID & FD & Baseline metric for overall quality (fidelity + diversity). & $W_2$ distance between Gaussian moments. FID uses Inception-V3. \\
    \hline
    FD$\infty$ & FD & Remove finite-sample bias from the FD calculation. & Extrapolates FD value to $N=\infty$ using a linear fit. \\
    \hline
    sFID & FD / FID & Evaluate on spatial features, not just semantic features. & Uses an intermediate layer (\texttt{mixed\_6/conv}) of Inception-V3. \\
    \hline
    \end{tabular}
\end{table}

\subsubsection{Applications}

The Fréchet Distance and its variants constitute the standard toolkit for automated model validation. Each variant is applied to address specific engineering challenges in the training and selection of generative models.

\begin{itemize}
    \item \textbf{Convergence Monitoring and Mode Collapse Detection}: Heusel et al. (2017) introduced the standard FID specifically to overcome the limitations of the Inception Score in detecting "intra-class mode dropping" (a form of mode collapse). In modern training pipelines, FID is applied as the primary feedback signal to monitor the convergence of the generator distribution towards the real data distribution, particularly when tuning complex update rules like TTUR (Two Time-Scale Update Rule) \cite{heusel2017gans}.

    \item \textbf{Unbiased Comparative Benchmarking}: Standard FID allows models to "game" the metric simply by generating more samples (since bias decreases with $N$). Chong and Forsyth (2020) apply the $\text{FD}_\infty$ variant to remove this finite-sample bias. This allows engineers to fairly rank models that were evaluated with different batch sizes or computational constraints, ensuring that improvements in the score reflect true gains in model quality rather than statistical artifacts \cite{chong2020effectively}.

    \item \textbf{Diagnostic of Spatial Structure}: Standard FID can sometimes be "fooled" by models that generate realistic textures but incoherent shapes. Nash et al. (2021) utilize sFID (Spatial FID) as a secondary diagnostic metric. By inspecting the intermediate layers of the network, this application verifies that the generative model preserves the global spatial layout and structure of the target domain, not just its low-level texture statistics \cite{nash2021generating}.

    \item \textbf{Cross-Architecture Evaluation (Diffusion vs. GANs)}: As generative architectures evolve, the standard Inception-based FID has shown misalignment with human perception. Stein et al. (2023) apply the generalized Fréchet Distance (using modern encoders like DINOv2 instead of Inception) to benchmark Diffusion models against GANs. This application is critical for modern model selection, as it prevents the unfair penalization of Diffusion models, which often have higher perceptual quality but poorer standard FID scores compared to GANs \cite{Stein2023MetricFlaws}.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Stein2023MetricFlaws,
heusel2017gans,
chong2020effectively,
nash2021generating,
}
\fussy
