\subsection{MAUVE}

\subsubsection*{Definition}
\textbf{MAUVE} (MAUVE Scores for Generative Models) is an automatic evaluation metric used to assess the quality of text generated by large language models.

It is categorized as an evaluation method based on large language models, specifically a \textbf{BERT-based} metric. MAUVE relies on reference texts to compare the generated output against a human-written baseline, focusing on the distribution of semantic features rather than exact string matching.

\subsubsection*{Formula (General Idea)}
While the provided context describes it broadly as a ``BERT-based'' metric, the mathematical core of MAUVE involves comparing the distribution of the generated text ($P$) with the distribution of the reference text ($Q$) in a continuous embedding space (e.g., using BERT embeddings).

Conceptually, MAUVE measures the gap between these distributions, often visualizing it as an Information Divergence Frontier. The score is typically defined as the area under this divergence curve:

\begin{equation}
    \text{MAUVE} = \text{Area under the Curve } (C(P, Q))
\end{equation}

\noindent where $C(P, Q)$ represents the trade-off curve between Type I (precision) and Type II (recall) errors in the embedding space.

\subsubsection*{Purpose}
The general purpose of LLM-based metrics like MAUVE is to evaluate the performance of large language models in natural language generation tasks. They aim to ``better capture the nuances'' between different generative tasks and provide guidance on model performance that aligns closer to human judgment than n-gram metrics.

\subsubsection*{Applications in Software Engineering}
Although MAUVE is primarily designed for open-ended natural language generation, its methodology is applicable to software engineering domains where Large Language Models (LLMs) are utilized:

\begin{itemize}
    \item \textbf{Evaluation of Code Generation}: MAUVE is applicable to the evaluation of generative models tasked with producing "computer programs" and code, assessing how closely the distribution of generated code matches human-written baselines \cite{pillutla2023mauve}.
    \item \textbf{Hyperparameter Tuning for Coding Models}: The metric is used by developers to tune hyperparameters (such as the $p$ value in nucleus sampling) and to evaluate "decoding algorithms," which is crucial for optimizing models to generate syntactically correct and diverse code \cite{pillutla2023mauve}.
    \item \textbf{Assessment of Architectural Innovations}: It serves as a benchmark to measure the performance impact of "architectural innovations" in LLMs developed for various tasks, including those used in software engineering contexts \cite{pillutla2023mauve}.
    \item \textbf{Comprehensive LLM Testing}: In the broader context of software quality engineering, MAUVE is part of the suite of reference-based metrics used to conduct "comprehensive testing" on LLMs that serve as tools for natural language processing and coding tasks \cite{Lin2024SWC}.
\end{itemize}
\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Deep Semantic Understanding}: As an LLM-based metric, it exhibits ``greater semantic understanding ability,'' capturing meaning rather than just surface form.
    \item \textbf{Captures Nuance}: Designed to distinguish subtle quality differences between generative models.
    \item \textbf{Human-like Evaluation}: Aims to provide assessments with ``intelligence similar to that of humans.''
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Reference Dependency}: MAUVE relies on the availability of high-quality reference texts, which may not always be easy to procure for open-ended generation tasks.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Lin2024SWC,
pillutla2023mauve,
}
\fussy
