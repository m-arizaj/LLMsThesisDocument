\subsection{MAUVE}

\subsubsection*{Definition}
\textbf{MAUVE} (MAUVE Scores for Generative Models) is an automatic evaluation metric used to assess the quality of text generated by large language models.

It is categorized as an evaluation method based on large language models, specifically a \textbf{BERT-based} metric. MAUVE relies on reference texts to compare the generated output against a human-written baseline, focusing on the distribution of semantic features rather than exact string matching.

\subsubsection*{Formula (General Idea)}
While the provided context describes it broadly as a ``BERT-based'' metric, the mathematical core of MAUVE involves comparing the distribution of the generated text ($P$) with the distribution of the reference text ($Q$) in a continuous embedding space (e.g., using BERT embeddings).

Conceptually, MAUVE measures the gap between these distributions, often visualizing it as an Information Divergence Frontier. The score is typically defined as the area under this divergence curve:

\begin{equation}
    \text{MAUVE} = \text{Area under the Curve } (C(P, Q))
\end{equation}

\noindent where $C(P, Q)$ represents the trade-off curve between Type I (precision) and Type II (recall) errors in the embedding space.

\subsubsection*{Purpose}
The general purpose of LLM-based metrics like MAUVE is to evaluate the performance of large language models in natural language generation tasks. They aim to ``better capture the nuances'' between different generative tasks and provide guidance on model performance that aligns closer to human judgment than n-gram metrics.

\subsubsection*{Domains}
\begin{itemize}
    \item LLM Evaluation / Natural Language Processing (NLP)
    \item Natural Language Generation (NLG)
    \item Evaluation of Generative Models
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Deep Semantic Understanding}: As an LLM-based metric, it exhibits ``greater semantic understanding ability,'' capturing meaning rather than just surface form.
    \item \textbf{Captures Nuance}: Designed to distinguish subtle quality differences between generative models.
    \item \textbf{Human-like Evaluation}: Aims to provide assessments with ``intelligence similar to that of humans.''
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Reference Dependency}: MAUVE relies on the availability of high-quality reference texts, which may not always be easy to procure for open-ended generation tasks.
\end{itemize}

% a√±adir estas entradas al archivo .bib

% @article{pillutla2023mauve,
%   title={MAUVE scores for generative models: Theory and practice},
%   author={Pillutla, K. and Liu, L. and Thickstun, J. and Welleck, S. and Swayamdipta, S. and Zellers, R. and Oh, S. and Choi, Y. and Harchaoui, Z.},
%   journal={arXiv preprint arXiv:2212.14578},
%   year={2023},
%   doi={10.48550/arXiv.2212.14578}
% }

% @inproceedings{lin2024overview,
%   title={Overview of the comprehensive evaluation of Large Language Models},
%   author={Lin, L. and Zhu, D. and Shang, J.},
%   booktitle={2024 IEEE Smart World Congress (SWC)},
%   pages={1--8},
%   year={2024},
%   publisher={IEEE},
%   doi={10.1109/SWC62898.2024.00231}
% }