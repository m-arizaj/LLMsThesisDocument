\subsection{Inception Score (IS)}

\subsubsection*{Definition}
The \textbf{Inception Score (IS)} is a well-known metric used to evaluate the quality of images generated by generative models, particularly Generative Adversarial Networks (GANs). Introduced by Salimans et al. (2016), the metric is designed to measure two desirable properties simultaneously:

\begin{itemize}
    \item \textbf{Fidelity (Image Quality)}: Images should be sharp, clear, and look like a specific object. The IS measures this by checking if the conditional label distribution $p(y|x)$ (the probability of labels given a generated image $x$) has \textbf{low entropy}. A low entropy score means the Inception-V3 model is confident the image belongs to one specific class.
    \item \textbf{Diversity}: The model should generate a wide variety of images covering all classes. The IS measures this by checking if the marginal distribution $p(y)$ (the average distribution of labels over all generated images) has \textbf{high entropy}.
\end{itemize}

The score is maximized when both conditions are met: generated images are individually distinct (low entropy) but collectively cover many classes (high entropy).

\subsubsection*{Formula}
The Inception Score is calculated as the exponential of the Kullback-Leibler (KL) divergence between the conditional label distribution $p(y|x_i^g)$ and the marginal label distribution $p(y)$ for a set of $n$ generated images $\{x_i^g\}$.

\begin{equation}
    \text{IS}(\{x_i^g\}_{i=1}^n) = \exp\left(\frac{1}{n} \sum_{i=1}^n D_{KL}(p(y|x_i^g) \parallel p(y))\right)
\end{equation}

\noindent where:
\begin{itemize}
    \item $p(y|x_i^g)$: The label probability distribution for a single generated image $x_i^g$, as predicted by the Inception-V3 network.
    \item $p(y)$: The marginal distribution, calculated by averaging the label probabilities over all generated samples.
    \item $D_{KL}$: The KL divergence, which measures the difference between the two distributions.
\end{itemize}

\subsubsection*{Purpose}
The IS is a ranking metric used to assess the ``overall quality'' of generative models. It attempts to group both image fidelity and sample diversity into a single numerical score, which was historically used to rank models against each other.

\subsubsection*{Domains \& Benchmarks}
\begin{itemize}
    \item \textbf{Domains}: Generative Models, Image Generation.
    \item \textbf{Benchmarks}: Intrinsically tied to the \textbf{Inception-V3} classification model. It is used to evaluate models trained on datasets like CIFAR-10 and ImageNet.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Combined Metric}: It was one of the first widely adopted metrics to attempt to quantify both fidelity and diversity in a single score.
    \item \textbf{Automated}: It provides an automated, quantitative measure of performance without requiring human evaluation.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Inception-V3 Dependency}: The metric is exclusively dependent on the Inception-V3 model. This introduces bias towards the 1,000 ImageNet classes and tends to prioritize texture over shape.
    \item \textbf{Poor Human Correlation}: IS does not correlate well with human evaluations of image realism, particularly on complex, high-resolution datasets.
    \item \textbf{Sensitivity to Gaming}: The score can be artificially inflated by models that learn to generate one high-quality image per class, even if they lack true diversity (mode collapse).
\end{itemize}

% a√±adir esta entrada a tu archivo .bib
% @inproceedings{salimans2016improved,
%   title={Improved techniques for training GANs},
%   author={Salimans, T. and Goodfellow, I. and Zaremba, W. and Cheung, V. and Radford, A. and Chen, X.},
%   booktitle={Advances in Neural Information Processing Systems},
%   volume={29},
%   year={2016},
%   doi={10.48550/arXiv.1606.03498}
% }