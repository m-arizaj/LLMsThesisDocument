\subsection{AMBER Score}

\subsubsection{Introduction}

The AMBER Score is a composite metric designed to evaluate hallucination behavior in Multi-modal Large Language Models (MLLMs).
It was introduced by Wang et al. (2023) in the benchmark \textit{AMBER: An LLM-Free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation}, providing a unified and low-cost method for assessing hallucinations in both generative and discriminative multimodal tasks \cite{Wang2023AMBER}. Unlike previous evaluation methods that rely on large models or human raters, AMBER provides an LLM-free, automatic, and multidimensional approach.

It measures hallucinations across three key dimensions:
\begin{enumerate}
    \item \textbf{Existence:} objects falsely introduced or omitted in model output.
    \item \textbf{Attribute:} hallucinations related to color, number, or action.
    \item \textbf{Relation:} false relational claims between objects.
\end{enumerate}

AMBER integrates generative hallucination detection metrics (e.g., CHAIR) and classification metrics (Precision, Recall, F1) into a single interpretable score that reflects the overall reliability of a multimodal model \cite{Wang2023AMBER}.

\subsubsection{Formula and Structure}

The AMBER Score combines generative and discriminative evaluations into a single metric:

\[
\text{AMBER Score} = \frac{1}{2} \times (1 - \text{CHAIR} + \text{F1})
\]

where:
\begin{itemize}
    \item $\text{CHAIR}$ = frequency of hallucinated objects in generative outputs (lower is better).
    \item $\text{F1}$ = harmonic mean of Precision and Recall in discriminative hallucination detection (higher is better).
\end{itemize}

This formulation ensures that models are rewarded for both minimizing hallucinations and maintaining discriminative reliability.
The final score ranges from 0 to 1, where higher values indicate fewer hallucinations and stronger consistency across both evaluation dimensions.

\subsubsection{Supporting Metrics in AMBER}

The benchmark defines additional metrics to decompose hallucination performance before aggregation into the AMBER Score \cite{Wang2023AMBER}:

\textbf{1. CHAIR (Object Hallucination Rate)}
\[
\text{CHAIR}(R) = 1 - \frac{\text{len}(R'_{\text{obj}} \cap A_{\text{obj}})}{\text{len}(R'_{\text{obj}})}
\]
Measures the proportion of objects in a model’s description that do not exist in the reference annotation.

\textbf{2. Cover (Object Coverage)}
\[
\text{Cover}(R) = \frac{\text{len}(R'_{\text{obj}} \cap A_{\text{obj}})}{\text{len}(A_{\text{obj}})}
\]
Indicates how well the response covers true objects present in the image.

\textbf{3. Hal (Hallucination Ratio)}
\[
\text{Hal}(R) =
\begin{cases}
1, & \text{if } \text{CHAIR}(R) \neq 0 \\
0, & \text{otherwise}
\end{cases}
\]
Reports whether a response contains any hallucination at all.

\textbf{4. Cog (Cognitive Hallucination Likelihood)}
\[
\text{Cog}(R) = \frac{\text{len}(R'_{\text{obj}} \cap H_{\text{obj}})}{\text{len}(R'_{\text{obj}})}
\]
Quantifies the extent to which generated hallucinations align with common human perceptual errors.

\subsubsection{Variants and Dimensions}

\begin{itemize}
    \item \textbf{Generative Hallucination Evaluation:} Evaluates free-form text outputs generated from images. Focuses on hallucinated objects or attributes appearing in descriptive captions.
    \item \textbf{Discriminative Hallucination Evaluation:} Evaluates binary (yes/no) responses in tasks such as object verification, attribute confirmation, and relation assessment. Performance is measured using Accuracy, Precision, Recall, and F1 scores.
    \item \textbf{Dimensional Analysis:} AMBER supports analysis of hallucination across:
    \begin{itemize}
        \item \textit{Existence hallucination} — incorrect claims of object presence,
        \item \textit{Attribute hallucination} — incorrect color, number, or action,
        \item \textit{Relation hallucination} — incorrect claims of object interaction.
    \end{itemize}
\end{itemize}

\subsubsection{Applications}

\begin{itemize}
    \item \textbf{Evaluation of Vision-Language Models:} Used to compare MLLMs such as GPT-4V, Qwen-VL, LLaVA-1.5, and mPLUG-Owl2 under consistent hallucination evaluation protocols \cite{Wang2023AMBER}.
    \item \textbf{Benchmark Development:} Serves as the foundation of the AMBER Benchmark, integrating 1,004 annotated images with attribute and relation metadata.
    \item \textbf{Model Diagnostics:} Enables fine-grained detection of specific hallucination types and model weaknesses (e.g., attribute or relation errors).
    \item \textbf{Training and Robustness Analysis:} Guides dataset curation and fine-tuning by identifying failure patterns and sensitivity to prompt types.
\end{itemize}

\subsubsection{Limitations}

\begin{itemize}
    \item The benchmark focuses mainly on visual hallucination and may not generalize to pure text-based settings.
    \item Object extraction errors from NLP toolkits (e.g., NLTK) can occasionally misclassify terms as hallucinated.
    \item Evaluations of attribute and relation hallucinations remain limited to discriminative tasks.
    \item The annotation of rare or uncommon objects may still be incomplete.
\end{itemize}