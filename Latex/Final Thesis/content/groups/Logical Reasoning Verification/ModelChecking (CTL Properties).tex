\subsection{Model Checking (CTL Properties)}

\textbf{Model Checking} is an automated technique used in behavioral verification to determine whether a system's model conforms to a given set of formal properties.

In the context of software verification, this process often involves:
\begin{itemize}
    \item \textbf{Inferring a Model}: Automatically generating a behavioral model (like an automaton or Mealy machine) from a running application, often using Active Automata Learning (AAL).
    \item \textbf{Specifying Properties}: Formally defining the desired system behaviors using a temporal logic. A common choice for this is \textbf{Computation Tree Logic (CTL)}, which can express properties over the possible execution paths of the system.
    \item \textbf{Verification}: Using a model checker tool to algorithmically and automatically check if the inferred model satisfies the specified CTL properties.
\end{itemize}

Model Checking is not based on a single arithmetic formula but on an algorithmic decision process. The formal verification condition is expressed as:

\begin{equation}
    M \models \phi
\end{equation}

\noindent where:
\begin{itemize}
    \item $M$: The behavioral model of the system (e.g., an inferred automaton).
    \item $\phi$ (phi): The property (or specification) written in a temporal logic like CTL.
    \item $\models$ (``models''): The satisfaction relation, meaning the model checker verifies that the property holds true for the model.
\end{itemize}

If the property does not hold ($M \not\models \phi$), the checker typically returns a \textbf{counterexample} (a trace or execution path) that demonstrates the violation.

\subsubsection*{Purpose}
The primary purpose of Model Checking with CTL properties is to provide \textbf{fully automated quality control and validation} for software systems. Specific goals include:
\begin{itemize}
    \item \textbf{Verifying Correctness}: Automatically verifying that a generated application behaves as intended.
    \item \textbf{Validating LLM-Generated Code}: Serving as a runtime check to control the quality of code produced by Large Language Models.
    \item \textbf{Validating Migration}: Confirming that a system migration (e.g., JS to TS) preserves the original behavior by comparing models.
    \item \textbf{Automating Correction}: Using counterexamples to automatically refine prompts or code.
\end{itemize}

\subsubsection*{Applications}
Based on the provided literature, Model Checking with CTL properties is applied in Software Engineering in the following areas:

\begin{itemize}
    \item \textbf{Verification of Concurrent Protocols}: It is used to verify the correctness of finite-state concurrent systems. For example, ensuring that messages in network protocols (like the Alternating Bit Protocol) are eventually received and acknowledged without loss or corruption \cite{clarke1986automatic}.
    
    \item \textbf{Synchronization Analysis}: It is applied to verify synchronization primitives, such as ensuring "absence of starvation" or "mutual exclusion" in multi-process systems, guaranteeing that a process is not indefinitely prevented from accessing critical resources \cite{clarke1986automatic}.

    \item \textbf{Validation of LLM-Generated Code}: In modern Language-Driven Engineering (LDE), CTL model checking is used to validate code generated by Large Language Models (LLMs). By inferring a behavioral model of the generated application (e.g., a web-based game), the system automatically verifies if the LLM output satisfies logic properties (such as "win" or "loss" conditions) \cite{Busch2025LLMCodeMigration}.

    \item \textbf{Automated System Migration}: It validates the migration of software systems (e.g., moving from JavaScript to TypeScript). By learning models of both the original and migrated systems, developers use model checking to ensure that the migration preserves the original behavior and introduces no regressions \cite{Busch2025LLMCodeMigration}.

    \item \textbf{Prompt Refinement Feedback}: When a CTL property is violated, the model checker generates counterexamples (error traces). These traces are used to provide feedback to the LLM, allowing for the iterative refinement of natural language prompts to correct the generated code automatically \cite{Busch2025LLMCodeMigration}.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Automation}: Provides a fully automated method for verifying system properties, requiring no manual test case creation.
    \item \textbf{Intuitive Feedback}: Inferred models (via AAL) provide a clear visual understanding of the system's actual behavior.
    \item \textbf{Actionable Corrections}: Generates specific counterexample traces upon failure, which can be fed back into the development loop.
    \item \textbf{Effective for Migration}: Can quantify behavioral discrepancies between system versions (``difference automata'').
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Scalability}: Depends on the scalability of the Active Automata Learning (AAL) process and managing state explosion.
    \item \textbf{Property Specification}: Writing correct and complete CTL properties requires specialized expertise in formal methods.
    \item \textbf{Model Fidelity}: Verification is performed on an \textit{inferred} model. Accuracy depends on how faithfully this model captures the real system's behavior.
\end{itemize}




\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
clarke1986automatic,
Busch2025LLMCodeMigration,
}
\fussy
