\subsection{Equivalent}

\subsubsection*{Introduction}
Equivalent (or Mathematical Equivalence) is an accuracy metric used in the evaluation of mathematical reasoning tasks. It defines a model's output as correct if the final answer is mathematically equivalent to the ground-truth reference, even if the string representation is different.

This metric is specifically designed for scenarios where numerical answers can be expressed in multiple valid formats (e.g., as fractions, decimals, or with different notation). It operates as a binary correctness check rather than a complex formula.

\subsubsection*{Definition}
The metric evaluates to \textbf{True} (or 1) if the model's final answer is mathematically equivalent to the reference, and \textbf{False} (or 0) otherwise.

\begin{equation}
    \text{Score} = \mathbb{1}(\text{Model}_{\text{final}} \equiv \text{Reference}_{\text{final}})
\end{equation}

For example, in datasets like MATH where answers are enclosed in a box:

\begin{center}
Ref: $\boxed{1/2} \quad \equiv \quad \text{Model}: \boxed{0.5} \implies \textbf{True}$
\end{center}

\subsubsection*{Purpose}
To provide a robust measure of \textbf{mathematical correctness} for models evaluated on problem-solving tasks. It is intentionally more flexible than ``Exact Match'' to avoid penalizing models for correct answers that are simply formatted differently.

\subsubsection*{Applications}
\begin{itemize}
    \item \textbf{LLM Evaluation}: Specifically in the domain of Mathematics and Reasoning.
    \item \textbf{Benchmarks}: It is the default accuracy metric for the \textbf{MATH} benchmark and its chain-of-thought variants.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Robust to Formatting}: Correctly scores answers that are numerically identical but expressed in different formats (e.g., 0.5 vs. 1/2 vs. 50\%).
    \item \textbf{Focuses on Correctness}: Measures the model's ability to derive the correct final answer, rather than its ability to perfectly mimic a single reference string.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Does Not Evaluate Reasoning}: This metric is designed to check the final answer only. It does not validate the correctness of the intermediate reasoning steps (the ``chain-of-thought'') that produced the answer. A model could arrive at the correct answer via flawed logic.
    \item \textbf{Domain-Specific}: It is highly specialized for mathematical or numerical outputs and is not suitable as a general-purpose metric for evaluating text.
\end{itemize}


\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Liang2022HELM,
}
\fussy