\subsection{Equivalent}

Equivalent (or Mathematical Equivalence) is an accuracy metric used in the evaluation of mathematical reasoning tasks. It defines a model's output as correct if the final answer is mathematically equivalent to the ground-truth reference, even if the string representation is different.

This metric is specifically designed for scenarios where numerical answers can be expressed in multiple valid formats (e.g., as fractions, decimals, or with different notation). It operates as a binary correctness check rather than a complex formula.

The metric evaluates to \textbf{True} (or 1) if the model's final answer is mathematically equivalent to the reference, and \textbf{False} (or 0) otherwise.

\begin{equation}
    \text{Score} = \mathbb{1}(\text{Model}_{\text{final}} \equiv \text{Reference}_{\text{final}})
\end{equation}

For example, in datasets like MATH where answers are enclosed in a box:

\begin{center}
Ref: $\boxed{1/2} \quad \equiv \quad \text{Model}: \boxed{0.5} \implies \textbf{True}$
\end{center}

\subsubsection*{Purpose}
To provide a robust measure of \textbf{mathematical correctness} for models evaluated on problem-solving tasks. It is intentionally more flexible than ``Exact Match'' to avoid penalizing models for correct answers that are simply formatted differently.

\subsubsection*{Applications}
\begin{itemize}
\item \textbf{LLM Evaluation}: Specifically in the domain of Mathematics and Reasoning. \cite{Liang2022HELM}
\item \textbf{Benchmarks}: It is the default accuracy metric for the \textbf{MATH} benchmark and its chain-of-thought variants. \cite{Liang2022HELM}
\item \textbf{Software Engineering and Code Synthesis}: Used to evaluate models on tasks like \textbf{HumanEval} and \textbf{APPS}, where mathematical equivalence is necessary to verify the correctness of outputs generated by code-specific models (e.g., \textit{code-davinci-002}). \cite{Liang2022HELM}
\item \textbf{Symbolic Reasoning}: Applied in scenarios such as \textbf{GSM8K} and synthetic reasoning tasks where the model's output must be parsed and compared against a ground-truth reference through symbolic equivalence. \cite{Liang2022HELM}
\item \textbf{Scientific Knowledge Verification}: Essential for evaluating the accuracy of models in STEM scenarios (Science, Technology, Engineering, and Mathematics) where numerical consistency is more important than string matching. \cite{Liang2022HELM}
\end{itemize}


\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Does Not Evaluate Reasoning}: This metric is designed to check the final answer only. It does not validate the correctness of the intermediate reasoning steps (the ``chain-of-thought'') that produced the answer. A model could arrive at the correct answer via flawed logic.
    \item \textbf{Domain-Specific}: It is highly specialized for mathematical or numerical outputs and is not suitable as a general-purpose metric for evaluating text.
\end{itemize}


\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Liang2022HELM,
}
\fussy