\subsection{EXAM (Explainable Automated Debugging)}

\subsubsection*{Introduction}
EXAM (Explainable Automated Debugging) is an evaluation benchmark used in software engineering to measure a model's capability in \textit{Fault Localization}. Introduced by Tran et al. (2022), it provides a standardized framework for assessing how well a model can debug code, specifically its ability to ``identify the buggy code lines.''

Unlike metrics that evaluate code generation (like BLEU), EXAM focuses on code \textbf{verification} and \textbf{augmentation}, specifically debugging. To quantify performance on this task, the benchmark utilizes specific ranking metrics.

\subsubsection*{Definition}
EXAM is a benchmark consisting of a set of ``288 Python programs with 12 error types.'' The primary task for a model evaluated on this benchmark is \textbf{fault localization}: the model must correctly identify the specific lines of code containing errors within these programs.

\subsubsection*{Purpose}
The purpose of EXAM is to evaluate models' capabilities in \textbf{code debugging}. It is used to quantitatively measure performance in localizing errors in Python programs, moving beyond simple generation correctness to understanding code logic and flaws.

\subsubsection*{Metrics Used within EXAM}
The EXAM benchmark uses the following metrics to score a model's performance:

\begin{itemize}
    \item \textbf{Top-N Accuracy}: Measures if the correct buggy line is found among the model's Top-N most likely predictions.
    \item \textbf{Mean Average Rank (MAR)}: Measures the average rank (position) of the correct buggy line in the model's prediction list. A lower MAR indicates better performance.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Chen2024DLBasedSE,
}
\fussy