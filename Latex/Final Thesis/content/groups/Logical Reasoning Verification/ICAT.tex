\subsection{Idealized Context Association (ICAT)}

The \textbf{Idealized Context Association Test (iCAT) Score} is a metric introduced by Nadeem, Bethke, and Reddy (2021) to measure stereotypical bias in language models. It is designed to be used with the \textbf{StereoSet} dataset.

The metric provides a single score that balances a model's language modeling capability (its ability to distinguish meaningful sentences from meaningless ones) with its level of stereotypical bias (its preference for stereotypical sentences over anti-stereotypical ones).

The iCAT score is calculated based on two sub-scores derived from the StereoSet benchmark:

\begin{itemize}
    \item \textbf{$lms$ (Language Modeling Score)}: The percentage of instances where the model correctly prefers a meaningful sentence option over a meaningless (unrelated) one. Ideally 100.
    \item \textbf{$ss$ (Stereotype Score)}: The percentage of instances where the model prefers a stereotypical option over an anti-stereotypical one. Ideally 50 (indicating no preference/bias).
\end{itemize}

The iCAT score is defined as:

\begin{equation}
    \text{iCAT} = lms \times \frac{\min(ss, 100 - ss)}{50}
\end{equation}

\subsubsection*{Purpose}
The purpose of iCAT is to provide a single, consolidated metric that evaluates a model's stereotypical bias while also accounting for its fundamental language understanding. 

The goal is to reward models that are both coherent (high $lms$) and unbiased ($ss$ near 50). A model that simply chooses ``anti-stereotype'' every time might get a good bias score but would be penalized by iCAT if it fails to distinguish meaningful sentences from nonsensical ones.

\subsubsection{Applications}

Within the domain of Software Engineering, specifically in \textbf{AI Engineering} and \textbf{Model Evaluation}, the ICAT score serves as a critical quality assurance metric for Pretrained Language Models (PLMs). Based on the provided text, its primary applications include:

\begin{itemize}
    \item \textbf{Quality Assurance for AI Services}: PLMs are widely deployed as software services (e.g., via Google Cloud or Amazon AWS) to serve millions of users \cite{nadeem2021stereoset}. ICAT allows engineers to quantify and assess the "adverse effects" of these models before they are integrated into downstream applications, ensuring that the software does not propagate harmful real-world biases \cite{nadeem2021stereoset}.
    
    \item \textbf{Model Selection (Performance vs. Fairness Trade-off)}: In software design, choosing the correct model requires balancing raw capability with safety. ICAT helps engineers select models that excel at "language modeling" (ranking meaningful contexts higher than meaningless ones) while behaving in an "unbiased manner" \cite{nadeem2021stereoset}. It penalizes models that are highly capable but biased, as well as models that are unbiased but nonsensical (random) \cite{nadeem2021stereoset}.
    
    \item \textbf{Bias Regression Tracking}: The metric supports a leaderboard system with a hidden test set to "track the bias of future language models" \cite{nadeem2021stereoset}. This functions as a regression testing mechanism, allowing software maintainers to verify if newer model architectures or training iterations have regressed in terms of fairness compared to previous baselines.
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{Balanced Score}: It combines a measure of language ability ($lms$) with a measure of bias ($ss$) into a single, interpretable score.
    \item \textbf{Penalizes Naive Models}: A model cannot achieve a high iCAT score by simply avoiding stereotypes; it must also demonstrate strong language modeling capabilities.
    \item \textbf{Clear Target}: The metric provides a clear ``ideal'' target (an $lms$ of 100 and an $ss$ of 50) for model evaluation.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{Dataset Dependency}: The metric is intrinsically tied to the StereoSet dataset. Criticisms of the dataset's validity apply to the metric as well.
    \item \textbf{Validity Concerns}: Research (e.g., Blodgett et al., 2021) has noted that many instances in StereoSet contain ``ambiguities about what stereotypes they capture'' or inconsistent perturbations.
    \item \textbf{Weak Downstream Correlation}: The iCAT score is based on pseudo-log-likelihood (PLL) preferences. Some studies caution that PLL metrics may have only weak correlations with biases that appear in real-world downstream tasks.
\end{itemize}


\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):

\sloppy
\cite{
nadeem2021stereoset,
}
\fussy
