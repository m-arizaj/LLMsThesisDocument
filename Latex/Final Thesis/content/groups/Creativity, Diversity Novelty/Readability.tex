\subsection{Readability}

\subsubsection*{Definition}
\textbf{Readability} is a criterion used to evaluate the \textbf{Linguistic Quality} of text generated by Large Language Models (LLMs).

In the context of recent surveys (e.g., Chang et al., 2024), Readability is not treated as a standalone automatic metric but as a key component of \textbf{Fluency}, which is a fundamental criterion for the \textbf{Human Evaluation} of LLMs. It assesses the model's ability to produce content that flows smoothly, is grammatically correct, maintains a consistent tone, and ensures a seamless user experience.

\subsubsection*{Formula}
In this specific context, Readability is treated as a component of Fluency within \textbf{Human Evaluation} rather than a specific automatic formula.

It is a \textbf{qualitative assessment} made by human evaluators who judge the quality of the generated text. This contrasts with automatic metrics like ROUGE or F1-Score which are calculated based on token overlap.

\subsubsection*{Purpose}
The primary purpose is to assess the quality of the generated text from a human perspective. This evaluation aims to:
\begin{itemize}
    \item Ensure the text ``flows smoothly.''
    \item Verify that the text is ``grammatically correct.''
    \item Confirm the model maintains a ``consistent tone and style.''
    \item Measure if the text provides a ``seamless user experience.''
    \item Determine if the model avoids awkward expressions and abrupt shifts.
\end{itemize}

\subsubsection*{Domains}
\begin{itemize}
    \item General LLM Evaluation
    \item Human Evaluation of LLMs
    \item Open-ended generation tasks
    \item Open-domain conversations
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{More Accurate Feedback}: As a human evaluation metric, it is ``closer to the actual application scenario'' and provides feedback that is more comprehensive than automatic metrics.
    \item \textbf{Reliable for Open Generation}: Considered ``more reliable'' for open-ended tasks where reference-based metrics (like BLEU) are often unsuitable.
    \item \textbf{Captures Nuance}: Assesses subtle linguistic qualities like flow and tone that automatic formulas fail to capture.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{High Variance}: As a human-judged metric, it can suffer from ``high variance and instability.''
    \item \textbf{Subjectivity}: Evaluation can be heavily influenced by ``cultural and individual differences'' among evaluators.
    \item \textbf{Requires Rigor}: Reliability depends on factors like the number of evaluators, their expertise, and the clarity of evaluation rubrics.
\end{itemize}

\subsubsection{Additional References}

This metric is referenced and/or used in the following paper(s):


\sloppy
\cite{
Chang2023SurveyLLMs,
vanderlee2019best,
}
\fussy

