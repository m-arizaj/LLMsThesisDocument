\subsection{Readability}

\subsubsection*{Definition}
\textbf{Readability} is a criterion used to evaluate the \textbf{Linguistic Quality} of text generated by Large Language Models (LLMs).

In the context of recent surveys (e.g., Chang et al., 2024), Readability is not treated as a standalone automatic metric but as a key component of \textbf{Fluency}, which is a fundamental criterion for the \textbf{Human Evaluation} of LLMs. It assesses the model's ability to produce content that flows smoothly, is grammatically correct, maintains a consistent tone, and ensures a seamless user experience.

\subsubsection*{Formula}
In this specific context, Readability is treated as a component of Fluency within \textbf{Human Evaluation} rather than a specific automatic formula.

It is a \textbf{qualitative assessment} made by human evaluators who judge the quality of the generated text. This contrasts with automatic metrics like ROUGE or F1-Score which are calculated based on token overlap.

\subsubsection*{Purpose}
The primary purpose is to assess the quality of the generated text from a human perspective. This evaluation aims to:
\begin{itemize}
    \item Ensure the text ``flows smoothly.''
    \item Verify that the text is ``grammatically correct.''
    \item Confirm the model maintains a ``consistent tone and style.''
    \item Measure if the text provides a ``seamless user experience.''
    \item Determine if the model avoids awkward expressions and abrupt shifts.
\end{itemize}

\subsubsection*{Domains}
\begin{itemize}
    \item General LLM Evaluation
    \item Human Evaluation of LLMs
    \item Open-ended generation tasks
    \item Open-domain conversations
\end{itemize}

\subsubsection*{Advantages}
\begin{itemize}
    \item \textbf{More Accurate Feedback}: As a human evaluation metric, it is ``closer to the actual application scenario'' and provides feedback that is more comprehensive than automatic metrics.
    \item \textbf{Reliable for Open Generation}: Considered ``more reliable'' for open-ended tasks where reference-based metrics (like BLEU) are often unsuitable.
    \item \textbf{Captures Nuance}: Assesses subtle linguistic qualities like flow and tone that automatic formulas fail to capture.
\end{itemize}

\subsubsection*{Limitations}
\begin{itemize}
    \item \textbf{High Variance}: As a human-judged metric, it can suffer from ``high variance and instability.''
    \item \textbf{Subjectivity}: Evaluation can be heavily influenced by ``cultural and individual differences'' among evaluators.
    \item \textbf{Requires Rigor}: Reliability depends on factors like the number of evaluators, their expertise, and the clarity of evaluation rubrics.
\end{itemize}

% a√±adir estas entradas al archivo .bib

% @article{chang2024survey,
%   title={A survey on evaluation of large language models},
%   author={Chang, Y. and Wang, X. and Wang, J. and others},
%   journal={ACM Transactions on Intelligent Systems and Technology},
%   volume={15},
%   number={3},
%   pages={1--45},
%   year={2024},
%   doi={10.1145/3641289}
% }

% @inproceedings{vanderlee2019best,
%   title={Best practices for the human evaluation of automatically generated text},
%   author={Van der Lee, C. and Gatt, A. and Van Miltenburg, E. and Wubben, S. and Krahmer, E.},
%   booktitle={Proceedings of the 12th International Conference on Natural Language Generation},
%   pages={355--368},
%   year={2019},
%   doi={10.18653/v1/W19-8643}
% }