\subsection{Memory-related Metrics}

Memory-related metrics quantify how efficiently a model utilizes system or GPU memory during inference, training, or long-context evaluation. In the context of Large Language Models (LLMs) and Software Engineering (SE), these metrics are essential for assessing scalability, efficiency, and the feasibility of running large models under constrained hardware.
Recent work, such as \textit{HeadInfer} (Luo et al., 2025), highlights the importance of tracking and reducing GPU memory footprint through techniques like head-wise KV-cache offloading \cite{Luo2025HeadInfer}. Meanwhile, \textit{LoCoBench} (Qiu et al., 2025) introduces memory-retention metrics to evaluate multi-session information persistence \cite{Qiu2025LoCoBench}, and Stein et al. (2023) analyze memorization behavior in generative models using memorization ratios \cite{Stein2023MetricFlaws}. 
Together, these perspectives connect computational memory usage with cognitive-style memory behaviors in LLM systems.

Because the reviewed papers do not present unified formulas for all memory metrics, the following expressions reflect generalized formulations consistent with concepts explicitly present in the literature.

\textbf{1. Memory Usage (Hardware-Level)}
Represents the peak memory requirement during execution (as used in HeadInfer) \cite{Luo2025HeadInfer}:
\begin{equation}
\text{Memory Usage} = \max_{t}(M_t)
\end{equation}
where $M_t$ is the memory consumed at time step $t$.
This aligns with measurements of GPU memory footprint, KV-cache size, and offloaded-cache proportions.

\textbf{2. Memory Reduction / Savings}
HeadInfer reports memory savings as \cite{Luo2025HeadInfer}:
\begin{equation}
\text{Memory Reduction} = \frac{M_{\text{baseline}} - M_{\text{optimized}}}{M_{\text{baseline}}}
\end{equation}

\textbf{3. Memorization Ratio (Generative Models)}
Adapted from Stein et al. (2023), measuring the extent to which generated outputs replicate training data \cite{Stein2023MetricFlaws}:
\begin{equation}
\text{Memorization Ratio} = \frac{N_{\text{reproduced}}}{N_{\text{total}}}
\end{equation}
Used for datasets such as CIFAR10, ImageNet, FFHQ, and LSUN-Bedroom to quantify training-data recall.

\textbf{4. Multi-Session Memory Retention (Long-Context Models)}
LoCoBench evaluates how much information a model successfully recalls across multiple sessions \cite{Qiu2025LoCoBench}.
A generalized form is:
\begin{equation}
\text{Retention Score} = \frac{\text{Relevant Information}_{\text{retrieved}}}{\text{Relevant Information}_{\text{expected}}}
\end{equation}
This matches the purpose of LoCoBench's MMR metric, even though the paper does not define a closed-form equation.

\subsubsection{Variants}

\begin{itemize}
    \item \textbf{Memory Usage:} Measures memory footprint during inference or training (e.g., GPU allocation, KV-cache size). Used primarily in efficiency studies such as HeadInfer \cite{Luo2025HeadInfer}.
    \item \textbf{Memory Reduction / Offloading Efficiency:} Evaluates the effectiveness of optimizations that reduce memory consumption by offloading parts of the model state or KV-cache \cite{Luo2025HeadInfer}.
    \item \textbf{Memorization Ratio:} Quantifies the degree of memorization in generative models, indicating whether outputs reproduce training data. Important for privacy, generalization, and overfitting analysis \cite{Stein2023MetricFlaws}.
    \item \textbf{Multi-Session Memory Retention:} Assesses how well a model retains information across long interactions or multiple sessions, as introduced in LoCoBench \cite{Qiu2025LoCoBench}.
\end{itemize}

\subsubsection{Applications in Software Engineering}

Memory metrics appear in SE-related LLM evaluations in several ways:

\begin{itemize}
    \item \textbf{Efficiency Benchmarks (e.g., HeadInfer):} Measure GPU memory footprint when running code-related models or long-context agents \cite{Luo2025HeadInfer}.
    \item \textbf{Long-Context SE Tasks (e.g., LoCoBench):} Test models' ability to retain and recall extended sequences such as long code files or documentation \cite{Qiu2025LoCoBench}.
    \item \textbf{Generative Model Memorization (Stein et al., 2023):} Relevant when assessing whether models leak or reproduce training data embedded in software repositories or documentation datasets \cite{Stein2023MetricFlaws}.
\end{itemize}

These applications help characterize both the computational and memory-behavior properties of systems used for code generation, analysis, and long-context reasoning.

\subsubsection{Interpretation}

Memory metrics illuminate the balance between hardware constraints, efficiency, and cognitive-style behavior in LLMs:

\begin{itemize}
    \item High memory usage may indicate inefficient caching or insufficient optimization for long-context inference \cite{Luo2025HeadInfer}.
    \item Effective memory reduction suggests improved deployability on limited hardware \cite{Luo2025HeadInfer}.
    \item Memorization ratios reveal potential overfitting, privacy risk, or lack of generalization \cite{Stein2023MetricFlaws}.
    \item Memory retention performance is crucial for conversational continuity and multi-step code-reasoning tasks \cite{Qiu2025LoCoBench}.
\end{itemize}

Together, these measures provide system-level insights essential for scaling LLMs in both research and applied software engineering settings.

\subsubsection{Additional References}
This metric and its variants are referenced and/or used in the following papers:

\cite{Chen2024SurveyCodeGen, Yeo2024FrameworkEvaluatingCode, Qiu2025LoCoBench, Stein2023MetricFlaws}