\subsection{Time-based Metrics}

\subsubsection{Introduction}

Time-based metrics measure the temporal efficiency of Large Language Models (LLMs) and software engineering systems during code generation, testing, or execution. These metrics capture how quickly a model performs inference, generates code, executes tests, or completes a task. They are central to evaluating performance, efficiency, and resource utilization, particularly when balancing speed against quality or correctness.
Time metrics include Execution Time, Runtime, Latency, Response Time, Task Completion Time, and Timeout Ratios, each emphasizing a different aspect of performance in LLM-based software engineering tasks.

\subsubsection{Formula and Structure}

There is no single formula for runtime metrics; rather, they rely on direct measurement of elapsed time across specific operational phases.
Common representations include:

\begin{itemize}
    \item \textbf{Execution Time ($T_e$):}
    \[ T_e = t_{\text{end}} - t_{\text{start}} \]
    Measures total time for code execution or inference.

    \item \textbf{Runtime per Token ($R_t$):}
    \[ R_t = \frac{T_{\text{total}}}{N_{\text{tokens}}} \]
    Evaluates per-token generation speed.

    \item \textbf{Latency ($L$):}
    Time delay between input submission and the first output token.
    \[ L = t_{\text{first output}} - t_{\text{input}} \]

    \item \textbf{Average Normalized Runtime:}
    Mean runtime across benchmarks normalized by reference implementation or baseline model.
    \[ \bar{T}_{\text{norm}} = \frac{1}{n} \sum_i \frac{T_i}{T_{\text{baseline}}} \]

    \item \textbf{Timeout Ratio:}
    Proportion of tasks exceeding a predefined time threshold.
    \[ \text{TimeoutRatio} = \frac{N_{\text{timeouts}}}{N_{\text{total}}} \]
\end{itemize}

\textbf{Derived Metrics from lm-Meter (Wang et al., 2025)}
Recent work introduces standardized metrics for on-device latency measurement \cite{Wang2025LMMeter}:
\begin{itemize}
    \item \textbf{Phase-level latency profiling:} Analyzing Embedding, Prefill, Decoding, and Sampling phases separately.
    \item \textbf{Accuracy of latency measurement ($\alpha_k$):}
    \[ \alpha_k = \left(1 - \frac{|t_{lm} - t_{GT}|}{t_{GT}}\right) \times 100\% \]
    \item \textbf{Scaled Error Rate ($\epsilon^*_k$):}
    \[ \epsilon^*_k = \frac{10^3 \Delta_k [\mu s]}{t_{GT} [ms]} \]
\end{itemize}
These formulations provide detailed profiling at microsecond precision to assess performance bottlenecks \cite{Wang2025LMMeter}.

\subsubsection{Variants}

\begin{enumerate}
    \item \textbf{Execution Time:} Measures overall duration of model execution or code runtime \cite{Chen2024SurveyCodeGen, Nascimento2024LLM4DS}.
    \item \textbf{Runtime / Average Normalized Runtime:} Focuses on total time for task completion, often normalized against a baseline \cite{Yeo2024FrameworkEvaluatingCode, Niu2024EvaluatingEfficiency}.
    \item \textbf{Latency:} Measures inference delay or reaction time during generation \cite{Bistarelli2025UsageLLMCode}.
    \item \textbf{Median Time to Generate Tests:} Used in test generation tasks \cite{Schafer2024UnitTestGeneration}.
    \item \textbf{Response Time and Run Duration:} Applied in symbolic regression and evolutionary code generation benchmarks \cite{Hemberg2024EvolvingCodeLLM}.
    \item \textbf{Timeout Rate / Timeout Ratio:} Quantifies performance degradation when tasks exceed runtime limits \cite{Liu2023IsYourCodeCorrect, Chen2021EvaluatingLLMCode}.
    \item \textbf{Task Completion Time:} Measures end-to-end productivity for multi-step coding workflows \cite{Paul2024BenchmarksMetricsCodeGen}.
    \item \textbf{Average Refactoring Time:} Used in refactoring or code editing tasks to measure time efficiency \cite{Anand2024AnalysisLLMCode}.
    \item \textbf{Time Taken:} Tracks total duration of automated program repair or specification synthesis \cite{Alhanahnah2025FormalSpecRepair}.
\end{enumerate}

\subsubsection{Interpretation}

Runtime-based metrics offer a direct measure of efficiency and responsiveness in LLM-assisted code generation.
Lower runtime or latency values indicate higher efficiency but must be balanced against accuracy, correctness, and energy consumption.

In software engineering contexts:
\begin{itemize}
    \item \textbf{Low Execution Time or Latency} $\rightarrow$ Efficient model inference or code execution.
    \item \textbf{High Timeout Ratios} $\rightarrow$ Potential inefficiencies or unoptimized model reasoning.
    \item \textbf{Average Refactoring Time and Task Completion Time} provide insights into productivity, reflecting user or system-level efficiency.
\end{itemize}

Recent studies such as lm-Meter (Wang et al., 2025) emphasize detailed profiling of on-device inference latency, enabling deeper understanding of phase-level bottlenecks and trade-offs between quality and performance \cite{Wang2025LMMeter}.

\subsubsection{Additional References}
This metric and its variants are referenced and/or used in the following papers:

\cite{Chen2024SurveyCodeGen, Anand2024AnalysisLLMCode, Paul2024BenchmarksMetricsCodeGen, Liu2023IsYourCodeCorrect, Yeo2024FrameworkEvaluatingCode, Bistarelli2025UsageLLMCode, Hemberg2024EvolvingCodeLLM, Chen2021EvaluatingLLMCode, Niu2024EvaluatingEfficiency, Nascimento2024LLM4DS, Schafer2024UnitTestGeneration, Alhanahnah2025FormalSpecRepair}