% !TEX root = ../thesis-example.tex
%
\chapter{Introduction}
\label{sec:intro}

\section{Motivation}

Software engineering is undergoing a profound transformation driven by the rapid advancement of Large Language Models (LLMs). These models now participate in tasks traditionally reserved for human developers: generating code, identifying defects, synthesizing documentation, analysing requirements, and assisting in design decisions. Their integration into development environments is expanding at unprecedented speed, offering new opportunities for productivity and automation. Yet, with this rapid progress comes an equally significant challenge: understanding and reliably measuring the quality of their outputs.

Evaluation has always played a central role in software engineering. Whether validating correctness, assessing maintainability, or ensuring that systems meet functional and non-functional requirements, robust measurement is essential for decision-making and quality assurance. However, the rise of generative models introduces complexities that traditional software metrics were never designed to address. LLM-generated outputs can be syntactically correct but semantically flawed, they may appear coherent yet contain subtle logical errors, they may solve a task in unexpected but valid ways, or they may generate misleading, biased, or unsafe content. Standard metrics struggle to capture these nuances.

At the same time, software engineering tasks are uniquely demanding for LLMs. Unlike general natural language, code is brittle, execution-dependent, and governed by strict semantics. Evaluating the behaviour, correctness, and usefulness of generated content requires a level of rigor that goes far beyond surface similarity or pattern matching. As a result, there is a growing need for evaluation frameworks specifically grounded in software artifacts and development workflows.

Despite substantial efforts across the industry, evaluation practices remain fragmented. Different studies rely on different benchmarks, inconsistent definitions, incompatible metrics, or incomplete reporting. Some evaluations focus solely on functional correctness, while others emphasize textual similarity, robustness, or human preference. These discrepancies make it difficult to compare systems, reproduce results, or understand what current LLMs are truly capable of. Furthermore, the increasing complexity of software-oriented LLMs, handling long-context interactions, multi-step reasoning, or tool-assisted execution, demands a new generation of metrics that are both interpretable and comprehensive.

Therefore, there is a critical need to systematically analyse, organize, and interpret the metrics used to evaluate LLMs in software engineering. A clear, structured understanding of what each metric measures, when it is appropriate, and how it contributes to reliable assessment is essential for advancing the field. Such work provides the foundation for rigorous benchmarking, improves scientific reproducibility, and supports the safe deployment of LLM-based systems in real-world development contexts.

This project is motivated by the need to fill that gap: to create a coherent, validated framework that helps researchers and organizations evaluate LLMs with clarity, consistency, and confidence. By studying the landscape of existing metrics and how they relate to software engineering tasks, this work aims to contribute to the development of trustworthy and scientifically grounded evaluation practices for the next generation of AI-driven software systems.

\section{Objectives}

\begin{enumerate}
	\item Identify the most relevant metrics used in the evaluation of Large Language Models (LLMs), encompassing both widely adopted measures and those that remain underexplored.
	\item Analyse the advantages, limitations, and conditions of applicability of each metric within the context of software engineering.
	\item Examine how these metrics are employed across software engineering research publications and empirical studies.
	\item Develop a standardized documentation framework that enables software engineers to apply evaluation metrics in an objective, consistent, and reproducible manner.

\end{enumerate}

\section{Expected results}
At the end of the project, a clear and organized analysis of the metrics used to evaluate LLMs in software engineering is expected to be delivered, together with a standardized documentation framework that supports objective and reproducible metric selection. The results should help establish a more coherent evaluation landscape, enabling consistent comparison of models and clearer understanding of each metric's purpose and limitations.
\section{Achieved results}
This project provides a structured classification of LLM evaluation metrics in software engineering and clarifies their applicability, strengths, and limitations. It also introduces a standardized documentation framework that supports reproducible and transparent metric usage. These contributions strengthen current evaluation practices and offer practical guidance for future research and applied SE contexts.
