% !TEX root = ../thesis-example.tex
%
\chapter{Conclusion}
\label{chapter5}

The work conducted enabled the identification, analysis, and organization of a wide range of evaluation metrics used in the assessment of Large Language Models, particularly within the context of software engineering. By consolidating dispersed definitions into a unified catalog, this thesis provides a structured roadmap for researchers and practitioners navigating the complex landscape of automated code evaluation.

The following sections present the fulfilled objectives, a synthesis of the key findings regarding the state of the art, project limitations, and potential directions for future work.

\section{Objectives and Key Findings}

A comprehensive review of the literature allowed the identification of the most relevant and frequently used evaluation metrics for LLMs, covering both established measures and emerging ones that remain underexplored. Furthermore, the project examined how these metrics appear across software engineering publications and empirical studies, offering clarity on their role in the field.

A key outcome of this analytical process was the rigorous refinement of the metrics taxonomy. Although the initial scoping phase identified \textbf{128 potential metric clusters}, the subsequent validation and detailed analysis led to a final catalog of \textbf{115 distinct metrics}. This consolidation was necessary as 13 clusters were either absorbed into broader categories to eliminate redundancy or discarded due to a lack of sufficient theoretical detail in the primary sources. This curation ensures that every entry in the final catalog represents a robust and distinct evaluation tool.

Beyond the identification and curation, this study revealed a clear evolutionary trend in evaluation strategies. The analysis highlights a paradigm shift from traditional NLP metrics, focused on lexical similarity like BLEU and ROUGE, towards semantic and execution-based metrics like CodeBERTScore, Data-Flow Match and Pass@k. This transition underscores a critical insight of this work: in the domain of software engineering, "correctness" is not merely about textual resemblance to a reference, but about functional validity, structural coherence, and logic preservation.

While the original objective included analysing the conditions of applicability of each metric, this aspect was not consistently addressed for every metric due to the heterogeneity of the sources. Instead, the documentation followed a flexible structure that, although built around a common pattern, was adapted to the specific characteristics and insights relevant to each metric. This methodological choice ensured clarity and relevance, allowing the catalog to accommodate everything from simple count-based metrics to complex LLM-as-a-judge frameworks.

Despite these adjustments, the overall goals of identifying metrics, analysing their advantages and limitations, understanding their specific usage within software engineering (such as code generation, translation, or refinement), and developing a standardized documentation framework were effectively accomplished.

\section{Limitations}

Although the project provides extensive information on a large set of LLM evaluation metrics, several limitations must be acknowledged. First, the metrics identified and documented were not empirically tested as part of this work. As a result, the study does not evaluate how closely real-world metric performance aligns with what is reported in the literature. Practical experimentation would have allowed a deeper understanding of metric reliability, robustness, and potential discrepancies between theory and practice.

Second, the time constraints of the project limited the depth of exploration possible for a domain as rapidly evolving as LLM evaluation. The landscape of large-scale AI models changes daily, with new benchmarks (such as LoCoBench or DevEval), vulnerabilities, and research insights emerging continuously. While the literature review was broad and rigorous, it is possible that extremely recent niche metrics or proprietary evaluation frameworks were not included.

Finally, the study depended on the availability and consistency of information within academic papers. A recurrent challenge was the ambiguity in metric definitions across different studies; some metrics lacked detailed mathematical formalizations or clear usage contexts. While the "Reference Validation" step mitigated this by tracing concepts back to seminal works, the inherent lack of standardization in the field remains a challenge that this taxonomy aims to alleviate but cannot entirely eliminate.

\section{Future Work}

Several directions emerge as valuable opportunities for extending this work. A natural next step is the empirical validation of the documented metrics. Implementing, testing, and comparing these metrics across a common set of LLMs would help determine their practical usefulness, computational cost, and correlation with human judgment beyond theoretical descriptions.

Another promising direction involves further developing the analysis of qualitative and efficiency-oriented evaluation metrics. Although the project addressed aspects such as environmental impact, resource usage (e.g., CPU/GPU utilization), and latency, these topics were explored at a high level. In practice, real-world adoption of LLMs depends heavily on factors like inference cost and stability dimensions that traditional quantitative metrics do not always capture in depth. Conducting more comprehensive research on metrics related to user experience, model robustness, and long-term reliability would offer a more complete and realistic perspective.

Finally, future work should explore the integration of "Repository-Level" evaluation frameworks. As demonstrated by metrics like Build@k or Cross-File Reasoning Depth, the field is moving away from isolated function generation towards complex, multi-file software engineering tasks. Expanding the catalog to include more metrics that assess architectural consistency, dependency management, and integration testing would be vital for the next generation of SE-specific LLMs.