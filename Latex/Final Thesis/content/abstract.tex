% !TEX root = ../thesis-example.tex
%
\pdfbookmark[0]{Summary}{Summary}
\chapter*{Summary}
\label{sec:abstract}
\vspace*{-10mm}
Large Language Models (LLMs) are increasingly used in Software Engineering (SE) tasks such as code generation, debugging, summarization, and test creation. Despite the wide range of available evaluation metrics, from accuracy and similarity measures to structural and execution-based metrics, the SE community still lacks a systematic, comprehensive, and critical analysis of how these metrics are defined, applied, and interpreted. As a result, researchers often select metrics inconsistently or without sufficient justification, which undermines the reliability, comparability, and rigor of LLM evaluations within the field.

This project addresses that gap by compiling the first exhaustive documentation and classification of LLM evaluation metrics used in recent SE literature, covering both widely adopted and lesser-known measures. The goal is to provide a structured reference that clarifies what each metric measures, how it should be used, and its limitations, ultimately supporting more objective, transparent, and reproducible evaluations. By establishing a standardized foundation for metric selection and interpretation, the project seeks to strengthen the methodological quality of Machine Learning for SE and promote the development of more consistent and trustworthy evaluation practices.

\vspace*{20mm}
