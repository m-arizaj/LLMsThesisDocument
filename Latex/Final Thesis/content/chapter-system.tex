\chapter{LLMs Metrics Catalog for Software Engineering}
\label{chapter3}

\section{Search Strategy and Theoretical Framework}

The first stage of the project consisted of a systematic search for literature addressing evaluation metrics for Large Language Models (LLMs) within the domains of Software Engineering (SE) and Machine Learning (ML). Two core search queries were designed, each targeting a different body of literature. The first query focused specifically on software engineering research, while the second targeted the broader machine learning literature. The primary objective of these queries was to identify relevant publications from 2019 onwards, coinciding with the period in which LLMs began to gain significant traction within both academia and industry.  

The queries were intentionally constructed to include a wide set of synonyms and keyword variations related to \textit{LLMs}, \textit{metrics}, \textit{machine learning}, and \textit{software engineering}, ensuring comprehensive coverage across research communities. The search expressions were centered and formatted as follows:

\begin{center}
\texttt{("large language model" OR LLM OR "language model" OR "foundation model")}\\
\texttt{AND ("evaluation metric" OR "performance metric" OR "benchmark" OR "evaluation framework" OR "assessment")}\\
\texttt{AND ("software engineering" OR "software development" OR "program synthesis" OR "code generation" OR "software quality" OR "SE")}\\
\texttt{since:2019}
\end{center}

\begin{center}
\texttt{("large language model" OR LLM OR "language model" OR "generative AI" OR "foundation model")}\\
\texttt{AND ("evaluation metric" OR "performance metric" OR "benchmark" OR "evaluation framework" OR "assessment")}\\
\texttt{AND ("machine learning" OR "deep learning" OR "neural network" OR "ML model" OR "predictive model")}\\
\texttt{since:2019}
\end{center}

These queries were executed across multiple academic repositories and digital libraries, including arXiv, SpringerLink, IEEE Xplore, ScienceDirect, ACM Digital Library, MIT Press Direct, and other relevant indexing platforms. Because each database implements its own query syntax and filtering interface, minor adjustments were applied as needed. However, the conceptual structure of the queries was preserved across all sources to ensure consistency in the search scope.

After retrieving the initial sets of results, each paper underwent a relevance screening process. This involved keyword-based filtering and rapid skimming of abstracts, introductions, and methodology sections to verify that the article contained substantive content related to LLM evaluation metrics. Only papers that explicitly discussed metrics, evaluation frameworks, benchmark designs, or methodological considerations for assessing LLM performance were retained for full analysis.

Through this search and filtering process, a total of 68 primary sources were selected as foundational to the construction of the LLMs Metrics Catalog. These references are publicly documented and accessible on the project website at:

\begin{center}
\url{https://m-arizaj.github.io/LLMs-metrics-catalog/references}
\end{center}

This curated set of sources forms the theoretical backbone of the catalog and provides comprehensive coverage of both software engineering--oriented evaluation practices and general-purpose LLM assessment methodologies.

\section{Paper Review and Metric Extraction}

Once the relevant literature was identified, a detailed extraction process was carried out to collect all metric-related information reported in the selected publications. For this purpose, a dedicated Excel-based dataset was constructed, consisting of two main sheets that supported the organization, normalization, and subsequent analysis of the extracted data.

The first sheet acted as a paper index and contained one entry per publication. Each row included the paper's unique identifier, its type, full name, and DOI link. This sheet served as the reference backbone for all subsequent processing. Table~\ref{tab:paperinfo} provides a simplified example of the structure used:

\begin{table}[H]
\centering
\caption{Example structure of the paper index sheet.}
\label{tab:paperinfo}
\begin{tabularx}{\textwidth}{ l l X X }
\hline
\textbf{ID} & \textbf{Type} & \textbf{Name} & \textbf{DOI} \\
\hline
1 & ML & Overview of the Comprehensive Evaluation of LLMs & 
\url{https://doi.org/10.1109/SWC62898.2024.00231} \\
\hline
\end{tabularx}
\end{table}

The second sheet contained the granular metric-level dataset. Unlike the first sheet, which stored one record per paper, this sheet stored \emph{one record per metric mentioned in a paper}. Each row represented a single occurrence of a metric, including the following fields: paper ID, publication year, domain, benchmark or dataset usage (when applicable), metric name, and the category assigned during the classification process. This structure allowed multiple entries for the same paper whenever multiple metrics were discussed. A conceptual example is shown in Table~\ref{tab:metricssheet}:

\begin{table}[H]
\centering
\caption{Example structure of the metric extraction sheet.}
\label{tab:metricssheet}
\resizebox{\textwidth}{!}{
\begin{tabular}{ l l l l l l }
\hline
\textbf{Paper ID} & \textbf{Year} & \textbf{Domain} & \textbf{Benchmark/Dataset} & \textbf{Metric} & \textbf{Category} \\
\hline
1 & 2024 & LLM Evaluation / NLP & GEM, GLGE & BLEU & Intelligence (NLG) \\
2 & 2024 & NLP & PubMed, BioMedSumm & BLEU & Token-Similarity (TS) \\
\hline
\end{tabular}
}
\end{table}

To populate the dataset, a careful reading of each paper was performed, with emphasis on sections explicitly describing evaluation metrics, benchmarks, experimental setups, or assessment methodologies. Whenever a metric appeared in any part of the paper, whether as part of an experiment, a comparison baseline, an ablation, or a discussion, it was recorded in the metric sheet following the structure above. This process was repeated independently for each of the 68 selected papers, ensuring that all mentioned metrics were captured with consistent formatting and metadata.

This systematic extraction pipeline enabled the construction of a granular and analyzable dataset, which later served as the foundation for the categorization, clustering, and comparative analysis of LLM evaluation metrics.


\section{Data Normalization and Frequency Analysis}

After the extraction phase, the initial dataset showed some problems due to how the different metrics were named accross papers for example: formatting differences, capitalization, accents. To enhance data integrity and avoid duplications, a normalization process was implemented using the Python programming language and Pandas as the library to transform the data.

We did the following steps:


\begin{enumerate}
	\item Text Normalization: Metric names were stripped (spaces were eliminated) and lowercased. Also, special characters like accents were standarized (for example from Métrica to metrica) using the unicode library. This process to have the different variants of a metric name treated like a single entity.
	\item Frequency Calculation: After the normalization process, we performed a frequency calculation that consisted of counting the times a metric appeared in the literature. This analysis helped us identify the most prevalent metrics in the community.
	\item Ranking and Filtering: The metrics were sorted in a descending order of frequency. This ranking showed the current state-of-the-art evaluation landscape and helped to create the future structure of the subsequent taxonomy.
\end{enumerate}

\section{Taxonomy Construction and Metric Classification}

Given that the existing literature includes a huge variety of disparate metrics without a common standard, we developed a classification taxonomy to logically organize our findings. The grouping followed a bottom-up inductive approach, where metrics were categorized based on their specific characteristics and shared properties. The steps taken were the following:

\begin{enumerate}
	\item Metrics Analysis: We took every metric and analyzed it to understand its main focus, for example syntactic overlap, functional correctness, or human preference?.
	\item Similarity Clustering: Metrics that share these same characteristics were grouped together. For instance, metrics that rely on human judgment were grouped separately from those relying on automated execution.
	\item Category Definition: Final labels were assigned to each group of metrics to represent the specific dimension of quality being assessed.
\end{enumerate}

We tried to use machine learning models such as Naive Bayes and KNN with TF-IDF vectorizer to make the clusters but the results very disperse (About 300 clusters). This is why Table 1 was created, this table allowed us to reduce the number of clusters from 300 to 128 clusters, a significant reduction in dispersion. The clusters were the following:

\begin{enumerate}
	\item Core Accuracy \& Overlap Metrics: Classic metrics, mostly used in natural language processing (NLP). They focus on measuring how close the text generated by the LLM is to a human reference based on word superposition or n-grams. For example: BLUE, ROUGE, Exact Math.
	\item Statistical \& Correlation Metrics: Grouping of metrics that use statistical methods to determine the linear relation or dependencies between the model and the reference. It doesn't measure "Quality" in itself, but how well the model's distribution or ranking aligns with the expected data.
	\item Code Quality \& Structural Metrics: This cluster is essential for software engineering. It evaluates if the code is generated with good practices, is readable, maintainable, and structurally valid.
	\item Human \& Subjective Evaluation: This cluster groups evaluations that are done by humans. It includes surveys, Likert scales, and side-by-side comparisons where experts determine utility or quality.
	\item Generative \& Distribution Metrics: It evaluates the probability of the model to generating tokens. It is useful to detect if the model is guessing or is confident in the answer.
	\item Logical Reasoning \& Verification: It focuses on the capability of the LLM to do a logical reasonig or formal deductions. It is really useful in SE in task that require complex requirements or business logic.
	\item Robustness, Security \& Reliability: It evaluates if the code is safe, checking if it has vulnerabilities or if it is resilient with malicious inputs. Additionaly, measures the stability of the answers to changes in the prompt.
	\item Efficiency \& Resource Usage: It measures the cost of using the model. It can refer to computational cost, latency or memory usage.
	\item Architectural \& System-level Metrics: It evaluates not only the fragment of code but the impact on the architecture, dependencies and coupling.
	\item Creativity, Diversity \& Novelty: Measures if the models always displays the same answer or if it has the ability to create different approaches for the same prompt.
	\item Ranking, Reward \& Optimization: Evaluates the capacity of the model to rank answers from best to worst or metrics derived from reward models that make predictions of the quality if the code without executing it.
	\item Semantic, Coherence \& Hallucination: Measures not only the exact meaning of the words generated by the LLM, but also whether the text makes sense, is factual or if the model is hallucinating.
\end{enumerate}


\section{Implementation of the Web-Based Metric Catalog}

To make this unified metric catalog accessible to the community, we decided to create a web-based platform using Docusaurus. This technology allows us to create modern static web pages with the ability to render Markdown documents into a high-performance single-page application, making it easy to maintain and deploy using the standard Node.js environment. This final product can be find in the following link: https://m-arizaj.github.io/LLMs-metrics-catalog/.

The structure of the web platform is simple and easy to understand; it was designed to function as a comprehensive knowledge base, similar to a wiki. It has a left sidebar that contains the metrics grouped by their corresponding taxonomy as defined in the previous section. This structure makes it simple for users to quickly access a certain group or a specific metric according to their needs.

The homepage gives an introduction to the importance of having a unify framework of metrics to evaluate LLMs in the Software Engineering community, highlighting the importance of standardized evaluation.

Furthermore, every page follows a standardized metadata schema that contains a definition, how the metric is measured, use cases, limitations, and references. This schema is prevalent on every page, nonetheless, some pages have variants. For example, in the case of BLEU, we have different types of this metric (BLEU, CodeBLEU, CrystalBLEU, Smoothed BLEU), so the page has a comparative summary to emphasize the key differences between them. Also, some pages have additional information like benchmarks, domains, and advantages. Additionally, the webpage features a search bar to easily find content by metric or keyword.

\begin{figure}[H]
	\textbf{Figure 1}

    \textit{Interface of the LLMs Metrics Catalog built with Docusaurus.} % Título en cursiva

    \centering
    \includegraphics[width=0.9\textwidth]{img/pagin.png}
    
    % --- INICIO DEL FORMATO APA ---
        
    % Nota/Fuente al pie (importante para APA)
    \vspace{0.2cm}
    \begin{flushleft}
    \footnotesize
    \textbf{Note.} Adapted from a screenshot of the project's website (Ariza et al., 2025). 
    \end{flushleft}
    % --- FIN DEL FORMATO APA ---
    
    \label{fig:web-catalog} % Mantén el label para citar en el texto (\ref{...})
\end{figure}


\section{Enrichment and Validation via Primary Sources}

During the previous steps, we initially identified metrics in the context of Software Engineering. This was done with papers that addressed metrics for LLM evaluation. These papers were the foundation of our project. However, relying only on secondary sources can be a risk, since definitions are often adapted to the specific needs of the author. So, to make sure this catalog has the most accurate definitions, an additional validation step was performed.

This process was applied to widely used metrics such as BLEU, CodeBERTScore, Rouge, or cases where the definition in the secondary source was incomplete or ambiguous. For these cases, we traced the citations back to the seminal work (the original paper where the metric was first proposed or defined).

This enrichment allowed us to:

\begin{enumerate}
	\item Verify information: We compared the information contained in the papers to identify if it was the same or was changed for a specific purpose.
	\item Enrich Definitions and Metadata: Consulting these primary sources allows us to be more precise with the theoretical foundations, specific usages, advantages, etc., that are often omitted in secondary sources.
	\item Enhanced Reliability: Provide users with canonical and precise definitions so they can read them directly on the webpage or consult the original citation to verify the information.
	\item Reference: Create a link for each metric on the webpage to its primary source reference, ensuring users can easily access the original or complementary papers for deeper investigation.
\end{enumerate}
