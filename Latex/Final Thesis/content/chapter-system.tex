\chapter{LLMs Metrics Catalog for Software Engineering}
\label{chapter3}

\section{Search Strategy and Theoretical Framework}

The first stage of the project consisted of a systematic search for literature addressing evaluation metrics for Large Language Models (LLMs) within the domains of Software Engineering (SE) and Machine Learning (ML). Two core search queries were designed, each targeting a different body of literature. The first query focused specifically on software engineering research, while the second targeted the broader machine learning literature. The primary objective of these queries was to identify relevant publications from 2019 onwards, coinciding with the period in which LLMs began to gain significant traction within both academia and industry.  

The queries were intentionally constructed to include a wide set of synonyms and keyword variations related to \textit{LLMs}, \textit{metrics}, \textit{machine learning}, and \textit{software engineering}, ensuring comprehensive coverage across research communities. The search expressions were centered and formatted as follows:

\begin{center}
\texttt{("large language model" OR LLM OR "language model" OR "foundation model")}\\
\texttt{AND ("evaluation metric" OR "performance metric" OR "benchmark" OR "evaluation framework" OR "assessment")}\\
\texttt{AND ("software engineering" OR "software development" OR "program synthesis" OR "code generation" OR "software quality" OR "SE")}\\
\texttt{since:2019}
\end{center}

\begin{center}
\texttt{("large language model" OR LLM OR "language model" OR "generative AI" OR "foundation model")}\\
\texttt{AND ("evaluation metric" OR "performance metric" OR "benchmark" OR "evaluation framework" OR "assessment")}\\
\texttt{AND ("machine learning" OR "deep learning" OR "neural network" OR "ML model" OR "predictive model")}\\
\texttt{since:2019}
\end{center}

These queries were executed across multiple academic repositories and digital libraries, including arXiv, SpringerLink, IEEE Xplore, ScienceDirect, ACM Digital Library, MIT Press Direct, and other relevant indexing platforms. Because each database implements its own query syntax and filtering interface, minor adjustments were applied as needed. However, the conceptual structure of the queries was preserved across all sources to ensure consistency in the search scope.

After retrieving the initial sets of results, each paper underwent a relevance screening process. This involved keyword-based filtering and rapid skimming of abstracts, introductions, and methodology sections to verify that the article contained substantive content related to LLM evaluation metrics. Only papers that explicitly discussed metrics, evaluation frameworks, benchmark designs, or methodological considerations for assessing LLM performance were retained for full analysis.

Through this search and filtering process, a total of 68 primary sources were selected as foundational to the construction of the LLMs Metrics Catalog. These references are publicly documented and accessible on the project website at:

\begin{center}
\url{https://m-arizaj.github.io/LLMs-metrics-catalog/references}
\end{center}

This curated set of sources forms the theoretical backbone of the catalog and provides comprehensive coverage of both software engineering--oriented evaluation practices and general-purpose LLM assessment methodologies.

\section{Paper Review and Metric Extraction}

Once the relevant literature was identified, a detailed extraction process was carried out to collect all metric-related information reported in the selected publications. For this purpose, a dedicated Excel-based dataset was constructed, consisting of two main sheets that supported the organization, normalization, and subsequent analysis of the extracted data.

The first sheet acted as a paper index and contained one entry per publication. Each row included the paper's unique identifier, its type, full name, and DOI link. This sheet served as the reference backbone for all subsequent processing. Table~\ref{tab:paperinfo} provides a simplified example of the structure used:

\begin{table}[H]
\centering
\caption{Example structure of the paper index sheet.}
\label{tab:paperinfo}
\begin{tabularx}{\textwidth}{ l l X X }
\hline
\textbf{ID} & \textbf{Type} & \textbf{Name} & \textbf{DOI} \\
\hline
1 & ML & Overview of the Comprehensive Evaluation of LLMs & 
\url{https://doi.org/10.1109/SWC62898.2024.00231} \\
\hline
\end{tabularx}
\end{table}

The second sheet contained the granular metric-level dataset. Unlike the first sheet, which stored one record per paper, this sheet stored \emph{one record per metric mentioned in a paper}. Each row represented a single occurrence of a metric, including the following fields: paper ID, publication year, domain, benchmark or dataset usage (when applicable), metric name, and the category assigned during the classification process. This structure allowed multiple entries for the same paper whenever multiple metrics were discussed. A conceptual example is shown in Table~\ref{tab:metricssheet}:

\begin{table}[H]
\centering
\caption{Example structure of the metric extraction sheet.}
\label{tab:metricssheet}
\resizebox{\textwidth}{!}{
\begin{tabular}{ l l l l l l }
\hline
\textbf{Paper ID} & \textbf{Year} & \textbf{Domain} & \textbf{Benchmark/Dataset} & \textbf{Metric} & \textbf{Category} \\
\hline
1 & 2024 & LLM Evaluation / NLP & GEM, GLGE & BLEU & Intelligence (NLG) \\
2 & 2024 & NLP & PubMed, BioMedSumm & BLEU & Token-Similarity (TS) \\
\hline
\end{tabular}
}
\end{table}

To populate the dataset, a careful reading of each paper was performed, with emphasis on sections explicitly describing evaluation metrics, benchmarks, experimental setups, or assessment methodologies. Whenever a metric appeared in any part of the paper, whether as part of an experiment, a comparison baseline, an ablation, or a discussion, it was recorded in the metric sheet following the structure above. This process was repeated independently for each of the 68 selected papers, ensuring that all mentioned metrics were captured with consistent formatting and metadata.

This systematic extraction pipeline enabled the construction of a granular and analyzable dataset, which later served as the foundation for the categorization, clustering, and comparative analysis of LLM evaluation metrics.


\section{Data Normalization and Frequency Analysis}

After the extraction phase, the initial dataset showed some problems due to how the different metrics were named accross papers (e.g., formatting differences, capitalization, accents). To enhance data integrity and avoid duplications, a normalization process was implemented using the Python programming language and Pandas as the library to transform the data.

We did the following steps:


\begin{enumerate}
	\item Text Normalization: Metric names were stripped (spaces were eliminated) and lowercased. Also, special characters like accents were standarized (for example from MÃ©trica to metrica) using the unicode library. This process to have the different variants of a metric name treated like a single entity.
	\item Frequency Calculation: After the normalization process, we performed a frequency calculation that consisted of counting the times a metric appeared in the literature. This analysis helped us identify the most prevalent metrics in the community.
	\item Ranking and Filtering: The metrics were sorted in a descending order of frequency. This ranking showed the current state-of-the-art evaluation landscape and helped to create the future structure of the subsequent taxonomy.
\end{enumerate}

\section{Taxonomy Construction and Metric Classification}

Given that the existing literature includes a huge variety of disparate metrics without a common standard, we developed a classification taxonomy to logically organize our findings. The grouping followed a bottom-up inductive approach, where metrics were categorized based on their specific characteristics and shared properties. The steps taken were the following:

\begin{enumerate}
	\item Metrics Analysis: We took every metric and analyzed it to understand its main focus (e.g., does it measure syntactic overlap, functional correctness, or human preference?).
	\item Similarity Clustering: Metrics that share these same characteristics were grouped together. For instance, metrics that rely on human judgment were grouped separately from those relying on automated execution.
	\item Category Definition: Final labels were assigned to each group of metrics to represent the specific dimension of quality being assessed.
\end{enumerate}

\section{Implementation of the Web-Based Metric Catalog}

To make this unified metric catalog accessible to the community, we decided to create a web-based platform using Docusaurus. This technology allows us to create modern static web pages with the ability to render Markdown documents into a high-performance single-page application, making it easy to maintain and deploy using the standard Node.js environment.

The structure of the web platform is simple and easy to understand; it was designed to function as a comprehensive knowledge base, similar to a wiki. It has a left sidebar that contains the metrics grouped by their corresponding taxonomy as defined in the previous section. This structure makes it simple for users to quickly access a certain group or a specific metric according to their needs.

The homepage gives an introduction to the importance of having a unify framework of metrics to evaluate LLMs in the Software Engineering community, highlighting the importance of standardized evaluation.

Furthermore, every page follows a standardized metadata schema that contains a definition, how the metric is measured, use cases, limitations, and references. This schema is prevalent on every page, nonetheless, some pages have variants. For example, in the case of BLEU, we have different types of this metric (BLEU, CodeBLEU, CrystalBLEU, Smoothed BLEU), so the page has a comparative summary to emphasize the key differences between them. Also, some pages have additional information like benchmarks, domains, and advantages.


\section{Enrichment and Validation via Primary Sources}

During the previous steps, we initially identified metrics in the context of Software Engineering papers. However, relying only on secondary sources can be a risk, since definitions are often adapted to the specific needs of the author. So, to make sure this catalog has the most accurate definitions, an additional validation step was performed.

This process was applied to widely used metrics (e.g., BLEU, CodeBERTScore) or cases where the definition in the secondary source was incomplete or ambiguous. For these cases, we traced the citations back to the seminal work (the original paper where the metric was first proposed or defined).

This enrichment allowed us to:

\begin{enumerate}
	\item Verify information: We compared the information contained in the papers to identify if it was the same or was changed for a specific purpose.
	\item Enrich Definitions and Metadata: Consulting these primary sources allows us to be more precise with the theoretical foundations, specific usages, advantages, etc., that are often omitted in secondary sources.
	\item Enhanced Reliability: Provide users with canonical and precise definitions so they can read them directly on the webpage or consult the original citation to verify the information.
\end{enumerate}

