\chapter{LLMs Metrics Catalog for Software Engineering}
\label{chapter3}

\section{Search Strategy and Theoretical Framework}

The first stage of the project consisted of a systematic search for literature addressing evaluation metrics for Large Language Models (LLMs) within the domains of Software Engineering (SE) and Machine Learning (ML). Two core search queries were designed, each targeting a different body of literature. The first query focused specifically on software engineering research, while the second targeted the broader machine learning literature. The primary objective of these queries was to identify relevant publications from 2019 onwards, coinciding with the period in which LLMs began to gain significant traction within both academia and industry.  

The queries were intentionally constructed to include a wide set of synonyms and keyword variations related to \textit{LLMs}, \textit{metrics}, \textit{machine learning}, and \textit{software engineering}, ensuring comprehensive coverage across research communities. The search expressions were centered and formatted as follows:

\begin{center}
\texttt{("large language model" OR LLM OR "language model" OR "foundation model")}\\
\texttt{AND ("evaluation metric" OR "performance metric" OR "benchmark" OR "evaluation framework" OR "assessment")}\\
\texttt{AND ("software engineering" OR "software development" OR "program synthesis" OR "code generation" OR "software quality" OR "SE")}\\
\texttt{since:2019}
\end{center}

\begin{center}
\texttt{("large language model" OR LLM OR "language model" OR "generative AI" OR "foundation model")}\\
\texttt{AND ("evaluation metric" OR "performance metric" OR "benchmark" OR "evaluation framework" OR "assessment")}\\
\texttt{AND ("machine learning" OR "deep learning" OR "neural network" OR "ML model" OR "predictive model")}\\
\texttt{since:2019}
\end{center}

These queries were executed across multiple academic repositories and digital libraries, including arXiv, SpringerLink, IEEE Xplore, ScienceDirect, ACM Digital Library, MIT Press Direct, and other relevant indexing platforms. Because each database implements its own query syntax and filtering interface, minor adjustments were applied as needed. However, the conceptual structure of the queries was preserved across all sources to ensure consistency in the search scope.

After retrieving the initial sets of results, each paper underwent a relevance screening process. This involved keyword-based filtering and rapid skimming of abstracts, introductions, and methodology sections to verify that the article contained substantive content related to LLM evaluation metrics. Only papers that explicitly discussed metrics, evaluation frameworks, benchmark designs, or methodological considerations for assessing LLM performance were retained for full analysis.

Through this search and filtering process, a total of 68 primary sources were selected as foundational to the construction of the LLMs Metrics Catalog. These references are publicly documented and accessible on the project website at:

\begin{center}
\url{https://m-arizaj.github.io/LLMs-metrics-catalog/references}
\end{center}

This curated set of sources forms the theoretical backbone of the catalog and provides comprehensive coverage of both software engineering--oriented evaluation practices and general-purpose LLM assessment methodologies.

\section{Paper Review and Metric Extraction}

Once the relevant literature was identified, a detailed extraction process was carried out to collect all metric-related information reported in the selected publications. For this purpose, a dedicated Excel-based dataset was constructed, consisting of two main sheets that supported the organization, normalization, and subsequent analysis of the extracted data.

The first sheet acted as a paper index and contained one entry per publication. Each row included the paper's unique identifier, its type, full name, and DOI link. This sheet served as the reference backbone for all subsequent processing. Table~\ref{tab:paperinfo} provides a simplified example of the structure used:

\begin{table}[H]
\centering
\caption{Example structure of the paper index sheet.}
\label{tab:paperinfo}
\begin{tabularx}{\textwidth}{ l l X X }
\hline
\textbf{ID} & \textbf{Type} & \textbf{Name} & \textbf{DOI} \\
\hline
1 & ML & Overview of the Comprehensive Evaluation of LLMs & 
\url{https://doi.org/10.1109/SWC62898.2024.00231} \\
\hline
\end{tabularx}
\end{table}

The second sheet contained the granular metric-level dataset. Unlike the first sheet, which stored one record per paper, this sheet stored \emph{one record per metric mentioned in a paper}. Each row represented a single occurrence of a metric, including the following fields: paper ID, publication year, domain, benchmark or dataset usage (when applicable), metric name, and the category assigned during the classification process. This structure allowed multiple entries for the same paper whenever multiple metrics were discussed. A conceptual example is shown in Table~\ref{tab:metricssheet}:

\begin{table}[H]
\centering
\caption{Example structure of the metric extraction sheet.}
\label{tab:metricssheet}
\resizebox{\textwidth}{!}{
\begin{tabular}{ l l l l l l }
\hline
\textbf{Paper ID} & \textbf{Year} & \textbf{Domain} & \textbf{Benchmark/Dataset} & \textbf{Metric} & \textbf{Category} \\
\hline
1 & 2024 & LLM Evaluation / NLP & GEM, GLGE & BLEU & Intelligence (NLG) \\
2 & 2024 & NLP & PubMed, BioMedSumm & BLEU & Token-Similarity (TS) \\
\hline
\end{tabular}
}
\end{table}

To populate the dataset, a careful reading of each paper was performed, with emphasis on sections explicitly describing evaluation metrics, benchmarks, experimental setups, or assessment methodologies. Whenever a metric appeared in any part of the paper, whether as part of an experiment, a comparison baseline, an ablation, or a discussion, it was recorded in the metric sheet following the structure above. This process was repeated independently for each of the 68 selected papers, ensuring that all mentioned metrics were captured with consistent formatting and metadata.

This systematic extraction pipeline enabled the construction of a granular and analyzable dataset, which later served as the foundation for the categorization, clustering, and comparative analysis of LLM evaluation metrics.


\section{Data Normalization and Frequency Analysis}

After the extraction phase, the initial dataset showed some problems due to how the different metrics were named across papers for example: formatting differences, capitalization, accents. To enhance data integrity and avoid duplications, a normalization process was implemented using the Python programming language and Pandas as the library to transform the data.

We did the following steps:


\begin{enumerate}
	\item Metric Normalization: The metric names were systematically normalized. This involved stripping whitespace, converting all text to lowercase, and removing special characters (such as accents) using the Unicode library. This ensured that different labels for the same metric were treated as a single entity.
	\item Frequency Calculation: After the normalization process, we use Pandas to performe a frequency calculation that consisted of counting the times a metric appeared in the literature. This analysis helped us identify the most prevalent metrics in the community.
	\item Ranking and Filtering: The metrics were sorted in a descending order of frequency. This ranking showed the current state-of-the-art evaluation landscape and helped to create the future structure of the subsequent taxonomy.
\end{enumerate}

\subsection{Top Metrics Landscape}

The Table \ref{tab:top10metrics} presents the top 10 most frequently cited metrics in the analyzed literature.

\begin{table}[h]
    \centering
    \caption{Top 10 Most Frequent Evaluation Metrics}
    \label{tab:top10metrics}
    \begin{tabular}{lcc}
        \hline
        \textbf{Rank} & \textbf{Metric Name} & \textbf{Frequency} \\ \hline
        1 & BLEU & 40 \\ 
        2 & Accuracy & 29 \\
        3 & Pass@k & 27 \\
        4 & CodeBLEU & 25 \\
        5 & F1-score & 19 \\
        6 & Exact Match & 14 \\
        7 & Rouge-L & 13 \\
        8 & Recall & 13 \\
        9 & Pass@1 & 12 \\
        10 & Precision & 12 \\ \hline
    \end{tabular}
\end{table}

Most of the metrics that appeared in the top 10 share a common purpose. They focuses on direct comparisons between model outputs and reference solutions, emphasizing correctness and matching behavior. This is an important remark to understand not only which metrics are the most used, but also the specific purpose behind their adoption.

\section{Taxonomy Construction and Metric Classification}

Given that the existing literature includes a huge variety of disparate metrics without a common standard, we developed a classification taxonomy to logically organize our findings. The grouping followed a bottom-up inductive approach, where metrics were categorized based on their specific characteristics and shared properties. The steps taken were the following:

\begin{enumerate}
	\item Metrics analysis: We take each metric and analyze it to understand its area of focus, for example, syntactic overlap, functional correctness, or human preference.
	\item Similarity grouping: Metrics that share these same characteristics were grouped together. For instance, metrics that rely on human judgment were grouped separately from those relying on automated execution.
	\item Category Definition: Final labels were assigned to each group of metrics to represent the specific dimension of quality being assessed.
\end{enumerate}

\subsection{Clustering Methodology}

To manage the high volume of raw metrics found in the literature, we initially explored an automated approach to group them. We implemented machine learning models such as Naive Bayes and KNN with a TF-IDF vectorizer to generate preliminary groupings. However, the results were highly dispersed (yielding about 300 groups) with weak semantic connections.

To address this dispersion and reduce redundancy, we moved to a structured manual refinement process defined by two hierarchical levels:

Metric Clusters (N=128): We define a "cluster" as a specific evaluation method that joins all its naming variants and specific implementations. For example, pass@1, pass@10, and pass@k were grouped into a single cluster named Pass@k. This process allowed us to reduce the initial 300 into 128 distinct metric clusters.

Taxonomy Categories (N=13): These represent high-level quality dimensions. Each category groups multiple Metric Clusters that share similar characteristics or goals. To make an example, all clusters relying on computational efficiency are grouped under Efficiency \& Resource Usage.
\subsection{The 13 Quality Categories}

Based on the 128 defined clusters, we established a taxonomy consisting of 13 categories as follows:

\begin{enumerate}
	\item Core Accuracy \& Overlap Metrics: Classic metrics, mostly used in natural language processing (NLP). In this category metrics focus on measuring how close the text that is generated by the LLM is to a human reference based on word overlap or n-grams. For example: BLEU, ROUGE, Exact Match.
	\item Statistical \& Correlation Metrics: Grouping of metrics that use statistical methods to determine the linear relation or dependencies between the model and the reference. They do not only measure "Quality" in itself, but how well the model's distribution or ranking aligns with the expected data.
	\item Code Quality \& Structural Metrics: This group is essential for software engineering. It evaluates whether the code is generated with good practices, is readable, maintainable, and structurally valid.
	\item Functional \& Test-based Evaluation: This group measures if the code is functional enough to pass various test cases. Each cluster in this category has their own set of test cases or quantitative counts (such as iterations) to prove if the code succeeds or fails.
	\item Human \& Subjective Evaluation: This section groups evaluations conducted by humans. It includes surveys, Likert scales, and side-by-side comparisons where experts determine utility or quality.
	\item Generative \& Distribution Metrics: This group of clusters analyzes how well the model's answers match the patterns found in data generated by humans. This category looks at the 'big picture' of the model's behavior: it measures coverage (to ensure it covers the full range of expected scenarios), diversity (to be sure the output is not duplicating or getting stuck), and realism (to verify that the generated text can be compared with human work)."
	\item Logical Reasoning \& Verification: It focuses on the capability of the LLM to do a logical reasoning or formal deductions. It is really useful in SE in task that require complex requirements or business logic.
    \item Robustness, Security \& Reliability: It focuses on safety and consistency, checking for vulnerabilities and resilience against malicious inputs. Additionally, it measures the stability of the answers to changes in the prompt.	
    \item Efficiency \& Resource Usage: It measures the cost of using the model. It can refer to computational cost, latency or memory usage. This group is key when optimization is required for the success of the project.
    \item Architectural \& System-level Metrics: This category evaluates not only code fragments but focuses primarily on a high level of abstraction. This group investigates system-wide characteristics, architectural behavior, dependencies, and coupling.	
    \item Creativity, Diversity \& Novelty: Measures if the models always displays the same answer or if it has the ability to create different approaches for the same prompt.
	\item Ranking, Reward \& Optimization: Evaluates the capacity of the model to rank answers from best to worst or metrics derived from reward models that make predictions of the quality if the code without executing it.
	\item Semantic, Coherence \& Hallucination: Measures not only the exact meaning of the words generated by the LLM, but also whether the text makes sense, is factual or is free from hallucinations.
\end{enumerate}

This taxonomy gives us a systematic organization and an effective filtering mechanism. It is important to specify that a metric can share characteristics from two or more different groups of categories, but each metric was assigned to a single primary category to maintain simplicity and structural clarity.

\section{Implementation of the Web-Based Metric Catalog}

To make this unified metric catalog accessible to the community, we decided to create a web-based platform using Docusaurus. This technology allows us to create modern static web pages with the ability to render Markdown documents into a high-performance single-page application, making it easy to maintain and deploy using the standard Node.js environment. This final product can be found in the following link:

\begin{center}
\url{https://m-arizaj.github.io/LLMs-metrics-catalog/}
\end{center}

The structure of the web platform is simple and easy to understand; it was designed to function as a comprehensive knowledge base, similar to a wiki. It has a left sidebar that contains the metrics grouped by their corresponding taxonomy as defined in the previous section. This structure makes it simple for users to quickly access a certain group or a specific metric according to their needs.

The homepage gives an introduction to the importance of having a unify framework of metrics to evaluate LLMs in the Software Engineering community, highlighting the importance of standardized evaluation.

Furthermore, every page follows a standardized metadata schema that contains a definition, how the metric is measured, use cases, limitations, and references. This schema is prevalent on every page, nonetheless, some pages have variants. For example, in the case of BLEU, we have different types of this metric (BLEU, CodeBLEU, CrystalBLEU, Smoothed BLEU), so the page has a comparative summary to emphasize the key differences between them. Also, some pages have additional information like benchmarks, domains, and advantages. Additionally, the webpage features a search bar to easily find content by metric or keyword.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{img/pagin.png}
	\vspace{-0.5cm}
	\caption{Interface of the LLMs Metrics Catalog built with Docusaurus}
	\label{procesoTests}
\end{figure} 

All the documentation, source files, and development process for the platform are openly available in the project repository. The codebase, including the Markdown files, page components, taxonomy structure, and contribution instructions, can be accessed at: https://github.com/m-arizaj/LLMs-metrics-catalog. This ensures full transparency of the project and enables others to extend or improve the catalog for future research.

\section{Enrichment and Validation via Primary Sources}

During the previous steps, we initially identified metrics in the context of Software Engineering. This was done with papers that addressed metrics for LLM evaluation. These papers were the foundation of our project. However, relying only on secondary sources can be a risk, since definitions are often adapted to the specific needs of the author. So, to make sure this catalog has the most accurate definitions, an additional validation step was performed.

This process was applied to widely used metrics such as BLEU, CodeBERTScore, Rouge, or cases where the definition in the secondary source was incomplete or ambiguous. For these cases, we traced the citations back to the seminal work (the original paper where the metric was first proposed or defined).

This enrichment allowed us to:

\begin{enumerate}
	\item Verify information: We compared the information contained in the papers to identify if it was the same or was changed for a specific purpose.
	\item Enrich Definitions and Metadata: Consulting these primary sources allows us to be more precise with the theoretical foundations, specific usages, advantages, etc., that are often omitted in secondary sources.
	\item Enhanced Reliability: Provide users with canonical and precise definitions so they can read them directly on the webpage or consult the original citation to verify the information.
	\item Reference: Create a link for each metric on the webpage to its primary source reference, ensuring users can easily access the original or complementary papers for deeper investigation.
\end{enumerate}
