% !TEX root = ../thesis-example.tex
%
\chapter{Related Work}
\label{chapter2}

The rapid development of large language models has motivated a growing body of work that seeks to systematically analyse how such models should be evaluated. Chang et al. (2023) \cite{Chang2023SurveyLLMs} provide an extensive survey of existing evaluation practices, organizing them around what to evaluate, where to evaluate it, and how to conduct the evaluation. Their review spans multiple application domains, including natural language processing and software engineering, and distinguishes between automatic metrics and human-based evaluation criteria used across general purpose benchmarks. The authors emphasize that traditional evaluation methods developed for earlier NLP systems are increasingly inadequate for capturing the multifaceted capabilities of modern LLMs, which engage in a broad range of tasks, interact with users, and often function as general-purpose assistants. Complementing this perspective, Guo et al. (2023) \cite{Guo2023EvalLLMs} survey contemporary evaluation methodologies, benchmarks, and metrics, highlighting both the rapid proliferation of evaluation frameworks and the lack of unified taxonomies or consistent standards for selecting appropriate evaluation methods.

The need for principled evaluation frameworks is further developed on holistic and comprehensive evaluation. Liang et al. (2022) introduce Holistic Evaluation of Language Models (HELM), which argues that evaluation must be multidimensional and transparent, covering various metrics at the same time under a single category. HELM proposes a structured framework in which models are tested across a wide range of scenarios, with explicit reporting of trade-offs and limitations \cite{Liang2022HELM}. Lin et al. (2024) \cite{Lin2024SWC} continue this line of work by providing an overview of comprehensive evaluation approaches for LLMs, synthesizing perspectives on how to integrate task performance, interpretability, and user-centric criteria into coherent evaluation dimensions. Together, these works underscore the need for systematic, multi-axis evaluation rather than reliance on a small set of isolated metrics.

Several studies highlight structural weaknesses in current evaluation metrics. Stein et al. (2023) \cite{Stein2023MetricFlaws} show that widely used generative-model metrics often fail to reflect human judgments of output quality, frequently mis-rank model performance and providing unreliable signals about fidelity, diversity, and memorization. Guo et al. (2023) \cite{Guo2023EvalLLMs} report similar concerns in the context of LLM evaluation, noting inconsistent correlations with human assessments, limited robustness across tasks and datasets, and difficulties interpreting metric outputs in a reliable and comparable way. Collectively, these findings underscore the need for more dependable and semantically grounded evaluation methodologies that better reflect human perception and model behaviour.

Safety and reliability constitute another major theme in recent evaluation research. Woesle et al. (2025) present a systematic literature review of hallucinations in large language models, synthesizing a wide range of empirical studies to characterize when and how hallucinations occur, how they are measured, and what mitigation strategies have been proposed. Their review shows that hallucinations are pervasive across tasks, and that existing evaluation methods for hallucinations are fragmented, often relying on task-specific heuristics or narrow benchmark datasets \cite{Woesle2025Hallucinations}. Chang et al. (2023) \cite{Chang2023SurveyLLMs} also highlight safety and hallucinations as central evaluation challenges, stressing that current metrics frequently struggle to capture factuality, grounding, and reliability in open-ended generation. These works collectively indicate that evaluation must go beyond correctness on predefined tasks to include systematic assessment of hallucinations and other safety-related behaviours.

Within software engineering, the evaluation of LLMs presents additional challenges due to the semantic strictness, structural constraints, and execution-dependent behaviour of code. Hou et al. (2023) \cite{Hou2023LLMsSESLR} report that evaluation practices across SE studies are highly inconsistent, noting that many works adopt NLP-oriented metrics without examining their suitability for code-centric tasks, and that evaluations often lack detailed justification or analysis of metric selection. Bektas (2025) \cite{Bektas2025CriticalReview} reinforces these concerns in a critical review of evaluation strategies for SE, emphasizing that current evaluation setups frequently suffer from inconsistencies in metrics, validation procedures, and reproducibility, and that the chosen evaluation methods often do not align with the goals of software engineering tasks. The review also highlights that evaluations of code generation, repair, and related tasks regularly overlook important software-engineering qualities such as maintainability, robustness, and developer-oriented usefulness.

Benchmark design for LLM-based software engineering tasks has also been examined in detail. Hu et al. (2025) assess and advance benchmarks for evaluating large language models in software engineering tasks, systematically analysing how existing benchmarks define tasks, construct datasets, and choose evaluation metrics. Their work shows that many benchmarks focus narrowly on functional correctness while underrepresenting other dimensions such as robustness, code quality, and difficulty in cross-domain transfer \cite{Hu2025BenchmarksSE}. They also identify inconsistencies in the use of metrics and call for more carefully designed benchmarks that explicitly consider evaluation criteria and reporting standards. This line of work suggests that both metrics and benchmarks must be co-designed to reflect the multifaceted nature of software engineering practice.

Taken together, these studies indicate that evaluation of large language models, and particularly of LLMs applied to software engineering, is still in a state of methodological flux. Surveys and frameworks at the general LLM level argue for multidimensional, transparent, and comprehensive evaluation \cite{Chang2023SurveyLLMs, Guo2023EvalLLMs, Liang2022HELM, Lin2024SWC}, while more specialized work reveals limitations and inconsistencies in current metrics and benchmarks, including weak correlation with human judgment, insufficient safety coverage, and poor alignment with software engineering concerns \cite{Bektas2025CriticalReview,Hou2023LLMsSESLR,Hu2025BenchmarksSE,Stein2023MetricFlaws, Woesle2025Hallucinations}. This body of related work highlights the need for systematic analysis and organization of evaluation metrics, particularly in the software engineering domain, where both the artifacts and the evaluation requirements differ substantially from those of traditional natural language tasks.


